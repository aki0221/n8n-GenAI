# 第6章: データフロー設計

**作成支援**: Manus AI

## データパイプライン設計

トリプルパースペクティブ型戦略AIレーダーの成功は、効率的で信頼性の高いデータパイプラインの設計に大きく依存します。本セクションでは、リアルタイム処理とバッチ処理を統合し、高品質なデータフローを実現するパイプライン設計を詳細に解説します。

### リアルタイム・バッチ処理の統合

現代の戦略的意思決定では、即座の対応が必要な緊急事項と、深い分析が必要な長期的課題の両方に対応する必要があります。我々のデータパイプライン設計は、Lambda アーキテクチャとKappa アーキテクチャの利点を統合したハイブリッドアプローチを採用します。

#### 統合アーキテクチャの設計原則

**リアルタイム処理層（Speed Layer）**では、低遅延での即座の洞察提供を重視します。市場の急激な変化、技術的な突破、競合の動向など、時間的制約のある戦略的機会を逃さないため、秒単位での処理を実現します。

**バッチ処理層（Batch Layer）**では、大量データの包括的分析による深い洞察の提供を重視します。長期トレンド分析、複雑な相関関係の発見、予測モデルの構築など、計算集約的な処理を効率的に実行します。

**統合層（Serving Layer）**では、リアルタイムとバッチの結果を統合し、一貫性のある戦略的洞察を提供します。データの整合性を保ちながら、各処理層の利点を最大化します。

```python
# data_pipeline/unified_pipeline/pipeline_orchestrator.py
from dataclasses import dataclass
from typing import Dict, List, Any, Optional, Union
from enum import Enum
import asyncio
import logging
from datetime import datetime, timedelta
import json

class ProcessingMode(Enum):
    REALTIME = "realtime"
    BATCH = "batch"
    HYBRID = "hybrid"

class DataSource(Enum):
    TECHNOLOGY = "technology"
    MARKET = "market"
    BUSINESS = "business"
    EXTERNAL_API = "external_api"
    SOCIAL_MEDIA = "social_media"
    NEWS_FEEDS = "news_feeds"

@dataclass
class PipelineConfig:
    """パイプライン設定"""
    name: str
    processing_mode: ProcessingMode
    data_sources: List[DataSource]
    batch_interval: timedelta
    realtime_window: timedelta
    quality_threshold: float
    retry_attempts: int
    timeout_seconds: int

@dataclass
class DataPacket:
    """データパケット"""
    id: str
    source: DataSource
    perspective: str
    content: Dict[str, Any]
    timestamp: datetime
    quality_score: float
    metadata: Dict[str, Any]

class UnifiedDataPipeline:
    """統合データパイプライン"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.realtime_buffer = []
        self.batch_queue = []
        self.processing_stats = {
            "realtime_processed": 0,
            "batch_processed": 0,
            "errors": 0,
            "quality_failures": 0
        }
        self.logger = logging.getLogger(__name__)
    
    async def process_data_stream(self, data_stream: List[DataPacket]) -> Dict[str, Any]:
        """データストリームの処理"""
        
        results = {
            "realtime_results": [],
            "batch_results": [],
            "processing_summary": {},
            "quality_report": {}
        }
        
        # データの分類と振り分け
        realtime_data, batch_data = self._classify_data(data_stream)
        
        # 並列処理の実行
        realtime_task = asyncio.create_task(
            self._process_realtime_data(realtime_data)
        )
        batch_task = asyncio.create_task(
            self._process_batch_data(batch_data)
        )
        
        # 結果の統合
        realtime_results = await realtime_task
        batch_results = await batch_task
        
        results["realtime_results"] = realtime_results
        results["batch_results"] = batch_results
        results["processing_summary"] = self._generate_processing_summary()
        results["quality_report"] = self._generate_quality_report()
        
        return results
    
    def _classify_data(self, data_stream: List[DataPacket]) -> tuple:
        """データの分類"""
        
        realtime_data = []
        batch_data = []
        
        for packet in data_stream:
            # 品質チェック
            if packet.quality_score < self.config.quality_threshold:
                self.processing_stats["quality_failures"] += 1
                continue
            
            # 緊急度による分類
            urgency_score = self._calculate_urgency(packet)
            
            if urgency_score > 0.7:  # 高緊急度
                realtime_data.append(packet)
            else:
                batch_data.append(packet)
        
        return realtime_data, batch_data
    
    async def _process_realtime_data(self, data: List[DataPacket]) -> List[Dict]:
        """リアルタイムデータ処理"""
        
        results = []
        
        for packet in data:
            try:
                # 高速処理パイプライン
                processed_result = await self._fast_processing_pipeline(packet)
                
                # 即座の洞察生成
                insights = await self._generate_immediate_insights(processed_result)
                
                result = {
                    "packet_id": packet.id,
                    "processed_data": processed_result,
                    "insights": insights,
                    "processing_time": datetime.utcnow(),
                    "latency_ms": self._calculate_latency(packet.timestamp)
                }
                
                results.append(result)
                self.processing_stats["realtime_processed"] += 1
                
            except Exception as e:
                self.logger.error(f"Realtime processing error: {e}")
                self.processing_stats["errors"] += 1
        
        return results
    
    async def _process_batch_data(self, data: List[DataPacket]) -> List[Dict]:
        """バッチデータ処理"""
        
        if not data:
            return []
        
        try:
            # データの前処理
            preprocessed_data = await self._preprocess_batch_data(data)
            
            # 包括的分析
            analysis_results = await self._comprehensive_analysis(preprocessed_data)
            
            # 深い洞察の生成
            deep_insights = await self._generate_deep_insights(analysis_results)
            
            # 予測モデルの更新
            model_updates = await self._update_prediction_models(analysis_results)
            
            result = {
                "batch_id": f"batch_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}",
                "data_count": len(data),
                "analysis_results": analysis_results,
                "deep_insights": deep_insights,
                "model_updates": model_updates,
                "processing_time": datetime.utcnow()
            }
            
            self.processing_stats["batch_processed"] += len(data)
            return [result]
            
        except Exception as e:
            self.logger.error(f"Batch processing error: {e}")
            self.processing_stats["errors"] += 1
            return []
    
    async def _fast_processing_pipeline(self, packet: DataPacket) -> Dict[str, Any]:
        """高速処理パイプライン"""
        
        # 基本的な特徴抽出
        features = {
            "text_length": len(packet.content.get("text", "")),
            "keyword_count": len(packet.content.get("keywords", [])),
            "sentiment_score": await self._quick_sentiment_analysis(packet.content),
            "relevance_score": await self._quick_relevance_scoring(packet),
            "urgency_indicators": await self._extract_urgency_indicators(packet.content)
        }
        
        # 簡易分類
        classification = await self._quick_classification(packet.content)
        
        # 重要度スコア計算
        importance_score = await self._calculate_importance_score(features, classification)
        
        return {
            "features": features,
            "classification": classification,
            "importance_score": importance_score,
            "processing_mode": "realtime"
        }
    
    async def _comprehensive_analysis(self, data: List[Dict]) -> Dict[str, Any]:
        """包括的分析"""
        
        # 高度な特徴抽出
        advanced_features = await self._extract_advanced_features(data)
        
        # 複雑な相関分析
        correlation_analysis = await self._perform_correlation_analysis(data)
        
        # トレンド分析
        trend_analysis = await self._perform_trend_analysis(data)
        
        # 異常検知
        anomaly_detection = await self._perform_anomaly_detection(data)
        
        # 予測分析
        prediction_analysis = await self._perform_prediction_analysis(data)
        
        return {
            "advanced_features": advanced_features,
            "correlation_analysis": correlation_analysis,
            "trend_analysis": trend_analysis,
            "anomaly_detection": anomaly_detection,
            "prediction_analysis": prediction_analysis,
            "processing_mode": "batch"
        }
    
    def _calculate_urgency(self, packet: DataPacket) -> float:
        """緊急度計算"""
        
        urgency_factors = {
            "time_sensitivity": 0.0,
            "market_impact": 0.0,
            "competitive_threat": 0.0,
            "regulatory_impact": 0.0
        }
        
        content = packet.content
        
        # 時間感応性の評価
        time_keywords = ["urgent", "immediate", "breaking", "alert", "crisis"]
        if any(keyword in content.get("text", "").lower() for keyword in time_keywords):
            urgency_factors["time_sensitivity"] = 0.8
        
        # 市場インパクトの評価
        market_keywords = ["market crash", "major acquisition", "regulatory change"]
        if any(keyword in content.get("text", "").lower() for keyword in market_keywords):
            urgency_factors["market_impact"] = 0.9
        
        # 競合脅威の評価
        competitive_keywords = ["competitor launch", "patent filed", "partnership"]
        if any(keyword in content.get("text", "").lower() for keyword in competitive_keywords):
            urgency_factors["competitive_threat"] = 0.7
        
        # 規制インパクトの評価
        regulatory_keywords = ["regulation", "compliance", "legal", "policy change"]
        if any(keyword in content.get("text", "").lower() for keyword in regulatory_keywords):
            urgency_factors["regulatory_impact"] = 0.6
        
        # 重み付き平均による総合緊急度
        weights = {"time_sensitivity": 0.4, "market_impact": 0.3, "competitive_threat": 0.2, "regulatory_impact": 0.1}
        total_urgency = sum(urgency_factors[factor] * weights[factor] for factor in urgency_factors)
        
        return min(total_urgency, 1.0)
    
    async def _quick_sentiment_analysis(self, content: Dict[str, Any]) -> float:
        """高速感情分析"""
        # 簡略化された実装
        text = content.get("text", "")
        positive_words = ["good", "excellent", "positive", "growth", "success"]
        negative_words = ["bad", "poor", "negative", "decline", "failure"]
        
        positive_count = sum(1 for word in positive_words if word in text.lower())
        negative_count = sum(1 for word in negative_words if word in text.lower())
        
        if positive_count + negative_count == 0:
            return 0.5  # 中性
        
        return positive_count / (positive_count + negative_count)
    
    async def _quick_relevance_scoring(self, packet: DataPacket) -> float:
        """高速関連性スコアリング"""
        # パースペクティブ別キーワードマッチング
        perspective_keywords = {
            "technology": ["AI", "machine learning", "blockchain", "cloud", "IoT"],
            "market": ["market share", "customer", "competition", "pricing", "demand"],
            "business": ["revenue", "profit", "strategy", "operations", "efficiency"]
        }
        
        text = packet.content.get("text", "").lower()
        keywords = perspective_keywords.get(packet.perspective, [])
        
        matches = sum(1 for keyword in keywords if keyword.lower() in text)
        return min(matches / len(keywords), 1.0) if keywords else 0.0
    
    async def _extract_urgency_indicators(self, content: Dict[str, Any]) -> List[str]:
        """緊急度指標の抽出"""
        text = content.get("text", "").lower()
        
        urgency_indicators = []
        
        # 時間表現
        time_expressions = ["today", "now", "immediately", "asap", "urgent"]
        urgency_indicators.extend([expr for expr in time_expressions if expr in text])
        
        # 影響度表現
        impact_expressions = ["critical", "major", "significant", "substantial"]
        urgency_indicators.extend([expr for expr in impact_expressions if expr in text])
        
        # 行動要求表現
        action_expressions = ["must", "should", "need to", "required"]
        urgency_indicators.extend([expr for expr in action_expressions if expr in text])
        
        return list(set(urgency_indicators))
    
    async def _quick_classification(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """高速分類"""
        text = content.get("text", "").lower()
        
        # カテゴリ分類
        categories = {
            "technology": ["technology", "innovation", "digital", "software", "hardware"],
            "market": ["market", "customer", "sales", "marketing", "competition"],
            "business": ["business", "strategy", "finance", "operations", "management"],
            "regulatory": ["regulation", "compliance", "legal", "policy", "government"]
        }
        
        category_scores = {}
        for category, keywords in categories.items():
            score = sum(1 for keyword in keywords if keyword in text)
            category_scores[category] = score / len(keywords)
        
        primary_category = max(category_scores, key=category_scores.get)
        
        return {
            "primary_category": primary_category,
            "category_scores": category_scores,
            "confidence": category_scores[primary_category]
        }
    
    async def _calculate_importance_score(
        self, 
        features: Dict[str, Any], 
        classification: Dict[str, Any]
    ) -> float:
        """重要度スコア計算"""
        
        # 特徴量ベースのスコア
        feature_score = (
            features["relevance_score"] * 0.3 +
            features["sentiment_score"] * 0.2 +
            len(features["urgency_indicators"]) * 0.1 +
            min(features["keyword_count"] / 10, 1.0) * 0.2
        )
        
        # 分類ベースのスコア
        classification_score = classification["confidence"] * 0.2
        
        # 総合重要度スコア
        total_score = feature_score + classification_score
        
        return min(total_score, 1.0)
    
    def _calculate_latency(self, original_timestamp: datetime) -> float:
        """レイテンシ計算（ミリ秒）"""
        return (datetime.utcnow() - original_timestamp).total_seconds() * 1000
    
    async def _generate_immediate_insights(self, processed_result: Dict[str, Any]) -> List[str]:
        """即座の洞察生成"""
        insights = []
        
        importance = processed_result["importance_score"]
        classification = processed_result["classification"]
        features = processed_result["features"]
        
        # 重要度ベースの洞察
        if importance > 0.8:
            insights.append("高重要度アイテム: 即座の注意が必要")
        elif importance > 0.6:
            insights.append("中重要度アイテム: 監視継続を推奨")
        
        # カテゴリベースの洞察
        primary_category = classification["primary_category"]
        if primary_category == "technology" and importance > 0.7:
            insights.append("技術的変化: 競争優位性への影響を評価")
        elif primary_category == "market" and importance > 0.7:
            insights.append("市場変化: 戦略調整の検討を推奨")
        elif primary_category == "regulatory" and importance > 0.6:
            insights.append("規制変化: コンプライアンス対応の確認")
        
        # 緊急度ベースの洞察
        if len(features["urgency_indicators"]) > 2:
            insights.append("緊急性指標検出: 迅速な対応を検討")
        
        return insights
    
    async def _preprocess_batch_data(self, data: List[DataPacket]) -> List[Dict]:
        """バッチデータの前処理"""
        preprocessed = []
        
        for packet in data:
            # データクリーニング
            cleaned_content = await self._clean_data(packet.content)
            
            # 特徴量エンジニアリング
            engineered_features = await self._engineer_features(cleaned_content)
            
            # 正規化
            normalized_data = await self._normalize_data(engineered_features)
            
            preprocessed.append({
                "original_packet": packet,
                "cleaned_content": cleaned_content,
                "engineered_features": engineered_features,
                "normalized_data": normalized_data
            })
        
        return preprocessed
    
    async def _clean_data(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """データクリーニング"""
        cleaned = content.copy()
        
        # テキストクリーニング
        if "text" in cleaned:
            text = cleaned["text"]
            # HTML タグの除去
            import re
            text = re.sub(r'<[^>]+>', '', text)
            # 特殊文字の正規化
            text = re.sub(r'\s+', ' ', text).strip()
            cleaned["text"] = text
        
        # 数値データの検証
        for key, value in cleaned.items():
            if isinstance(value, (int, float)) and (value < 0 or value > 1000000):
                cleaned[key] = None  # 異常値の除去
        
        return cleaned
    
    async def _engineer_features(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """特徴量エンジニアリング"""
        features = {}
        
        text = content.get("text", "")
        
        # テキスト特徴量
        features["text_length"] = len(text)
        features["word_count"] = len(text.split())
        features["sentence_count"] = len(text.split('.'))
        features["avg_word_length"] = sum(len(word) for word in text.split()) / max(len(text.split()), 1)
        
        # 感情特徴量
        features["sentiment_score"] = await self._quick_sentiment_analysis(content)
        
        # エンティティ特徴量
        entities = await self._extract_entities(text)
        features["entity_count"] = len(entities)
        features["entity_types"] = list(set(entity["type"] for entity in entities))
        
        # 時間特徴量
        if "timestamp" in content:
            timestamp = content["timestamp"]
            features["hour_of_day"] = timestamp.hour
            features["day_of_week"] = timestamp.weekday()
            features["is_weekend"] = timestamp.weekday() >= 5
        
        return features
    
    async def _extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """エンティティ抽出"""
        # 簡略化された実装
        entities = []
        
        # 企業名パターン
        import re
        company_pattern = r'\b[A-Z][a-z]+ (?:Inc|Corp|Ltd|LLC)\b'
        companies = re.findall(company_pattern, text)
        entities.extend([{"text": company, "type": "COMPANY"} for company in companies])
        
        # 数値パターン
        number_pattern = r'\b\d+(?:\.\d+)?%?\b'
        numbers = re.findall(number_pattern, text)
        entities.extend([{"text": number, "type": "NUMBER"} for number in numbers])
        
        return entities
    
    async def _normalize_data(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """データ正規化"""
        normalized = {}
        
        # 数値特徴量の正規化
        numeric_features = ["text_length", "word_count", "sentence_count", "avg_word_length"]
        
        for feature in numeric_features:
            if feature in features:
                value = features[feature]
                # Min-Max正規化（簡略化）
                normalized[feature] = min(value / 1000, 1.0)  # 適切な最大値で正規化
        
        # カテゴリ特徴量の保持
        categorical_features = ["entity_types", "is_weekend"]
        for feature in categorical_features:
            if feature in features:
                normalized[feature] = features[feature]
        
        return normalized
    
    async def _perform_correlation_analysis(self, data: List[Dict]) -> Dict[str, Any]:
        """相関分析"""
        # 簡略化された実装
        return {
            "feature_correlations": {
                "text_length_sentiment": 0.15,
                "entity_count_importance": 0.32,
                "weekend_urgency": -0.08
            },
            "perspective_correlations": {
                "technology_market": 0.45,
                "market_business": 0.67,
                "business_technology": 0.38
            }
        }
    
    async def _perform_trend_analysis(self, data: List[Dict]) -> Dict[str, Any]:
        """トレンド分析"""
        # 簡略化された実装
        return {
            "emerging_trends": [
                {"topic": "AI adoption", "growth_rate": 0.25, "confidence": 0.85},
                {"topic": "sustainability", "growth_rate": 0.18, "confidence": 0.78}
            ],
            "declining_trends": [
                {"topic": "legacy systems", "decline_rate": -0.12, "confidence": 0.72}
            ]
        }
    
    async def _perform_anomaly_detection(self, data: List[Dict]) -> Dict[str, Any]:
        """異常検知"""
        # 簡略化された実装
        return {
            "anomalies_detected": 3,
            "anomaly_types": ["unusual_volume", "sentiment_spike", "new_entity"],
            "severity_levels": {"high": 1, "medium": 1, "low": 1}
        }
    
    async def _perform_prediction_analysis(self, data: List[Dict]) -> Dict[str, Any]:
        """予測分析"""
        # 簡略化された実装
        return {
            "short_term_predictions": [
                {"metric": "market_volatility", "prediction": 0.65, "confidence": 0.78},
                {"metric": "technology_adoption", "prediction": 0.82, "confidence": 0.85}
            ],
            "long_term_predictions": [
                {"metric": "industry_transformation", "prediction": 0.75, "confidence": 0.68}
            ]
        }
    
    async def _generate_deep_insights(self, analysis_results: Dict[str, Any]) -> List[str]:
        """深い洞察の生成"""
        insights = []
        
        # トレンド分析からの洞察
        trends = analysis_results.get("trend_analysis", {})
        emerging = trends.get("emerging_trends", [])
        
        for trend in emerging:
            if trend["confidence"] > 0.8:
                insights.append(f"新興トレンド '{trend['topic']}' が高い成長率 ({trend['growth_rate']:.1%}) で拡大中")
        
        # 異常検知からの洞察
        anomalies = analysis_results.get("anomaly_detection", {})
        if anomalies.get("anomalies_detected", 0) > 0:
            insights.append(f"{anomalies['anomalies_detected']}件の異常を検出。詳細調査を推奨")
        
        # 予測分析からの洞察
        predictions = analysis_results.get("prediction_analysis", {})
        short_term = predictions.get("short_term_predictions", [])
        
        for prediction in short_term:
            if prediction["confidence"] > 0.75:
                insights.append(f"{prediction['metric']} の短期予測: {prediction['prediction']:.1%} (信頼度: {prediction['confidence']:.1%})")
        
        return insights
    
    async def _update_prediction_models(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """予測モデルの更新"""
        # 簡略化された実装
        return {
            "models_updated": ["sentiment_predictor", "trend_detector", "anomaly_detector"],
            "performance_improvements": {
                "sentiment_predictor": 0.03,
                "trend_detector": 0.05,
                "anomaly_detector": 0.02
            },
            "update_timestamp": datetime.utcnow().isoformat()
        }
    
    def _generate_processing_summary(self) -> Dict[str, Any]:
        """処理サマリーの生成"""
        total_processed = self.processing_stats["realtime_processed"] + self.processing_stats["batch_processed"]
        
        return {
            "total_processed": total_processed,
            "realtime_processed": self.processing_stats["realtime_processed"],
            "batch_processed": self.processing_stats["batch_processed"],
            "error_rate": self.processing_stats["errors"] / max(total_processed, 1),
            "quality_failure_rate": self.processing_stats["quality_failures"] / max(total_processed, 1),
            "processing_efficiency": (total_processed - self.processing_stats["errors"]) / max(total_processed, 1)
        }
    
    def _generate_quality_report(self) -> Dict[str, Any]:
        """品質レポートの生成"""
        return {
            "quality_threshold": self.config.quality_threshold,
            "quality_failures": self.processing_stats["quality_failures"],
            "quality_pass_rate": 1 - (self.processing_stats["quality_failures"] / max(
                self.processing_stats["realtime_processed"] + self.processing_stats["batch_processed"], 1
            )),
            "recommendations": self._generate_quality_recommendations()
        }
    
    def _generate_quality_recommendations(self) -> List[str]:
        """品質改善推奨事項の生成"""
        recommendations = []
        
        quality_pass_rate = 1 - (self.processing_stats["quality_failures"] / max(
            self.processing_stats["realtime_processed"] + self.processing_stats["batch_processed"], 1
        ))
        
        if quality_pass_rate < 0.9:
            recommendations.append("データ品質の向上が必要。ソースデータの検証を強化してください")
        
        if self.processing_stats["errors"] > 0:
            recommendations.append("処理エラーが発生。エラーハンドリングの改善を検討してください")
        
        if quality_pass_rate > 0.95:
            recommendations.append("データ品質は良好。現在の品質管理プロセスを維持してください")
        
        return recommendations

# 使用例
async def main():
    """メイン実行例"""
    
    # パイプライン設定
    config = PipelineConfig(
        name="triple_perspective_pipeline",
        processing_mode=ProcessingMode.HYBRID,
        data_sources=[DataSource.TECHNOLOGY, DataSource.MARKET, DataSource.BUSINESS],
        batch_interval=timedelta(hours=1),
        realtime_window=timedelta(minutes=5),
        quality_threshold=0.7,
        retry_attempts=3,
        timeout_seconds=30
    )
    
    # パイプラインの初期化
    pipeline = UnifiedDataPipeline(config)
    
    # サンプルデータの作成
    sample_data = [
        DataPacket(
            id="data_001",
            source=DataSource.TECHNOLOGY,
            perspective="technology",
            content={
                "text": "Breaking: Major AI breakthrough announced by leading tech company",
                "keywords": ["AI", "breakthrough", "technology"],
                "timestamp": datetime.utcnow()
            },
            timestamp=datetime.utcnow(),
            quality_score=0.9,
            metadata={"source_url": "https://example.com/news/1"}
        ),
        DataPacket(
            id="data_002",
            source=DataSource.MARKET,
            perspective="market",
            content={
                "text": "Market analysis shows steady growth in renewable energy sector",
                "keywords": ["market", "growth", "renewable energy"],
                "timestamp": datetime.utcnow()
            },
            timestamp=datetime.utcnow(),
            quality_score=0.8,
            metadata={"source_url": "https://example.com/market/1"}
        )
    ]
    
    # パイプライン実行
    results = await pipeline.process_data_stream(sample_data)
    
    print("Processing Results:")
    print(f"Realtime Results: {len(results['realtime_results'])}")
    print(f"Batch Results: {len(results['batch_results'])}")
    print(f"Processing Summary: {results['processing_summary']}")
    print(f"Quality Report: {results['quality_report']}")

if __name__ == "__main__":
    asyncio.run(main())
```

### データ品質管理とバリデーション

データの戦略的価値は、その品質と信頼性に直接依存します。我々のデータパイプラインは、包括的な品質管理とバリデーション機能を統合し、高品質なデータフローを保証します。

#### 多層品質管理アーキテクチャ

**入力層品質管理**では、データソースからの入力時点での品質チェックを実行します。データ形式の検証、必須フィールドの確認、値の範囲チェック、重複検出などを自動化します。

**処理層品質管理**では、データ変換・加工プロセスでの品質維持を確保します。変換ロジックの検証、中間結果の整合性チェック、処理エラーの検出・回復を実装します。

**出力層品質管理**では、最終結果の品質保証を実行します。結果の完全性、一貫性、正確性の検証、ビジネスルールの適合性チェックを行います。

```python
# data_pipeline/quality_management/quality_controller.py
from dataclasses import dataclass
from typing import Dict, List, Any, Optional, Tuple
from enum import Enum
import re
import statistics
from datetime import datetime, timedelta
import logging

class QualityLevel(Enum):
    EXCELLENT = "excellent"  # 95%以上
    GOOD = "good"           # 85-95%
    ACCEPTABLE = "acceptable"  # 70-85%
    POOR = "poor"           # 50-70%
    UNACCEPTABLE = "unacceptable"  # 50%未満

class QualityDimension(Enum):
    COMPLETENESS = "completeness"
    ACCURACY = "accuracy"
    CONSISTENCY = "consistency"
    VALIDITY = "validity"
    TIMELINESS = "timeliness"
    UNIQUENESS = "uniqueness"

@dataclass
class QualityRule:
    """品質ルール定義"""
    name: str
    dimension: QualityDimension
    description: str
    validation_function: str
    threshold: float
    weight: float
    is_critical: bool

@dataclass
class QualityResult:
    """品質評価結果"""
    dimension: QualityDimension
    score: float
    level: QualityLevel
    passed_rules: int
    failed_rules: int
    details: Dict[str, Any]

class DataQualityController:
    """データ品質管理コントローラー"""
    
    def __init__(self):
        self.quality_rules = self._initialize_quality_rules()
        self.quality_history = []
        self.logger = logging.getLogger(__name__)
    
    def _initialize_quality_rules(self) -> List[QualityRule]:
        """品質ルールの初期化"""
        
        return [
            # 完全性ルール
            QualityRule(
                name="required_fields_present",
                dimension=QualityDimension.COMPLETENESS,
                description="必須フィールドの存在確認",
                validation_function="check_required_fields",
                threshold=1.0,
                weight=0.3,
                is_critical=True
            ),
            QualityRule(
                name="content_not_empty",
                dimension=QualityDimension.COMPLETENESS,
                description="コンテンツの非空確認",
                validation_function="check_content_not_empty",
                threshold=0.95,
                weight=0.2,
                is_critical=True
            ),
            
            # 正確性ルール
            QualityRule(
                name="data_format_valid",
                dimension=QualityDimension.ACCURACY,
                description="データ形式の妥当性確認",
                validation_function="check_data_format",
                threshold=0.98,
                weight=0.25,
                is_critical=True
            ),
            QualityRule(
                name="numerical_range_valid",
                dimension=QualityDimension.ACCURACY,
                description="数値範囲の妥当性確認",
                validation_function="check_numerical_range",
                threshold=0.95,
                weight=0.15,
                is_critical=False
            ),
            
            # 一貫性ルール
            QualityRule(
                name="cross_field_consistency",
                dimension=QualityDimension.CONSISTENCY,
                description="フィールド間の一貫性確認",
                validation_function="check_cross_field_consistency",
                threshold=0.90,
                weight=0.2,
                is_critical=False
            ),
            QualityRule(
                name="temporal_consistency",
                dimension=QualityDimension.CONSISTENCY,
                description="時系列データの一貫性確認",
                validation_function="check_temporal_consistency",
                threshold=0.85,
                weight=0.15,
                is_critical=False
            ),
            
            # 妥当性ルール
            QualityRule(
                name="business_rule_compliance",
                dimension=QualityDimension.VALIDITY,
                description="ビジネスルールの適合性確認",
                validation_function="check_business_rules",
                threshold=0.90,
                weight=0.25,
                is_critical=True
            ),
            
            # 適時性ルール
            QualityRule(
                name="data_freshness",
                dimension=QualityDimension.TIMELINESS,
                description="データの新鮮度確認",
                validation_function="check_data_freshness",
                threshold=0.80,
                weight=0.1,
                is_critical=False
            ),
            
            # 一意性ルール
            QualityRule(
                name="duplicate_detection",
                dimension=QualityDimension.UNIQUENESS,
                description="重複データの検出",
                validation_function="check_duplicates",
                threshold=0.95,
                weight=0.15,
                is_critical=False
            )
        ]
    
    async def evaluate_data_quality(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """データ品質の評価"""
        
        if not data:
            return self._create_empty_quality_report()
        
        # 各品質次元の評価
        dimension_results = {}
        
        for dimension in QualityDimension:
            dimension_rules = [rule for rule in self.quality_rules if rule.dimension == dimension]
            dimension_result = await self._evaluate_dimension(data, dimension_rules)
            dimension_results[dimension.value] = dimension_result
        
        # 総合品質スコアの計算
        overall_score = self._calculate_overall_score(dimension_results)
        overall_level = self._determine_quality_level(overall_score)
        
        # 品質レポートの生成
        quality_report = {
            "overall_score": overall_score,
            "overall_level": overall_level.value,
            "dimension_results": dimension_results,
            "data_count": len(data),
            "evaluation_timestamp": datetime.utcnow().isoformat(),
            "recommendations": self._generate_quality_recommendations(dimension_results),
            "critical_issues": self._identify_critical_issues(dimension_results)
        }
        
        # 履歴の更新
        self.quality_history.append(quality_report)
        
        return quality_report
    
    async def _evaluate_dimension(
        self, 
        data: List[Dict[str, Any]], 
        rules: List[QualityRule]
    ) -> QualityResult:
        """品質次元の評価"""
        
        if not rules:
            return QualityResult(
                dimension=QualityDimension.COMPLETENESS,
                score=1.0,
                level=QualityLevel.EXCELLENT,
                passed_rules=0,
                failed_rules=0,
                details={}
            )
        
        dimension = rules[0].dimension
        rule_results = {}
        passed_rules = 0
        failed_rules = 0
        
        # 各ルールの評価
        for rule in rules:
            try:
                rule_score = await self._evaluate_rule(data, rule)
                rule_results[rule.name] = {
                    "score": rule_score,
                    "threshold": rule.threshold,
                    "passed": rule_score >= rule.threshold,
                    "weight": rule.weight,
                    "is_critical": rule.is_critical
                }
                
                if rule_score >= rule.threshold:
                    passed_rules += 1
                else:
                    failed_rules += 1
                    
            except Exception as e:
                self.logger.error(f"Rule evaluation error for {rule.name}: {e}")
                rule_results[rule.name] = {
                    "score": 0.0,
                    "threshold": rule.threshold,
                    "passed": False,
                    "weight": rule.weight,
                    "is_critical": rule.is_critical,
                    "error": str(e)
                }
                failed_rules += 1
        
        # 次元スコアの計算（重み付き平均）
        total_weight = sum(rule.weight for rule in rules)
        weighted_score = sum(
            rule_results[rule.name]["score"] * rule.weight 
            for rule in rules
        ) / total_weight
        
        # 品質レベルの決定
        quality_level = self._determine_quality_level(weighted_score)
        
        return QualityResult(
            dimension=dimension,
            score=weighted_score,
            level=quality_level,
            passed_rules=passed_rules,
            failed_rules=failed_rules,
            details=rule_results
        )
    
    async def _evaluate_rule(self, data: List[Dict[str, Any]], rule: QualityRule) -> float:
        """個別ルールの評価"""
        
        validation_method = getattr(self, rule.validation_function, None)
        if not validation_method:
            raise ValueError(f"Validation function {rule.validation_function} not found")
        
        return await validation_method(data, rule)
    
    async def check_required_fields(self, data: List[Dict[str, Any]], rule: QualityRule) -> float:
        """必須フィールドの存在確認"""
        
        required_fields = ["id", "content", "timestamp", "source"]
        total_records = len(data)
        valid_records = 0
        
        for record in data:
            if all(field in record and record[field] is not None for field in required_fields):
                valid_records += 1
        
        return valid_records / total_records if total_records > 0 else 0.0
    
    async def check_content_not_empty(self, data: List[Dict[str, Any]], rule: QualityRule) -> float:
        """コンテンツの非空確認"""
        
        total_records = len(data)
        valid_records = 0
        
        for record in data:
            content = record.get("content", {})
            if isinstance(content, dict):
                text = content.get("text", "")
                if text and len(text.strip()) > 0:
                    valid_records += 1
            elif isinstance(content, str) and len(content.strip()) > 0:
                valid_records += 1
        
        return valid_records / total_records if total_records > 0 else 0.0
    
    async def check_data_format(self, data: List[Dict[str, Any]], rule: QualityRule) -> float:
        """データ形式の妥当性確認"""
        
        total_records = len(data)
        valid_records = 0
        
        for record in data:
            is_valid = True
            
            # ID形式の確認
            if "id" in record:
                id_value = record["id"]
                if not isinstance(id_value, str) or len(id_value) == 0:
                    is_valid = False
            
            # タイムスタンプ形式の確認
            if "timestamp" in record:
                timestamp = record["timestamp"]
                if not isinstance(timestamp, (datetime, str)):
                    is_valid = False
                elif isinstance(timestamp, str):
                    try:
                        datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    except ValueError:
                        is_valid = False
            
            # 数値フィールドの確認
            numeric_fields = ["quality_score", "relevance_score", "importance_score"]
            for field in numeric_fields:
                if field in record:
                    value = record[field]
                    if value is not None and not isinstance(value, (int, float)):
                        is_valid = False
                    elif isinstance(value, (int, float)) and (value < 0 or value > 1):
                        is_valid = False
            
            if is_valid:
                valid_records += 1
        
        return valid_records / total_records if total_records > 0 else 0.0
    
    async def check_numerical_range(self, data: List[Dict[str, Any]], rule: QualityRule) -> float:
        """数値範囲の妥当性確認"""
        
        total_checks = 0
        valid_checks = 0
        
        for record in data:
            # スコア系フィールドの範囲確認（0-1）
            score_fields = ["quality_score", "relevance_score", "importance_score", "confidence_score"]
            
            for field in score_fields:
                if field in record and record[field] is not None:
                    total_checks += 1
                    value = record[field]
                    if isinstance(value, (int, float)) and 0 <= value <= 1:
                        valid_checks += 1
            
            # その他の数値フィールドの範囲確認
            if "word_count" in record and record["word_count"] is not None:
                total_checks += 1
                if isinstance(record["word_count"], int) and record["word_count"] >= 0:
                    valid_checks += 1
        
        return valid_checks / total_checks if total_checks > 0 else 1.0
    
    async def check_cross_field_consistency(self, data: List[Dict[str, Any]], rule: QualityRule) -> float:
        """フィールド間の一貫性確認"""
        
        total_records = len(data)
        consistent_records = 0
        
        for record in data:
            is_consistent = True
            
            # コンテンツとメタデータの一貫性
            content = record.get("content", {})
            if isinstance(content, dict):
                text = content.get("text", "")
                word_count = record.get("word_count")
                
                if word_count is not None and text:
                    actual_word_count = len(text.split())
                    # 10%の誤差を許容
                    if abs(actual_word_count - word_count) > actual_word_count * 0.1:
                        is_consistent = False
            
            # スコア間の論理的一貫性
            quality_score = record.get("quality_score")
            relevance_score = record.get("relevance_score")
            importance_score = record.get("importance_score")
            
            if all(score is not None for score in [quality_score, relevance_score, importance_score]):
                # 重要度は品質と関連性の組み合わせを反映すべき
                expected_importance = (quality_score + relevance_score) / 2
                if abs(importance_score - expected_importance) > 0.3:
                    is_consistent = False
            
            if is_consistent:
                consistent_records += 1
        
        return consistent_records / total_records if total_records > 0 else 1.0
    
    async def check_temporal_consistency(self, data: List[Dict[str, Any]], rule: QualityRule) -> float:
        """時系列データの一貫性確認"""
        
        if len(data) < 2:
            return 1.0  # データが少ない場合は一貫性ありとみなす
        
        # タイムスタンプでソート
        sorted_data = sorted(
            [record for record in data if "timestamp" in record],
            key=lambda x: x["timestamp"] if isinstance(x["timestamp"], datetime) 
            else datetime.fromisoformat(x["timestamp"].replace('Z', '+00:00'))
        )
        
        if len(sorted_data) < 2:
            return 1.0
        
        consistent_pairs = 0
        total_pairs = len(sorted_data) - 1
        
        for i in range(total_pairs):
            current = sorted_data[i]
            next_record = sorted_data[i + 1]
            
            # タイムスタンプの順序確認
            current_time = current["timestamp"]
            next_time = next_record["timestamp"]
            
            if isinstance(current_time, str):
                current_time = datetime.fromisoformat(current_time.replace('Z', '+00:00'))
            if isinstance(next_time, str):
                next_time = datetime.fromisoformat(next_time.replace('Z', '+00:00'))
            
            # 時系列順序の確認
            if current_time <= next_time:
                consistent_pairs += 1
        
        return consistent_pairs / total_pairs if total_pairs > 0 else 1.0
    
    async def check_business_rules(self, data: List[Dict[str, Any]], rule: QualityRule) -> float:
        """ビジネスルールの適合性確認"""
        
        total_records = len(data)
        compliant_records = 0
        
        for record in data:
            is_compliant = True
            
            # ルール1: 高品質データは高い関連性を持つべき
            quality_score = record.get("quality_score")
            relevance_score = record.get("relevance_score")
            
            if quality_score is not None and relevance_score is not None:
                if quality_score > 0.8 and relevance_score < 0.3:
                    is_compliant = False
            
            # ルール2: 緊急度の高いデータは重要度も高いべき
            urgency_indicators = record.get("urgency_indicators", [])
            importance_score = record.get("importance_score")
            
            if len(urgency_indicators) > 2 and importance_score is not None:
                if importance_score < 0.6:
                    is_compliant = False
            
            # ルール3: 技術系データは特定のキーワードを含むべき
            perspective = record.get("perspective")
            content = record.get("content", {})
            
            if perspective == "technology" and isinstance(content, dict):
                text = content.get("text", "").lower()
                tech_keywords = ["technology", "innovation", "digital", "ai", "software", "hardware"]
                if not any(keyword in text for keyword in tech_keywords):
                    is_compliant = False
            
            if is_compliant:
                compliant_records += 1
        
        return compliant_records / total_records if total_records > 0 else 1.0
    
    async def check_data_freshness(self, data: List[Dict[str, Any]], rule: QualityRule) -> float:
        """データの新鮮度確認"""
        
        current_time = datetime.utcnow()
        total_records = len(data)
        fresh_records = 0
        
        # 新鮮度の閾値（24時間）
        freshness_threshold = timedelta(hours=24)
        
        for record in data:
            timestamp = record.get("timestamp")
            if timestamp:
                if isinstance(timestamp, str):
                    try:
                        timestamp = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    except ValueError:
                        continue
                
                age = current_time - timestamp.replace(tzinfo=None)
                if age <= freshness_threshold:
                    fresh_records += 1
        
        return fresh_records / total_records if total_records > 0 else 0.0
    
    async def check_duplicates(self, data: List[Dict[str, Any]], rule: QualityRule) -> float:
        """重複データの検出"""
        
        if len(data) <= 1:
            return 1.0
        
        # コンテンツハッシュによる重複検出
        content_hashes = set()
        unique_records = 0
        
        for record in data:
            content = record.get("content", {})
            if isinstance(content, dict):
                text = content.get("text", "")
            else:
                text = str(content)
            
            # 簡単なハッシュ生成（実際の実装ではより堅牢なハッシュを使用）
            content_hash = hash(text.strip().lower())
            
            if content_hash not in content_hashes:
                content_hashes.add(content_hash)
                unique_records += 1
        
        return unique_records / len(data)
    
    def _calculate_overall_score(self, dimension_results: Dict[str, QualityResult]) -> float:
        """総合品質スコアの計算"""
        
        # 次元別重み
        dimension_weights = {
            QualityDimension.COMPLETENESS.value: 0.25,
            QualityDimension.ACCURACY.value: 0.25,
            QualityDimension.CONSISTENCY.value: 0.15,
            QualityDimension.VALIDITY.value: 0.20,
            QualityDimension.TIMELINESS.value: 0.10,
            QualityDimension.UNIQUENESS.value: 0.05
        }
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for dimension_name, result in dimension_results.items():
            weight = dimension_weights.get(dimension_name, 0.0)
            weighted_sum += result.score * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    def _determine_quality_level(self, score: float) -> QualityLevel:
        """品質レベルの決定"""
        
        if score >= 0.95:
            return QualityLevel.EXCELLENT
        elif score >= 0.85:
            return QualityLevel.GOOD
        elif score >= 0.70:
            return QualityLevel.ACCEPTABLE
        elif score >= 0.50:
            return QualityLevel.POOR
        else:
            return QualityLevel.UNACCEPTABLE
    
    def _generate_quality_recommendations(self, dimension_results: Dict[str, QualityResult]) -> List[str]:
        """品質改善推奨事項の生成"""
        
        recommendations = []
        
        for dimension_name, result in dimension_results.items():
            if result.level in [QualityLevel.POOR, QualityLevel.UNACCEPTABLE]:
                if dimension_name == QualityDimension.COMPLETENESS.value:
                    recommendations.append("データの完全性向上: 必須フィールドの入力を徹底してください")
                elif dimension_name == QualityDimension.ACCURACY.value:
                    recommendations.append("データの正確性向上: 入力値の検証を強化してください")
                elif dimension_name == QualityDimension.CONSISTENCY.value:
                    recommendations.append("データの一貫性向上: フィールド間の整合性チェックを実装してください")
                elif dimension_name == QualityDimension.VALIDITY.value:
                    recommendations.append("ビジネスルール適合性向上: データ検証ルールを見直してください")
                elif dimension_name == QualityDimension.TIMELINESS.value:
                    recommendations.append("データの適時性向上: データ収集頻度を増加してください")
                elif dimension_name == QualityDimension.UNIQUENESS.value:
                    recommendations.append("重複データ削減: 重複検出・除去プロセスを強化してください")
        
        # 全体的な推奨事項
        overall_score = self._calculate_overall_score(dimension_results)
        if overall_score < 0.70:
            recommendations.append("総合的な品質向上が必要: データ管理プロセス全体の見直しを推奨します")
        
        return recommendations
    
    def _identify_critical_issues(self, dimension_results: Dict[str, QualityResult]) -> List[str]:
        """重要な問題の特定"""
        
        critical_issues = []
        
        for dimension_name, result in dimension_results.items():
            # クリティカルルールの失敗を確認
            for rule_name, rule_result in result.details.items():
                if rule_result.get("is_critical", False) and not rule_result.get("passed", True):
                    critical_issues.append(
                        f"クリティカル品質ルール '{rule_name}' が失敗 "
                        f"(スコア: {rule_result['score']:.2f}, 閾値: {rule_result['threshold']:.2f})"
                    )
        
        return critical_issues
    
    def _create_empty_quality_report(self) -> Dict[str, Any]:
        """空のデータに対する品質レポート"""
        
        return {
            "overall_score": 0.0,
            "overall_level": QualityLevel.UNACCEPTABLE.value,
            "dimension_results": {},
            "data_count": 0,
            "evaluation_timestamp": datetime.utcnow().isoformat(),
            "recommendations": ["データが存在しません。データ収集プロセスを確認してください。"],
            "critical_issues": ["データが空です"]
        }
    
    def get_quality_trends(self, days: int = 7) -> Dict[str, Any]:
        """品質トレンドの取得"""
        
        if not self.quality_history:
            return {"message": "品質履歴データがありません"}
        
        # 指定期間のデータを抽出
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        recent_history = [
            report for report in self.quality_history
            if datetime.fromisoformat(report["evaluation_timestamp"]) >= cutoff_date
        ]
        
        if not recent_history:
            return {"message": f"過去{days}日間の品質データがありません"}
        
        # トレンド分析
        scores = [report["overall_score"] for report in recent_history]
        
        trend_analysis = {
            "period_days": days,
            "evaluations_count": len(recent_history),
            "average_score": statistics.mean(scores),
            "min_score": min(scores),
            "max_score": max(scores),
            "score_trend": "improving" if scores[-1] > scores[0] else "declining" if scores[-1] < scores[0] else "stable",
            "score_variance": statistics.variance(scores) if len(scores) > 1 else 0.0
        }
        
        return trend_analysis

# 使用例
async def main():
    """品質管理の使用例"""
    
    quality_controller = DataQualityController()
    
    # サンプルデータ
    sample_data = [
        {
            "id": "data_001",
            "content": {"text": "AI technology breakthrough in machine learning"},
            "timestamp": datetime.utcnow(),
            "source": "tech_news",
            "perspective": "technology",
            "quality_score": 0.9,
            "relevance_score": 0.8,
            "importance_score": 0.85,
            "word_count": 7
        },
        {
            "id": "data_002",
            "content": {"text": "Market analysis shows growth"},
            "timestamp": datetime.utcnow() - timedelta(hours=2),
            "source": "market_data",
            "perspective": "market",
            "quality_score": 0.7,
            "relevance_score": 0.6,
            "importance_score": 0.65,
            "word_count": 5
        }
    ]
    
    # 品質評価の実行
    quality_report = await quality_controller.evaluate_data_quality(sample_data)
    
    print("Data Quality Report:")
    print(f"Overall Score: {quality_report['overall_score']:.2f}")
    print(f"Overall Level: {quality_report['overall_level']}")
    print(f"Data Count: {quality_report['data_count']}")
    
    print("\nDimension Results:")
    for dimension, result in quality_report['dimension_results'].items():
        print(f"  {dimension}: {result.score:.2f} ({result.level.value})")
    
    print("\nRecommendations:")
    for recommendation in quality_report['recommendations']:
        print(f"  - {recommendation}")
    
    if quality_report['critical_issues']:
        print("\nCritical Issues:")
        for issue in quality_report['critical_issues']:
            print(f"  ! {issue}")

if __name__ == "__main__":
    asyncio.run(main())
```

### 効率的なデータフロー設計

戦略的意思決定の速度と品質を最大化するため、我々のデータフロー設計は、最適化されたデータ移動、変換、配信を実現します。

#### ストリーミング・ファースト・アーキテクチャ

**イベント駆動データフロー**では、データの変更や新規追加を即座に検知し、関連するプロセスに自動的に伝播します。Apache Kafka を中核とするイベントストリーミング基盤により、リアルタイムデータフローを実現します。

**マイクロバッチ処理**では、ストリーミングとバッチ処理の利点を統合し、低遅延と高スループットを両立します。Apache Spark Structured Streaming により、秒単位のマイクロバッチでの連続処理を実現します。

**バックプレッシャー制御**では、システム負荷に応じた動的なフロー制御により、安定した性能を維持します。下流システムの処理能力に応じて、上流のデータ生成速度を自動調整します。

この包括的なデータパイプライン設計により、トリプルパースペクティブ型戦略AIレーダーは、高品質で信頼性の高いデータフローを実現し、戦略的意思決定の基盤となる優れたデータ基盤を提供します。


## データモデル設計

トリプルパースペクティブ型戦略AIレーダーの効果的な運用には、3つの視点（技術・市場・ビジネス）を統合し、一貫性を保ちながら柔軟性を提供するデータモデルが不可欠です。本セクションでは、包括的で拡張可能なデータモデル設計を詳細に解説します。

### 統合データモデルの詳細化

我々の統合データモデルは、ドメイン駆動設計（DDD）の原則に基づき、各視点の独立性を保ちながら、戦略的洞察の統合を可能にする設計を採用します。

#### コアエンティティ設計

**戦略的洞察エンティティ（StrategicInsight）**は、システムの中核となるドメインオブジェクトです。各視点からの分析結果を統合し、意思決定に必要な情報を包括的に提供します。

**視点エンティティ（Perspective）**は、技術・市場・ビジネスの各視点を表現し、それぞれの専門領域における分析ロジックと評価基準を管理します。

**コンセンサスエンティティ（Consensus）**は、複数視点間の合意形成プロセスと結果を管理し、静止点検出と意思決定支援を実現します。

```python
# data_model/core_entities/strategic_insight.py
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Union
from enum import Enum
from datetime import datetime, timedelta
import uuid
from decimal import Decimal

class PerspectiveType(Enum):
    TECHNOLOGY = "technology"
    MARKET = "market"
    BUSINESS = "business"

class InsightStatus(Enum):
    DRAFT = "draft"
    ANALYZING = "analyzing"
    CONSENSUS_PENDING = "consensus_pending"
    VALIDATED = "validated"
    IMPLEMENTED = "implemented"
    ARCHIVED = "archived"

class ConfidenceLevel(Enum):
    VERY_HIGH = "very_high"  # 90%以上
    HIGH = "high"           # 75-90%
    MEDIUM = "medium"       # 50-75%
    LOW = "low"            # 25-50%
    VERY_LOW = "very_low"  # 25%未満

class UrgencyLevel(Enum):
    CRITICAL = "critical"   # 即座の対応が必要
    HIGH = "high"          # 1週間以内
    MEDIUM = "medium"      # 1ヶ月以内
    LOW = "low"           # 3ヶ月以内
    PLANNING = "planning"  # 長期計画

@dataclass
class MetricValue:
    """メトリック値"""
    name: str
    value: Union[float, int, str, bool]
    unit: Optional[str] = None
    confidence: float = 0.0
    timestamp: datetime = field(default_factory=datetime.utcnow)
    source: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PerspectiveAnalysis:
    """視点別分析結果"""
    perspective_type: PerspectiveType
    analysis_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    
    # 基本評価指標
    importance_score: float = 0.0
    confidence_score: float = 0.0
    urgency_score: float = 0.0
    impact_score: float = 0.0
    
    # 詳細メトリクス
    metrics: List[MetricValue] = field(default_factory=list)
    
    # 分析結果
    key_findings: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    risks: List[str] = field(default_factory=list)
    opportunities: List[str] = field(default_factory=list)
    
    # メタデータ
    analysis_timestamp: datetime = field(default_factory=datetime.utcnow)
    analyst_id: Optional[str] = None
    data_sources: List[str] = field(default_factory=list)
    analysis_method: Optional[str] = None
    quality_score: float = 0.0
    
    # 関連情報
    related_insights: List[str] = field(default_factory=list)
    external_references: List[str] = field(default_factory=list)
    
    def add_metric(self, name: str, value: Union[float, int, str, bool], 
                   unit: Optional[str] = None, confidence: float = 1.0, 
                   source: Optional[str] = None) -> None:
        """メトリックの追加"""
        metric = MetricValue(
            name=name,
            value=value,
            unit=unit,
            confidence=confidence,
            source=source
        )
        self.metrics.append(metric)
    
    def get_metric_value(self, name: str) -> Optional[Union[float, int, str, bool]]:
        """メトリック値の取得"""
        for metric in self.metrics:
            if metric.name == name:
                return metric.value
        return None
    
    def calculate_overall_score(self) -> float:
        """総合スコアの計算"""
        weights = {
            'importance': 0.3,
            'confidence': 0.2,
            'urgency': 0.2,
            'impact': 0.3
        }
        
        return (
            self.importance_score * weights['importance'] +
            self.confidence_score * weights['confidence'] +
            self.urgency_score * weights['urgency'] +
            self.impact_score * weights['impact']
        )

@dataclass
class ConsensusResult:
    """コンセンサス結果"""
    consensus_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    
    # 合意形成結果
    consensus_score: float = 0.0
    convergence_achieved: bool = False
    equilibrium_point: Optional[Dict[str, float]] = None
    
    # 視点間整合性
    perspective_alignment: Dict[str, float] = field(default_factory=dict)
    conflict_areas: List[str] = field(default_factory=list)
    agreement_areas: List[str] = field(default_factory=list)
    
    # 統合推奨事項
    integrated_recommendations: List[str] = field(default_factory=list)
    priority_actions: List[str] = field(default_factory=list)
    
    # プロセス情報
    consensus_timestamp: datetime = field(default_factory=datetime.utcnow)
    iterations_count: int = 0
    convergence_time: Optional[timedelta] = None
    
    # 品質指標
    consensus_quality: float = 0.0
    stability_score: float = 0.0
    robustness_score: float = 0.0

@dataclass
class StrategicInsight:
    """戦略的洞察エンティティ"""
    insight_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    
    # 基本情報
    title: str = ""
    description: str = ""
    category: str = ""
    tags: List[str] = field(default_factory=list)
    
    # ステータス管理
    status: InsightStatus = InsightStatus.DRAFT
    confidence_level: ConfidenceLevel = ConfidenceLevel.MEDIUM
    urgency_level: UrgencyLevel = UrgencyLevel.MEDIUM
    
    # 視点別分析
    technology_analysis: Optional[PerspectiveAnalysis] = None
    market_analysis: Optional[PerspectiveAnalysis] = None
    business_analysis: Optional[PerspectiveAnalysis] = None
    
    # コンセンサス結果
    consensus_result: Optional[ConsensusResult] = None
    
    # 統合評価
    overall_importance: float = 0.0
    overall_confidence: float = 0.0
    overall_urgency: float = 0.0
    overall_impact: float = 0.0
    
    # 時系列情報
    created_timestamp: datetime = field(default_factory=datetime.utcnow)
    updated_timestamp: datetime = field(default_factory=datetime.utcnow)
    validation_timestamp: Optional[datetime] = None
    implementation_timestamp: Optional[datetime] = None
    
    # 関係性
    parent_insight_id: Optional[str] = None
    child_insight_ids: List[str] = field(default_factory=list)
    related_insight_ids: List[str] = field(default_factory=list)
    
    # メタデータ
    creator_id: Optional[str] = None
    stakeholder_ids: List[str] = field(default_factory=list)
    data_lineage: List[str] = field(default_factory=list)
    version: int = 1
    
    def add_perspective_analysis(self, analysis: PerspectiveAnalysis) -> None:
        """視点別分析の追加"""
        if analysis.perspective_type == PerspectiveType.TECHNOLOGY:
            self.technology_analysis = analysis
        elif analysis.perspective_type == PerspectiveType.MARKET:
            self.market_analysis = analysis
        elif analysis.perspective_type == PerspectiveType.BUSINESS:
            self.business_analysis = analysis
        
        self._update_overall_scores()
        self.updated_timestamp = datetime.utcnow()
    
    def _update_overall_scores(self) -> None:
        """総合スコアの更新"""
        analyses = [
            self.technology_analysis,
            self.market_analysis,
            self.business_analysis
        ]
        
        valid_analyses = [a for a in analyses if a is not None]
        
        if not valid_analyses:
            return
        
        # 各指標の平均値を計算
        self.overall_importance = sum(a.importance_score for a in valid_analyses) / len(valid_analyses)
        self.overall_confidence = sum(a.confidence_score for a in valid_analyses) / len(valid_analyses)
        self.overall_urgency = sum(a.urgency_score for a in valid_analyses) / len(valid_analyses)
        self.overall_impact = sum(a.impact_score for a in valid_analyses) / len(valid_analyses)
        
        # 信頼度レベルの更新
        self.confidence_level = self._determine_confidence_level(self.overall_confidence)
        
        # 緊急度レベルの更新
        self.urgency_level = self._determine_urgency_level(self.overall_urgency)
    
    def _determine_confidence_level(self, confidence_score: float) -> ConfidenceLevel:
        """信頼度レベルの決定"""
        if confidence_score >= 0.9:
            return ConfidenceLevel.VERY_HIGH
        elif confidence_score >= 0.75:
            return ConfidenceLevel.HIGH
        elif confidence_score >= 0.5:
            return ConfidenceLevel.MEDIUM
        elif confidence_score >= 0.25:
            return ConfidenceLevel.LOW
        else:
            return ConfidenceLevel.VERY_LOW
    
    def _determine_urgency_level(self, urgency_score: float) -> UrgencyLevel:
        """緊急度レベルの決定"""
        if urgency_score >= 0.9:
            return UrgencyLevel.CRITICAL
        elif urgency_score >= 0.7:
            return UrgencyLevel.HIGH
        elif urgency_score >= 0.5:
            return UrgencyLevel.MEDIUM
        elif urgency_score >= 0.3:
            return UrgencyLevel.LOW
        else:
            return UrgencyLevel.PLANNING
    
    def set_consensus_result(self, consensus: ConsensusResult) -> None:
        """コンセンサス結果の設定"""
        self.consensus_result = consensus
        
        if consensus.convergence_achieved:
            self.status = InsightStatus.VALIDATED
            self.validation_timestamp = datetime.utcnow()
        
        self.updated_timestamp = datetime.utcnow()
    
    def get_all_perspectives(self) -> List[PerspectiveAnalysis]:
        """全視点分析の取得"""
        perspectives = []
        if self.technology_analysis:
            perspectives.append(self.technology_analysis)
        if self.market_analysis:
            perspectives.append(self.market_analysis)
        if self.business_analysis:
            perspectives.append(self.business_analysis)
        return perspectives
    
    def is_complete(self) -> bool:
        """分析完了状態の確認"""
        return all([
            self.technology_analysis is not None,
            self.market_analysis is not None,
            self.business_analysis is not None
        ])
    
    def get_completion_percentage(self) -> float:
        """完了率の取得"""
        completed_analyses = len(self.get_all_perspectives())
        return (completed_analyses / 3.0) * 100
    
    def calculate_strategic_value(self) -> float:
        """戦略的価値の計算"""
        if not self.is_complete():
            return 0.0
        
        # 戦略的価値 = (重要度 × 影響度 × 信頼度) - (緊急度による割引)
        base_value = self.overall_importance * self.overall_impact * self.overall_confidence
        
        # 緊急度による調整（高緊急度は短期価値、低緊急度は長期価値）
        urgency_factor = 1.0 + (self.overall_urgency - 0.5) * 0.2
        
        return base_value * urgency_factor
    
    def generate_executive_summary(self) -> str:
        """エグゼクティブサマリーの生成"""
        if not self.is_complete():
            return "分析が完了していません。"
        
        summary_parts = [
            f"戦略的洞察: {self.title}",
            f"総合評価: 重要度{self.overall_importance:.1%}, 信頼度{self.overall_confidence:.1%}, 緊急度{self.overall_urgency:.1%}",
            f"戦略的価値: {self.calculate_strategic_value():.2f}"
        ]
        
        if self.consensus_result and self.consensus_result.convergence_achieved:
            summary_parts.append(f"コンセンサス達成: {self.consensus_result.consensus_score:.1%}")
            
            if self.consensus_result.integrated_recommendations:
                summary_parts.append("主要推奨事項:")
                for i, rec in enumerate(self.consensus_result.integrated_recommendations[:3], 1):
                    summary_parts.append(f"  {i}. {rec}")
        
        return "\n".join(summary_parts)

# データアクセス層の設計
class StrategicInsightRepository:
    """戦略的洞察リポジトリ"""
    
    def __init__(self, database_connection):
        self.db = database_connection
    
    async def save(self, insight: StrategicInsight) -> str:
        """洞察の保存"""
        # データベース保存ロジック
        pass
    
    async def find_by_id(self, insight_id: str) -> Optional[StrategicInsight]:
        """IDによる検索"""
        # データベース検索ロジック
        pass
    
    async def find_by_status(self, status: InsightStatus) -> List[StrategicInsight]:
        """ステータスによる検索"""
        # ステータス検索ロジック
        pass
    
    async def find_by_urgency(self, urgency: UrgencyLevel) -> List[StrategicInsight]:
        """緊急度による検索"""
        # 緊急度検索ロジック
        pass
    
    async def find_related_insights(self, insight_id: str) -> List[StrategicInsight]:
        """関連洞察の検索"""
        # 関連性検索ロジック
        pass
    
    async def search_by_criteria(self, criteria: Dict[str, Any]) -> List[StrategicInsight]:
        """条件による検索"""
        # 複合条件検索ロジック
        pass

# 使用例
def create_sample_insight() -> StrategicInsight:
    """サンプル洞察の作成"""
    
    # 戦略的洞察の作成
    insight = StrategicInsight(
        title="AI技術導入による市場競争力強化",
        description="機械学習技術の導入により、製品開発効率と市場対応速度の向上を図る戦略的施策",
        category="technology_adoption",
        tags=["AI", "machine_learning", "competitive_advantage", "product_development"]
    )
    
    # 技術視点分析
    tech_analysis = PerspectiveAnalysis(
        perspective_type=PerspectiveType.TECHNOLOGY,
        importance_score=0.85,
        confidence_score=0.78,
        urgency_score=0.72,
        impact_score=0.88,
        key_findings=[
            "機械学習技術の成熟度が実用レベルに到達",
            "既存システムとの統合が技術的に実現可能",
            "必要な技術人材の確保が課題"
        ],
        recommendations=[
            "段階的なAI技術導入計画の策定",
            "技術人材の採用・育成プログラムの実施",
            "POCによる技術検証の実行"
        ]
    )
    
    tech_analysis.add_metric("技術成熟度", 0.85, "スコア", 0.9)
    tech_analysis.add_metric("実装コスト", 50000000, "円", 0.8)
    tech_analysis.add_metric("開発期間", 12, "ヶ月", 0.85)
    
    # 市場視点分析
    market_analysis = PerspectiveAnalysis(
        perspective_type=PerspectiveType.MARKET,
        importance_score=0.82,
        confidence_score=0.75,
        urgency_score=0.68,
        impact_score=0.85,
        key_findings=[
            "AI活用企業の市場シェア拡大傾向",
            "顧客のAI機能に対する期待値上昇",
            "競合他社のAI投資加速"
        ],
        recommendations=[
            "AI機能を活用した差別化戦略の策定",
            "顧客ニーズに基づく機能優先順位の決定",
            "競合分析に基づく市場ポジショニング"
        ]
    )
    
    market_analysis.add_metric("市場成長率", 0.25, "年率", 0.8)
    market_analysis.add_metric("顧客満足度向上", 0.30, "予測値", 0.7)
    market_analysis.add_metric("市場シェア拡大", 0.15, "予測値", 0.75)
    
    # ビジネス視点分析
    business_analysis = PerspectiveAnalysis(
        perspective_type=PerspectiveType.BUSINESS,
        importance_score=0.88,
        confidence_score=0.82,
        urgency_score=0.75,
        impact_score=0.92,
        key_findings=[
            "ROI 280%の投資効果を予測",
            "3年間での投資回収が可能",
            "組織変革による間接効果も期待"
        ],
        recommendations=[
            "段階的投資による リスク分散",
            "ROI測定指標の明確化",
            "組織変革管理の並行実施"
        ]
    )
    
    business_analysis.add_metric("予測ROI", 2.8, "倍", 0.85)
    business_analysis.add_metric("投資回収期間", 36, "ヶ月", 0.8)
    business_analysis.add_metric("売上向上", 0.40, "予測値", 0.78)
    
    # 分析結果の追加
    insight.add_perspective_analysis(tech_analysis)
    insight.add_perspective_analysis(market_analysis)
    insight.add_perspective_analysis(business_analysis)
    
    return insight

if __name__ == "__main__":
    # サンプル実行
    sample_insight = create_sample_insight()
    print("Strategic Insight Created:")
    print(f"ID: {sample_insight.insight_id}")
    print(f"Title: {sample_insight.title}")
    print(f"Completion: {sample_insight.get_completion_percentage():.1f}%")
    print(f"Strategic Value: {sample_insight.calculate_strategic_value():.2f}")
    print("\nExecutive Summary:")
    print(sample_insight.generate_executive_summary())
```

### 視点間データ連携設計

3つの視点間での効果的なデータ連携は、統合的な戦略分析の核心です。我々の設計では、各視点の独立性を保ちながら、相互の影響と依存関係を適切に管理します。

#### 視点間相互作用モデル

**技術-市場連携**では、技術的実現可能性と市場受容性の相互検証を行います。技術的革新が市場機会を創出し、市場ニーズが技術開発方向を決定する双方向の関係を管理します。

**市場-ビジネス連携**では、市場動向とビジネス戦略の整合性を確保します。市場分析結果がビジネス戦略の調整を促し、ビジネス目標が市場分析の焦点を決定します。

**ビジネス-技術連携**では、ビジネス要件と技術制約のバランスを最適化します。ビジネス価値の最大化と技術的実現可能性の両立を図ります。

```python
# data_model/perspective_integration/cross_perspective_analyzer.py
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Tuple
from enum import Enum
import numpy as np
from datetime import datetime
import logging

class InteractionType(Enum):
    SYNERGY = "synergy"           # 相乗効果
    CONFLICT = "conflict"         # 競合・矛盾
    DEPENDENCY = "dependency"     # 依存関係
    INDEPENDENCE = "independence" # 独立性
    COMPLEMENTARY = "complementary" # 補完関係

class InfluenceDirection(Enum):
    BIDIRECTIONAL = "bidirectional"
    TECH_TO_MARKET = "tech_to_market"
    MARKET_TO_TECH = "market_to_tech"
    MARKET_TO_BUSINESS = "market_to_business"
    BUSINESS_TO_MARKET = "business_to_market"
    BUSINESS_TO_TECH = "business_to_tech"
    TECH_TO_BUSINESS = "tech_to_business"

@dataclass
class PerspectiveInteraction:
    """視点間相互作用"""
    interaction_id: str
    source_perspective: PerspectiveType
    target_perspective: PerspectiveType
    interaction_type: InteractionType
    influence_direction: InfluenceDirection
    
    # 相互作用の強度
    influence_strength: float = 0.0  # 0.0-1.0
    confidence_level: float = 0.0    # 0.0-1.0
    
    # 相互作用の詳細
    description: str = ""
    impact_areas: List[str] = field(default_factory=list)
    risk_factors: List[str] = field(default_factory=list)
    opportunity_factors: List[str] = field(default_factory=list)
    
    # 定量的指標
    correlation_coefficient: float = 0.0
    causality_strength: float = 0.0
    time_lag: Optional[int] = None  # 日数
    
    # メタデータ
    detected_timestamp: datetime = field(default_factory=datetime.utcnow)
    last_updated: datetime = field(default_factory=datetime.utcnow)
    validation_status: str = "pending"

@dataclass
class CrossPerspectiveMetric:
    """視点横断メトリック"""
    metric_name: str
    tech_value: Optional[float] = None
    market_value: Optional[float] = None
    business_value: Optional[float] = None
    
    # 統合値
    integrated_value: Optional[float] = None
    integration_method: str = "weighted_average"
    
    # 重み
    tech_weight: float = 0.33
    market_weight: float = 0.33
    business_weight: float = 0.34
    
    # 品質指標
    consistency_score: float = 0.0
    reliability_score: float = 0.0
    
    def calculate_integrated_value(self) -> float:
        """統合値の計算"""
        values = []
        weights = []
        
        if self.tech_value is not None:
            values.append(self.tech_value)
            weights.append(self.tech_weight)
        
        if self.market_value is not None:
            values.append(self.market_value)
            weights.append(self.market_weight)
        
        if self.business_value is not None:
            values.append(self.business_value)
            weights.append(self.business_weight)
        
        if not values:
            return 0.0
        
        # 重み正規化
        total_weight = sum(weights)
        normalized_weights = [w / total_weight for w in weights]
        
        # 重み付き平均
        self.integrated_value = sum(v * w for v, w in zip(values, normalized_weights))
        return self.integrated_value
    
    def calculate_consistency_score(self) -> float:
        """一貫性スコアの計算"""
        values = [v for v in [self.tech_value, self.market_value, self.business_value] if v is not None]
        
        if len(values) < 2:
            self.consistency_score = 1.0
            return self.consistency_score
        
        # 標準偏差による一貫性評価
        mean_value = np.mean(values)
        std_dev = np.std(values)
        
        # 一貫性スコア = 1 - (標準偏差 / 平均値)
        if mean_value > 0:
            self.consistency_score = max(0.0, 1.0 - (std_dev / mean_value))
        else:
            self.consistency_score = 1.0 if std_dev == 0 else 0.0
        
        return self.consistency_score

class CrossPerspectiveAnalyzer:
    """視点横断分析器"""
    
    def __init__(self):
        self.interactions: List[PerspectiveInteraction] = []
        self.cross_metrics: List[CrossPerspectiveMetric] = []
        self.logger = logging.getLogger(__name__)
    
    async def analyze_perspective_interactions(
        self, 
        insight: StrategicInsight
    ) -> List[PerspectiveInteraction]:
        """視点間相互作用の分析"""
        
        if not insight.is_complete():
            self.logger.warning("Incomplete insight provided for interaction analysis")
            return []
        
        interactions = []
        
        # 技術-市場相互作用
        tech_market_interaction = await self._analyze_tech_market_interaction(
            insight.technology_analysis, 
            insight.market_analysis
        )
        if tech_market_interaction:
            interactions.append(tech_market_interaction)
        
        # 市場-ビジネス相互作用
        market_business_interaction = await self._analyze_market_business_interaction(
            insight.market_analysis, 
            insight.business_analysis
        )
        if market_business_interaction:
            interactions.append(market_business_interaction)
        
        # ビジネス-技術相互作用
        business_tech_interaction = await self._analyze_business_tech_interaction(
            insight.business_analysis, 
            insight.technology_analysis
        )
        if business_tech_interaction:
            interactions.append(business_tech_interaction)
        
        self.interactions.extend(interactions)
        return interactions
    
    async def _analyze_tech_market_interaction(
        self, 
        tech_analysis: PerspectiveAnalysis, 
        market_analysis: PerspectiveAnalysis
    ) -> Optional[PerspectiveInteraction]:
        """技術-市場相互作用の分析"""
        
        # 相関分析
        correlation = self._calculate_perspective_correlation(tech_analysis, market_analysis)
        
        # 相互作用タイプの決定
        interaction_type = self._determine_interaction_type(correlation, tech_analysis, market_analysis)
        
        # 影響方向の分析
        influence_direction = await self._analyze_influence_direction(tech_analysis, market_analysis)
        
        # 相互作用の詳細分析
        impact_areas = await self._identify_tech_market_impact_areas(tech_analysis, market_analysis)
        risks = await self._identify_tech_market_risks(tech_analysis, market_analysis)
        opportunities = await self._identify_tech_market_opportunities(tech_analysis, market_analysis)
        
        interaction = PerspectiveInteraction(
            interaction_id=f"tech_market_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}",
            source_perspective=PerspectiveType.TECHNOLOGY,
            target_perspective=PerspectiveType.MARKET,
            interaction_type=interaction_type,
            influence_direction=influence_direction,
            influence_strength=abs(correlation),
            confidence_level=min(tech_analysis.confidence_score, market_analysis.confidence_score),
            description=f"技術視点と市場視点の相互作用（相関係数: {correlation:.3f}）",
            impact_areas=impact_areas,
            risk_factors=risks,
            opportunity_factors=opportunities,
            correlation_coefficient=correlation
        )
        
        return interaction
    
    async def _analyze_market_business_interaction(
        self, 
        market_analysis: PerspectiveAnalysis, 
        business_analysis: PerspectiveAnalysis
    ) -> Optional[PerspectiveInteraction]:
        """市場-ビジネス相互作用の分析"""
        
        correlation = self._calculate_perspective_correlation(market_analysis, business_analysis)
        interaction_type = self._determine_interaction_type(correlation, market_analysis, business_analysis)
        influence_direction = await self._analyze_influence_direction(market_analysis, business_analysis)
        
        impact_areas = await self._identify_market_business_impact_areas(market_analysis, business_analysis)
        risks = await self._identify_market_business_risks(market_analysis, business_analysis)
        opportunities = await self._identify_market_business_opportunities(market_analysis, business_analysis)
        
        interaction = PerspectiveInteraction(
            interaction_id=f"market_business_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}",
            source_perspective=PerspectiveType.MARKET,
            target_perspective=PerspectiveType.BUSINESS,
            interaction_type=interaction_type,
            influence_direction=influence_direction,
            influence_strength=abs(correlation),
            confidence_level=min(market_analysis.confidence_score, business_analysis.confidence_score),
            description=f"市場視点とビジネス視点の相互作用（相関係数: {correlation:.3f}）",
            impact_areas=impact_areas,
            risk_factors=risks,
            opportunity_factors=opportunities,
            correlation_coefficient=correlation
        )
        
        return interaction
    
    async def _analyze_business_tech_interaction(
        self, 
        business_analysis: PerspectiveAnalysis, 
        tech_analysis: PerspectiveAnalysis
    ) -> Optional[PerspectiveInteraction]:
        """ビジネス-技術相互作用の分析"""
        
        correlation = self._calculate_perspective_correlation(business_analysis, tech_analysis)
        interaction_type = self._determine_interaction_type(correlation, business_analysis, tech_analysis)
        influence_direction = await self._analyze_influence_direction(business_analysis, tech_analysis)
        
        impact_areas = await self._identify_business_tech_impact_areas(business_analysis, tech_analysis)
        risks = await self._identify_business_tech_risks(business_analysis, tech_analysis)
        opportunities = await self._identify_business_tech_opportunities(business_analysis, tech_analysis)
        
        interaction = PerspectiveInteraction(
            interaction_id=f"business_tech_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}",
            source_perspective=PerspectiveType.BUSINESS,
            target_perspective=PerspectiveType.TECHNOLOGY,
            interaction_type=interaction_type,
            influence_direction=influence_direction,
            influence_strength=abs(correlation),
            confidence_level=min(business_analysis.confidence_score, tech_analysis.confidence_score),
            description=f"ビジネス視点と技術視点の相互作用（相関係数: {correlation:.3f}）",
            impact_areas=impact_areas,
            risk_factors=risks,
            opportunity_factors=opportunities,
            correlation_coefficient=correlation
        )
        
        return interaction
    
    def _calculate_perspective_correlation(
        self, 
        analysis1: PerspectiveAnalysis, 
        analysis2: PerspectiveAnalysis
    ) -> float:
        """視点間相関の計算"""
        
        # 基本スコアの相関
        scores1 = [
            analysis1.importance_score,
            analysis1.confidence_score,
            analysis1.urgency_score,
            analysis1.impact_score
        ]
        
        scores2 = [
            analysis2.importance_score,
            analysis2.confidence_score,
            analysis2.urgency_score,
            analysis2.impact_score
        ]
        
        # ピアソン相関係数の計算
        correlation = np.corrcoef(scores1, scores2)[0, 1]
        
        # NaNの場合は0を返す
        return 0.0 if np.isnan(correlation) else correlation
    
    def _determine_interaction_type(
        self, 
        correlation: float, 
        analysis1: PerspectiveAnalysis, 
        analysis2: PerspectiveAnalysis
    ) -> InteractionType:
        """相互作用タイプの決定"""
        
        if abs(correlation) < 0.1:
            return InteractionType.INDEPENDENCE
        elif correlation > 0.7:
            return InteractionType.SYNERGY
        elif correlation < -0.7:
            return InteractionType.CONFLICT
        elif 0.3 <= abs(correlation) <= 0.7:
            return InteractionType.COMPLEMENTARY
        else:
            return InteractionType.DEPENDENCY
    
    async def _analyze_influence_direction(
        self, 
        analysis1: PerspectiveAnalysis, 
        analysis2: PerspectiveAnalysis
    ) -> InfluenceDirection:
        """影響方向の分析"""
        
        # 簡略化された実装
        # 実際の実装では、より詳細な因果関係分析を行う
        
        type1 = analysis1.perspective_type
        type2 = analysis2.perspective_type
        
        if type1 == PerspectiveType.TECHNOLOGY and type2 == PerspectiveType.MARKET:
            return InfluenceDirection.BIDIRECTIONAL
        elif type1 == PerspectiveType.MARKET and type2 == PerspectiveType.BUSINESS:
            return InfluenceDirection.BIDIRECTIONAL
        elif type1 == PerspectiveType.BUSINESS and type2 == PerspectiveType.TECHNOLOGY:
            return InfluenceDirection.BIDIRECTIONAL
        else:
            return InfluenceDirection.BIDIRECTIONAL
    
    async def _identify_tech_market_impact_areas(
        self, 
        tech_analysis: PerspectiveAnalysis, 
        market_analysis: PerspectiveAnalysis
    ) -> List[str]:
        """技術-市場影響領域の特定"""
        
        impact_areas = []
        
        # 技術成熟度と市場受容性
        tech_maturity = tech_analysis.get_metric_value("技術成熟度")
        if tech_maturity and tech_maturity > 0.8:
            impact_areas.append("技術成熟度による市場参入機会")
        
        # 市場成長率と技術投資
        market_growth = market_analysis.get_metric_value("市場成長率")
        if market_growth and market_growth > 0.2:
            impact_areas.append("市場成長による技術投資促進")
        
        # 共通的な影響領域
        impact_areas.extend([
            "製品開発サイクルの最適化",
            "顧客価値提案の技術的差別化",
            "市場ニーズに基づく技術ロードマップ"
        ])
        
        return impact_areas
    
    async def _identify_tech_market_risks(
        self, 
        tech_analysis: PerspectiveAnalysis, 
        market_analysis: PerspectiveAnalysis
    ) -> List[str]:
        """技術-市場リスクの特定"""
        
        risks = []
        
        # 技術と市場のミスマッチリスク
        if tech_analysis.confidence_score < 0.6 or market_analysis.confidence_score < 0.6:
            risks.append("技術-市場適合性の不確実性")
        
        # タイミングリスク
        if abs(tech_analysis.urgency_score - market_analysis.urgency_score) > 0.3:
            risks.append("技術開発と市場タイミングの不整合")
        
        risks.extend([
            "競合技術による市場シェア侵食",
            "技術標準化の遅れによる市場機会損失",
            "市場変化による技術投資の陳腐化"
        ])
        
        return risks
    
    async def _identify_tech_market_opportunities(
        self, 
        tech_analysis: PerspectiveAnalysis, 
        market_analysis: PerspectiveAnalysis
    ) -> List[str]:
        """技術-市場機会の特定"""
        
        opportunities = []
        
        # 高い技術・市場スコアの組み合わせ
        if tech_analysis.impact_score > 0.8 and market_analysis.impact_score > 0.8:
            opportunities.append("技術革新による新市場創出")
        
        # 相乗効果の機会
        if tech_analysis.importance_score > 0.7 and market_analysis.importance_score > 0.7:
            opportunities.append("技術-市場シナジーによる競争優位性確立")
        
        opportunities.extend([
            "技術プラットフォーム化による市場拡大",
            "市場フィードバックによる技術改善加速",
            "技術ライセンシングによる新収益源創出"
        ])
        
        return opportunities
    
    async def _identify_market_business_impact_areas(
        self, 
        market_analysis: PerspectiveAnalysis, 
        business_analysis: PerspectiveAnalysis
    ) -> List[str]:
        """市場-ビジネス影響領域の特定"""
        
        return [
            "収益モデルの最適化",
            "顧客セグメント戦略の調整",
            "価格戦略の市場適応",
            "チャネル戦略の効率化",
            "ブランドポジショニングの強化"
        ]
    
    async def _identify_market_business_risks(
        self, 
        market_analysis: PerspectiveAnalysis, 
        business_analysis: PerspectiveAnalysis
    ) -> List[str]:
        """市場-ビジネスリスクの特定"""
        
        return [
            "市場変化による収益性悪化",
            "競合激化による利益率低下",
            "顧客ニーズ変化への対応遅れ",
            "市場投資回収期間の延長",
            "ブランド価値の市場評価低下"
        ]
    
    async def _identify_market_business_opportunities(
        self, 
        market_analysis: PerspectiveAnalysis, 
        business_analysis: PerspectiveAnalysis
    ) -> List[str]:
        """市場-ビジネス機会の特定"""
        
        return [
            "新市場セグメントへの参入",
            "価値提案の差別化強化",
            "収益源の多様化",
            "顧客生涯価値の向上",
            "市場シェア拡大による規模効果"
        ]
    
    async def _identify_business_tech_impact_areas(
        self, 
        business_analysis: PerspectiveAnalysis, 
        tech_analysis: PerspectiveAnalysis
    ) -> List[str]:
        """ビジネス-技術影響領域の特定"""
        
        return [
            "業務プロセスの自動化・効率化",
            "技術投資のROI最適化",
            "デジタル変革の推進",
            "技術人材の戦略的活用",
            "技術資産の価値最大化"
        ]
    
    async def _identify_business_tech_risks(
        self, 
        business_analysis: PerspectiveAnalysis, 
        tech_analysis: PerspectiveAnalysis
    ) -> List[str]:
        """ビジネス-技術リスクの特定"""
        
        return [
            "技術投資の回収不能リスク",
            "技術的負債の蓄積",
            "技術人材不足による競争力低下",
            "レガシーシステムの維持コスト増大",
            "技術標準変化への対応遅れ"
        ]
    
    async def _identify_business_tech_opportunities(
        self, 
        business_analysis: PerspectiveAnalysis, 
        tech_analysis: PerspectiveAnalysis
    ) -> List[str]:
        """ビジネス-技術機会の特定"""
        
        return [
            "技術革新による新ビジネスモデル創出",
            "デジタル技術による顧客体験向上",
            "AI・自動化による生産性向上",
            "データ活用による意思決定高度化",
            "技術パートナーシップによる能力拡張"
        ]
    
    async def create_cross_perspective_metrics(
        self, 
        insight: StrategicInsight
    ) -> List[CrossPerspectiveMetric]:
        """視点横断メトリクスの作成"""
        
        if not insight.is_complete():
            return []
        
        metrics = []
        
        # 重要度メトリック
        importance_metric = CrossPerspectiveMetric(
            metric_name="統合重要度",
            tech_value=insight.technology_analysis.importance_score,
            market_value=insight.market_analysis.importance_score,
            business_value=insight.business_analysis.importance_score
        )
        importance_metric.calculate_integrated_value()
        importance_metric.calculate_consistency_score()
        metrics.append(importance_metric)
        
        # 信頼度メトリック
        confidence_metric = CrossPerspectiveMetric(
            metric_name="統合信頼度",
            tech_value=insight.technology_analysis.confidence_score,
            market_value=insight.market_analysis.confidence_score,
            business_value=insight.business_analysis.confidence_score
        )
        confidence_metric.calculate_integrated_value()
        confidence_metric.calculate_consistency_score()
        metrics.append(confidence_metric)
        
        # 緊急度メトリック
        urgency_metric = CrossPerspectiveMetric(
            metric_name="統合緊急度",
            tech_value=insight.technology_analysis.urgency_score,
            market_value=insight.market_analysis.urgency_score,
            business_value=insight.business_analysis.urgency_score
        )
        urgency_metric.calculate_integrated_value()
        urgency_metric.calculate_consistency_score()
        metrics.append(urgency_metric)
        
        # 影響度メトリック
        impact_metric = CrossPerspectiveMetric(
            metric_name="統合影響度",
            tech_value=insight.technology_analysis.impact_score,
            market_value=insight.market_analysis.impact_score,
            business_value=insight.business_analysis.impact_score
        )
        impact_metric.calculate_integrated_value()
        impact_metric.calculate_consistency_score()
        metrics.append(impact_metric)
        
        self.cross_metrics.extend(metrics)
        return metrics
    
    def generate_integration_report(self, insight: StrategicInsight) -> Dict[str, Any]:
        """統合レポートの生成"""
        
        interactions = [i for i in self.interactions if i.source_perspective in [
            insight.technology_analysis.perspective_type if insight.technology_analysis else None,
            insight.market_analysis.perspective_type if insight.market_analysis else None,
            insight.business_analysis.perspective_type if insight.business_analysis else None
        ]]
        
        metrics = [m for m in self.cross_metrics]
        
        report = {
            "insight_id": insight.insight_id,
            "integration_summary": {
                "total_interactions": len(interactions),
                "synergy_count": len([i for i in interactions if i.interaction_type == InteractionType.SYNERGY]),
                "conflict_count": len([i for i in interactions if i.interaction_type == InteractionType.CONFLICT]),
                "average_correlation": np.mean([i.correlation_coefficient for i in interactions]) if interactions else 0.0
            },
            "perspective_alignment": {
                "importance_consistency": next((m.consistency_score for m in metrics if m.metric_name == "統合重要度"), 0.0),
                "confidence_consistency": next((m.consistency_score for m in metrics if m.metric_name == "統合信頼度"), 0.0),
                "urgency_consistency": next((m.consistency_score for m in metrics if m.metric_name == "統合緊急度"), 0.0),
                "impact_consistency": next((m.consistency_score for m in metrics if m.metric_name == "統合影響度"), 0.0)
            },
            "integration_quality": self._calculate_integration_quality(interactions, metrics),
            "recommendations": self._generate_integration_recommendations(interactions, metrics)
        }
        
        return report
    
    def _calculate_integration_quality(
        self, 
        interactions: List[PerspectiveInteraction], 
        metrics: List[CrossPerspectiveMetric]
    ) -> float:
        """統合品質の計算"""
        
        if not interactions or not metrics:
            return 0.0
        
        # 相互作用の品質
        interaction_quality = np.mean([i.confidence_level for i in interactions])
        
        # メトリクスの一貫性
        consistency_quality = np.mean([m.consistency_score for m in metrics])
        
        # 統合品質 = (相互作用品質 + 一貫性品質) / 2
        return (interaction_quality + consistency_quality) / 2
    
    def _generate_integration_recommendations(
        self, 
        interactions: List[PerspectiveInteraction], 
        metrics: List[CrossPerspectiveMetric]
    ) -> List[str]:
        """統合推奨事項の生成"""
        
        recommendations = []
        
        # 競合の解決
        conflicts = [i for i in interactions if i.interaction_type == InteractionType.CONFLICT]
        if conflicts:
            recommendations.append(f"{len(conflicts)}件の視点間競合の解決が必要")
        
        # 一貫性の改善
        low_consistency_metrics = [m for m in metrics if m.consistency_score < 0.7]
        if low_consistency_metrics:
            recommendations.append(f"{len(low_consistency_metrics)}項目で視点間一貫性の改善が必要")
        
        # 相乗効果の活用
        synergies = [i for i in interactions if i.interaction_type == InteractionType.SYNERGY]
        if synergies:
            recommendations.append(f"{len(synergies)}件の相乗効果を積極的に活用")
        
        return recommendations

# 使用例
async def main():
    """視点間データ連携の使用例"""
    
    # サンプル洞察の作成
    insight = create_sample_insight()
    
    # 視点横断分析器の初期化
    analyzer = CrossPerspectiveAnalyzer()
    
    # 視点間相互作用の分析
    interactions = await analyzer.analyze_perspective_interactions(insight)
    
    # 視点横断メトリクスの作成
    metrics = await analyzer.create_cross_perspective_metrics(insight)
    
    # 統合レポートの生成
    integration_report = analyzer.generate_integration_report(insight)
    
    print("Cross-Perspective Analysis Results:")
    print(f"Interactions found: {len(interactions)}")
    print(f"Cross-perspective metrics: {len(metrics)}")
    print(f"Integration quality: {integration_report['integration_quality']:.2f}")
    
    print("\nPerspective Alignment:")
    for metric, score in integration_report['perspective_alignment'].items():
        print(f"  {metric}: {score:.2f}")
    
    print("\nRecommendations:")
    for rec in integration_report['recommendations']:
        print(f"  - {rec}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### データバージョニング戦略

戦略的洞察の進化と改善を適切に管理するため、包括的なデータバージョニング戦略を実装します。

#### 時系列データ管理

**スナップショット管理**では、各時点での完全なデータ状態を保存し、過去の分析結果の再現性を確保します。重要な意思決定時点でのデータ状態を完全に復元可能にします。

**差分管理**では、変更内容のみを効率的に記録し、ストレージ効率と変更履歴の追跡を両立します。Git のような分散バージョン管理の概念をデータ管理に適用します。

**ブランチ管理**では、異なる分析シナリオや仮説検証を並行して実行できる環境を提供します。本流の分析を継続しながら、実験的な分析を安全に実行できます。

この包括的なデータモデル設計により、トリプルパースペクティブ型戦略AIレーダーは、3つの視点を効果的に統合し、一貫性と柔軟性を両立した戦略的洞察を提供します。

