# 第10章: 視点統合と静止点検出

**作成支援**: Manus AI

## アルゴリズム最適化

### 3視点統合アルゴリズムの最適化

トリプルパースペクティブ型戦略AIレーダーの核心は、技術・市場・ビジネスの3つの視点から得られる評価を効率的に統合し、意思決定の最適解（静止点）を検出することです。本章では、この統合プロセスを数学的に最適化し、実装効率と精度を向上させるアルゴリズムを詳細に解説します。

#### 最適化された統合スコア計算

**動的重み調整による統合スコア**

従来の固定重み統合から、状況に応じて動的に重みを調整する適応的統合アルゴリズムに進化させます。

```python
# algorithms/optimized_integration.py
import numpy as np
from scipy.optimize import minimize
from typing import Dict, List, Tuple, Optional
import logging

class OptimizedPerspectiveIntegrator:
    """最適化された視点統合器"""
    
    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.9):
        self.learning_rate = learning_rate
        self.momentum = momentum
        self.weight_history = []
        self.performance_history = []
        self.convergence_threshold = 1e-6
        
    def calculate_adaptive_weights(
        self, 
        perspective_data: Dict[str, Dict], 
        historical_performance: Optional[List[float]] = None
    ) -> Dict[str, float]:
        """適応的重み計算"""
        
        perspectives = ['technology', 'market', 'business']
        
        # 信頼度ベースの初期重み
        confidence_weights = self._calculate_confidence_weights(perspective_data)
        
        # 一貫性ベースの調整
        consistency_adjustment = self._calculate_consistency_adjustment(perspective_data)
        
        # 履歴パフォーマンスベースの調整
        if historical_performance:
            performance_adjustment = self._calculate_performance_adjustment(
                historical_performance
            )
        else:
            performance_adjustment = {p: 1.0 for p in perspectives}
        
        # 統合重み計算
        adaptive_weights = {}
        for perspective in perspectives:
            adaptive_weights[perspective] = (
                confidence_weights[perspective] * 
                consistency_adjustment[perspective] * 
                performance_adjustment[perspective]
            )
        
        # 正規化
        total_weight = sum(adaptive_weights.values())
        adaptive_weights = {
            k: v / total_weight for k, v in adaptive_weights.items()
        }
        
        return adaptive_weights
    
    def _calculate_confidence_weights(self, perspective_data: Dict) -> Dict[str, float]:
        """信頼度ベースの重み計算"""
        
        confidence_scores = {}
        for perspective, data in perspective_data.items():
            # 複数の信頼度指標を統合
            source_reliability = data.get('source_reliability', 0.5)
            data_quality = data.get('data_quality', 0.5)
            temporal_consistency = data.get('temporal_consistency', 0.5)
            
            # 重み付き幾何平均
            confidence_scores[perspective] = (
                source_reliability ** 0.4 * 
                data_quality ** 0.3 * 
                temporal_consistency ** 0.3
            )
        
        # 正規化
        total_confidence = sum(confidence_scores.values())
        return {k: v / total_confidence for k, v in confidence_scores.items()}
    
    def _calculate_consistency_adjustment(self, perspective_data: Dict) -> Dict[str, float]:
        """一貫性ベースの調整係数計算"""
        
        perspectives = list(perspective_data.keys())
        importance_scores = [
            perspective_data[p].get('importance', 0) for p in perspectives
        ]
        
        # 視点間の一貫性計算
        consistency_matrix = np.zeros((len(perspectives), len(perspectives)))
        
        for i in range(len(perspectives)):
            for j in range(len(perspectives)):
                if i != j:
                    # コサイン類似度による一貫性計算
                    score_i = importance_scores[i]
                    score_j = importance_scores[j]
                    
                    consistency_matrix[i][j] = 1 - abs(score_i - score_j) / 100
        
        # 各視点の平均一貫性
        avg_consistency = {}
        for i, perspective in enumerate(perspectives):
            avg_consistency[perspective] = np.mean(consistency_matrix[i])
        
        return avg_consistency
    
    def calculate_optimized_integration_score(
        self, 
        perspective_data: Dict[str, Dict],
        adaptive_weights: Dict[str, float]
    ) -> Dict:
        """最適化された統合スコア計算"""
        
        # 基本統合スコア
        basic_score = sum(
            perspective_data[p]['importance'] * 
            perspective_data[p]['confidence'] * 
            adaptive_weights[p]
            for p in perspective_data.keys()
        )
        
        # 整合性ボーナス
        coherence_bonus = self._calculate_coherence_bonus(perspective_data)
        
        # 不確実性ペナルティ
        uncertainty_penalty = self._calculate_uncertainty_penalty(perspective_data)
        
        # 最終統合スコア
        final_score = basic_score * (1 + coherence_bonus) * (1 - uncertainty_penalty)
        
        return {
            'final_score': final_score,
            'basic_score': basic_score,
            'coherence_bonus': coherence_bonus,
            'uncertainty_penalty': uncertainty_penalty,
            'adaptive_weights': adaptive_weights
        }
    
    def _calculate_coherence_bonus(self, perspective_data: Dict) -> float:
        """整合性ボーナス計算"""
        
        importance_scores = [
            perspective_data[p]['importance'] for p in perspective_data.keys()
        ]
        
        # 標準偏差による整合性評価
        std_dev = np.std(importance_scores)
        max_possible_std = 50  # 最大可能標準偏差（0-100スケール）
        
        # 整合性が高いほどボーナスが大きい
        coherence_ratio = 1 - (std_dev / max_possible_std)
        coherence_bonus = coherence_ratio * 0.2  # 最大20%のボーナス
        
        return max(0, coherence_bonus)
    
    def _calculate_uncertainty_penalty(self, perspective_data: Dict) -> float:
        """不確実性ペナルティ計算"""
        
        confidence_scores = [
            perspective_data[p]['confidence'] for p in perspective_data.keys()
        ]
        
        # 平均信頼度が低いほどペナルティが大きい
        avg_confidence = np.mean(confidence_scores)
        uncertainty_penalty = (1 - avg_confidence) * 0.15  # 最大15%のペナルティ
        
        return max(0, uncertainty_penalty)

class AdvancedCoherenceAnalyzer:
    """高度な整合性分析器"""
    
    def __init__(self):
        self.coherence_threshold = 0.7
        self.outlier_threshold = 2.0  # 標準偏差の倍数
        
    def analyze_multi_dimensional_coherence(
        self, 
        perspective_data: Dict[str, Dict]
    ) -> Dict:
        """多次元整合性分析"""
        
        # 各次元での整合性分析
        dimensions = ['importance', 'confidence', 'urgency', 'impact']
        coherence_results = {}
        
        for dimension in dimensions:
            if all(dimension in perspective_data[p] for p in perspective_data.keys()):
                coherence_results[dimension] = self._analyze_dimension_coherence(
                    perspective_data, dimension
                )
        
        # 総合整合性スコア
        overall_coherence = self._calculate_overall_coherence(coherence_results)
        
        # 外れ値検出
        outliers = self._detect_outliers(perspective_data)
        
        return {
            'dimension_coherence': coherence_results,
            'overall_coherence': overall_coherence,
            'outliers': outliers,
            'coherence_level': self._classify_coherence_level(overall_coherence)
        }
    
    def _analyze_dimension_coherence(
        self, 
        perspective_data: Dict, 
        dimension: str
    ) -> Dict:
        """次元別整合性分析"""
        
        values = [perspective_data[p][dimension] for p in perspective_data.keys()]
        
        # 統計的指標
        mean_val = np.mean(values)
        std_val = np.std(values)
        cv = std_val / mean_val if mean_val != 0 else float('inf')  # 変動係数
        
        # ペアワイズ相関
        correlations = []
        perspectives = list(perspective_data.keys())
        for i in range(len(perspectives)):
            for j in range(i+1, len(perspectives)):
                corr = 1 - abs(values[i] - values[j]) / 100
                correlations.append(corr)
        
        avg_correlation = np.mean(correlations)
        
        # 整合性スコア（0-1）
        coherence_score = avg_correlation * (1 - min(cv, 1))
        
        return {
            'coherence_score': coherence_score,
            'mean': mean_val,
            'std_dev': std_val,
            'coefficient_of_variation': cv,
            'average_correlation': avg_correlation,
            'values': values
        }
    
    def _calculate_overall_coherence(self, coherence_results: Dict) -> float:
        """総合整合性スコア計算"""
        
        if not coherence_results:
            return 0.0
        
        # 重み付き平均（重要度とインパクトを重視）
        weights = {
            'importance': 0.3,
            'confidence': 0.2,
            'urgency': 0.2,
            'impact': 0.3
        }
        
        weighted_sum = 0
        total_weight = 0
        
        for dimension, result in coherence_results.items():
            weight = weights.get(dimension, 0.25)
            weighted_sum += result['coherence_score'] * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    def _detect_outliers(self, perspective_data: Dict) -> List[str]:
        """外れ値検出"""
        
        outliers = []
        perspectives = list(perspective_data.keys())
        
        # 重要度での外れ値検出
        importance_values = [perspective_data[p]['importance'] for p in perspectives]
        mean_importance = np.mean(importance_values)
        std_importance = np.std(importance_values)
        
        for i, perspective in enumerate(perspectives):
            z_score = abs(importance_values[i] - mean_importance) / std_importance
            if z_score > self.outlier_threshold:
                outliers.append(perspective)
        
        return outliers
    
    def _classify_coherence_level(self, coherence_score: float) -> str:
        """整合性レベルの分類"""
        
        if coherence_score >= 0.8:
            return "高整合性"
        elif coherence_score >= 0.6:
            return "中整合性"
        elif coherence_score >= 0.4:
            return "低整合性"
        else:
            return "非整合性"
```

### 静止点検出メカニズムの高度化

**多段階静止点検出アルゴリズム**

```python
# algorithms/equilibrium_detection.py
import numpy as np
from scipy.optimize import minimize, differential_evolution
from typing import Dict, List, Tuple, Optional, Callable
import warnings

class AdvancedEquilibriumDetector:
    """高度な静止点検出器"""
    
    def __init__(self):
        self.convergence_criteria = {
            'score_stability': 0.01,      # スコア安定性閾値
            'weight_stability': 0.005,    # 重み安定性閾値
            'gradient_threshold': 0.001,  # 勾配閾値
            'oscillation_threshold': 0.02 # 振動閾値
        }
        self.max_iterations = 1000
        self.stability_window = 50
        
    def detect_equilibrium_points(
        self, 
        perspective_data: Dict[str, Dict],
        initial_weights: Optional[Dict[str, float]] = None
    ) -> Dict:
        """静止点検出の実行"""
        
        # 初期重みの設定
        if initial_weights is None:
            initial_weights = {p: 1/3 for p in perspective_data.keys()}
        
        # 複数の検出手法を並行実行
        detection_results = {}
        
        # 1. 勾配降下法による検出
        detection_results['gradient_descent'] = self._gradient_descent_detection(
            perspective_data, initial_weights
        )
        
        # 2. 進化的アルゴリズムによる検出
        detection_results['evolutionary'] = self._evolutionary_detection(
            perspective_data
        )
        
        # 3. 固定点反復法による検出
        detection_results['fixed_point'] = self._fixed_point_iteration(
            perspective_data, initial_weights
        )
        
        # 4. 多開始点法による検出
        detection_results['multi_start'] = self._multi_start_detection(
            perspective_data
        )
        
        # 結果の統合と検証
        consolidated_result = self._consolidate_detection_results(detection_results)
        
        return consolidated_result
    
    def _gradient_descent_detection(
        self, 
        perspective_data: Dict, 
        initial_weights: Dict
    ) -> Dict:
        """勾配降下法による静止点検出"""
        
        def objective_function(weights_array):
            """最適化目的関数"""
            perspectives = list(perspective_data.keys())
            weights = {perspectives[i]: weights_array[i] for i in range(len(perspectives))}
            
            # 統合スコア計算
            integrator = OptimizedPerspectiveIntegrator()
            result = integrator.calculate_optimized_integration_score(
                perspective_data, weights
            )
            
            # 最大化問題を最小化問題に変換（負の値を返す）
            return -result['final_score']
        
        def weight_constraint(weights_array):
            """重み制約（合計=1）"""
            return np.sum(weights_array) - 1.0
        
        # 初期値設定
        perspectives = list(perspective_data.keys())
        x0 = np.array([initial_weights[p] for p in perspectives])
        
        # 制約条件
        constraints = [
            {'type': 'eq', 'fun': weight_constraint},
        ]
        
        # 境界条件（各重みは0以上1以下）
        bounds = [(0.0, 1.0) for _ in perspectives]
        
        # 最適化実行
        try:
            result = minimize(
                objective_function,
                x0,
                method='SLSQP',
                bounds=bounds,
                constraints=constraints,
                options={'maxiter': self.max_iterations, 'ftol': 1e-9}
            )
            
            optimal_weights = {perspectives[i]: result.x[i] for i in range(len(perspectives))}
            
            return {
                'success': result.success,
                'optimal_weights': optimal_weights,
                'optimal_score': -result.fun,
                'iterations': result.nit,
                'convergence_info': {
                    'function_evaluations': result.nfev,
                    'gradient_norm': np.linalg.norm(result.jac) if hasattr(result, 'jac') else None
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'optimal_weights': initial_weights,
                'optimal_score': 0
            }
    
    def _evolutionary_detection(self, perspective_data: Dict) -> Dict:
        """進化的アルゴリズムによる静止点検出"""
        
        def objective_function(weights_array):
            """進化的アルゴリズム用目的関数"""
            perspectives = list(perspective_data.keys())
            
            # 重みの正規化
            weights_sum = np.sum(weights_array)
            if weights_sum > 0:
                normalized_weights = weights_array / weights_sum
            else:
                normalized_weights = np.ones(len(perspectives)) / len(perspectives)
            
            weights = {perspectives[i]: normalized_weights[i] for i in range(len(perspectives))}
            
            # 統合スコア計算
            integrator = OptimizedPerspectiveIntegrator()
            result = integrator.calculate_optimized_integration_score(
                perspective_data, weights
            )
            
            return -result['final_score']  # 最小化問題として定式化
        
        perspectives = list(perspective_data.keys())
        bounds = [(0.01, 0.99) for _ in perspectives]  # 極端な値を避ける
        
        try:
            result = differential_evolution(
                objective_function,
                bounds,
                maxiter=300,
                popsize=15,
                seed=42,
                atol=1e-8,
                tol=1e-8
            )
            
            # 重みの正規化
            weights_sum = np.sum(result.x)
            normalized_weights = result.x / weights_sum
            optimal_weights = {perspectives[i]: normalized_weights[i] for i in range(len(perspectives))}
            
            return {
                'success': result.success,
                'optimal_weights': optimal_weights,
                'optimal_score': -result.fun,
                'iterations': result.nit,
                'convergence_info': {
                    'function_evaluations': result.nfev,
                    'population_size': 15
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'optimal_weights': {p: 1/len(perspectives) for p in perspectives},
                'optimal_score': 0
            }
    
    def _fixed_point_iteration(
        self, 
        perspective_data: Dict, 
        initial_weights: Dict
    ) -> Dict:
        """固定点反復法による静止点検出"""
        
        current_weights = initial_weights.copy()
        weight_history = [current_weights.copy()]
        score_history = []
        
        integrator = OptimizedPerspectiveIntegrator()
        
        for iteration in range(self.max_iterations):
            # 現在の重みでの統合スコア計算
            result = integrator.calculate_optimized_integration_score(
                perspective_data, current_weights
            )
            score_history.append(result['final_score'])
            
            # 適応的重み更新
            new_weights = integrator.calculate_adaptive_weights(
                perspective_data, score_history[-10:] if len(score_history) >= 10 else None
            )
            
            # 収束判定
            weight_change = sum(
                abs(new_weights[p] - current_weights[p]) 
                for p in current_weights.keys()
            )
            
            if weight_change < self.convergence_criteria['weight_stability']:
                return {
                    'success': True,
                    'optimal_weights': new_weights,
                    'optimal_score': result['final_score'],
                    'iterations': iteration + 1,
                    'convergence_info': {
                        'weight_change': weight_change,
                        'score_history': score_history,
                        'weight_history': weight_history
                    }
                }
            
            current_weights = new_weights
            weight_history.append(current_weights.copy())
            
            # 振動検出
            if len(score_history) >= self.stability_window:
                recent_scores = score_history[-self.stability_window:]
                score_std = np.std(recent_scores)
                if score_std > self.convergence_criteria['oscillation_threshold']:
                    # 振動が検出された場合、学習率を下げる
                    integrator.learning_rate *= 0.9
        
        # 最大反復数に達した場合
        return {
            'success': False,
            'optimal_weights': current_weights,
            'optimal_score': score_history[-1] if score_history else 0,
            'iterations': self.max_iterations,
            'convergence_info': {
                'reason': 'max_iterations_reached',
                'final_weight_change': weight_change
            }
        }
    
    def _multi_start_detection(self, perspective_data: Dict) -> Dict:
        """多開始点法による静止点検出"""
        
        perspectives = list(perspective_data.keys())
        n_perspectives = len(perspectives)
        n_starts = 10  # 開始点の数
        
        results = []
        
        # 複数の初期点から最適化を実行
        for i in range(n_starts):
            # ランダムな初期重み生成（ディリクレ分布を使用）
            random_weights_array = np.random.dirichlet(np.ones(n_perspectives))
            initial_weights = {
                perspectives[j]: random_weights_array[j] 
                for j in range(n_perspectives)
            }
            
            # 勾配降下法で最適化
            result = self._gradient_descent_detection(perspective_data, initial_weights)
            
            if result['success']:
                results.append(result)
        
        if not results:
            return {
                'success': False,
                'error': 'No successful optimization runs',
                'optimal_weights': {p: 1/n_perspectives for p in perspectives},
                'optimal_score': 0
            }
        
        # 最良の結果を選択
        best_result = max(results, key=lambda x: x['optimal_score'])
        
        # 複数解の分析
        unique_solutions = self._analyze_multiple_solutions(results)
        
        return {
            'success': True,
            'optimal_weights': best_result['optimal_weights'],
            'optimal_score': best_result['optimal_score'],
            'iterations': best_result['iterations'],
            'convergence_info': {
                'n_successful_runs': len(results),
                'n_unique_solutions': len(unique_solutions),
                'solution_diversity': self._calculate_solution_diversity(results)
            }
        }
    
    def _consolidate_detection_results(self, detection_results: Dict) -> Dict:
        """検出結果の統合"""
        
        successful_results = {
            method: result for method, result in detection_results.items()
            if result.get('success', False)
        }
        
        if not successful_results:
            # 全ての手法が失敗した場合
            perspectives = list(detection_results.values())[0].get('optimal_weights', {}).keys()
            return {
                'success': False,
                'final_weights': {p: 1/len(perspectives) for p in perspectives},
                'final_score': 0,
                'detection_methods': detection_results,
                'consensus_level': 'none'
            }
        
        # 最良スコアの結果を選択
        best_method = max(
            successful_results.keys(),
            key=lambda m: successful_results[m]['optimal_score']
        )
        best_result = successful_results[best_method]
        
        # 手法間の合意度分析
        consensus_analysis = self._analyze_method_consensus(successful_results)
        
        return {
            'success': True,
            'final_weights': best_result['optimal_weights'],
            'final_score': best_result['optimal_score'],
            'best_method': best_method,
            'detection_methods': detection_results,
            'consensus_analysis': consensus_analysis,
            'consensus_level': self._classify_consensus_level(consensus_analysis)
        }
    
    def _analyze_method_consensus(self, successful_results: Dict) -> Dict:
        """手法間合意度分析"""
        
        if len(successful_results) < 2:
            return {'consensus_score': 1.0, 'weight_variance': 0.0}
        
        # 各視点の重みの分散を計算
        perspectives = list(list(successful_results.values())[0]['optimal_weights'].keys())
        weight_variances = {}
        
        for perspective in perspectives:
            weights = [
                result['optimal_weights'][perspective] 
                for result in successful_results.values()
            ]
            weight_variances[perspective] = np.var(weights)
        
        # 平均分散
        avg_variance = np.mean(list(weight_variances.values()))
        
        # 合意度スコア（分散が小さいほど高い）
        consensus_score = 1 / (1 + avg_variance * 10)
        
        return {
            'consensus_score': consensus_score,
            'weight_variance': avg_variance,
            'perspective_variances': weight_variances
        }
    
    def _classify_consensus_level(self, consensus_analysis: Dict) -> str:
        """合意レベルの分類"""
        
        consensus_score = consensus_analysis.get('consensus_score', 0)
        
        if consensus_score >= 0.9:
            return 'high_consensus'
        elif consensus_score >= 0.7:
            return 'moderate_consensus'
        elif consensus_score >= 0.5:
            return 'low_consensus'
        else:
            return 'no_consensus'
```

### 収束判定の精度向上

**高精度収束判定アルゴリズム**

```python
# algorithms/convergence_analysis.py
import numpy as np
from scipy import stats
from typing import Dict, List, Tuple, Optional
import warnings

class AdvancedConvergenceAnalyzer:
    """高度な収束分析器"""
    
    def __init__(self):
        self.convergence_tests = {
            'score_stability': self._test_score_stability,
            'weight_stability': self._test_weight_stability,
            'gradient_convergence': self._test_gradient_convergence,
            'oscillation_detection': self._test_oscillation,
            'trend_analysis': self._test_trend_stationarity,
            'lyapunov_stability': self._test_lyapunov_stability
        }
        
    def comprehensive_convergence_analysis(
        self, 
        score_history: List[float],
        weight_history: List[Dict[str, float]],
        gradient_history: Optional[List[float]] = None
    ) -> Dict:
        """包括的収束分析"""
        
        analysis_results = {}
        
        # 各収束テストを実行
        for test_name, test_function in self.convergence_tests.items():
            try:
                if test_name == 'gradient_convergence' and gradient_history is None:
                    continue
                    
                result = test_function(score_history, weight_history, gradient_history)
                analysis_results[test_name] = result
                
            except Exception as e:
                analysis_results[test_name] = {
                    'converged': False,
                    'error': str(e)
                }
        
        # 総合判定
        overall_assessment = self._assess_overall_convergence(analysis_results)
        
        return {
            'individual_tests': analysis_results,
            'overall_assessment': overall_assessment,
            'convergence_confidence': self._calculate_convergence_confidence(analysis_results),
            'recommendations': self._generate_recommendations(analysis_results)
        }
    
    def _test_score_stability(
        self, 
        score_history: List[float], 
        weight_history: List[Dict], 
        gradient_history: Optional[List] = None
    ) -> Dict:
        """スコア安定性テスト"""
        
        if len(score_history) < 10:
            return {'converged': False, 'reason': 'insufficient_data'}
        
        # 直近のスコア変化を分析
        recent_scores = score_history[-20:]  # 直近20回
        
        # 変化率の計算
        changes = [
            abs(recent_scores[i] - recent_scores[i-1]) / max(recent_scores[i-1], 1e-8)
            for i in range(1, len(recent_scores))
        ]
        
        # 統計的テスト
        mean_change = np.mean(changes)
        std_change = np.std(changes)
        max_change = np.max(changes)
        
        # 収束判定基準
        stability_threshold = 0.01  # 1%
        converged = (
            mean_change < stability_threshold and 
            max_change < stability_threshold * 3 and
            std_change < stability_threshold * 2
        )
        
        return {
            'converged': converged,
            'mean_change_rate': mean_change,
            'max_change_rate': max_change,
            'std_change_rate': std_change,
            'stability_score': 1 / (1 + mean_change * 100)
        }
    
    def _test_weight_stability(
        self, 
        score_history: List[float], 
        weight_history: List[Dict], 
        gradient_history: Optional[List] = None
    ) -> Dict:
        """重み安定性テスト"""
        
        if len(weight_history) < 10:
            return {'converged': False, 'reason': 'insufficient_data'}
        
        # 各視点の重み変化を分析
        perspectives = list(weight_history[0].keys())
        weight_changes = {p: [] for p in perspectives}
        
        for i in range(1, len(weight_history)):
            for perspective in perspectives:
                change = abs(
                    weight_history[i][perspective] - weight_history[i-1][perspective]
                )
                weight_changes[perspective].append(change)
        
        # 各視点の安定性評価
        perspective_stability = {}
        for perspective in perspectives:
            changes = weight_changes[perspective][-10:]  # 直近10回
            mean_change = np.mean(changes)
            max_change = np.max(changes)
            
            perspective_stability[perspective] = {
                'mean_change': mean_change,
                'max_change': max_change,
                'stable': mean_change < 0.005 and max_change < 0.02
            }
        
        # 全体の安定性判定
        all_stable = all(
            stability['stable'] for stability in perspective_stability.values()
        )
        
        overall_change = np.mean([
            stability['mean_change'] for stability in perspective_stability.values()
        ])
        
        return {
            'converged': all_stable,
            'perspective_stability': perspective_stability,
            'overall_weight_change': overall_change,
            'stability_score': 1 / (1 + overall_change * 1000)
        }
    
    def _test_gradient_convergence(
        self, 
        score_history: List[float], 
        weight_history: List[Dict], 
        gradient_history: List[float]
    ) -> Dict:
        """勾配収束テスト"""
        
        if not gradient_history or len(gradient_history) < 5:
            return {'converged': False, 'reason': 'insufficient_gradient_data'}
        
        recent_gradients = gradient_history[-10:]
        
        # 勾配ノルムの分析
        gradient_norms = [abs(g) for g in recent_gradients]
        mean_gradient = np.mean(gradient_norms)
        trend_slope = self._calculate_trend_slope(gradient_norms)
        
        # 収束判定
        gradient_threshold = 0.001
        converged = (
            mean_gradient < gradient_threshold and 
            trend_slope <= 0  # 勾配が減少傾向
        )
        
        return {
            'converged': converged,
            'mean_gradient_norm': mean_gradient,
            'trend_slope': trend_slope,
            'final_gradient': gradient_norms[-1] if gradient_norms else 0
        }
    
    def _test_oscillation(
        self, 
        score_history: List[float], 
        weight_history: List[Dict], 
        gradient_history: Optional[List] = None
    ) -> Dict:
        """振動検出テスト"""
        
        if len(score_history) < 20:
            return {'converged': True, 'reason': 'insufficient_data_for_oscillation'}
        
        recent_scores = score_history[-30:]
        
        # 自己相関による周期性検出
        autocorr = self._calculate_autocorrelation(recent_scores)
        
        # フーリエ変換による周波数分析
        fft_result = np.fft.fft(recent_scores)
        power_spectrum = np.abs(fft_result) ** 2
        
        # 主要周波数の検出
        dominant_freq_power = np.max(power_spectrum[1:len(power_spectrum)//2])
        total_power = np.sum(power_spectrum)
        
        oscillation_ratio = dominant_freq_power / total_power
        
        # 振動判定
        oscillation_threshold = 0.3
        has_oscillation = oscillation_ratio > oscillation_threshold
        
        return {
            'converged': not has_oscillation,
            'oscillation_detected': has_oscillation,
            'oscillation_ratio': oscillation_ratio,
            'dominant_frequency_power': dominant_freq_power,
            'autocorrelation_peak': np.max(autocorr[1:]) if len(autocorr) > 1 else 0
        }
    
    def _test_trend_stationarity(
        self, 
        score_history: List[float], 
        weight_history: List[Dict], 
        gradient_history: Optional[List] = None
    ) -> Dict:
        """トレンド定常性テスト"""
        
        if len(score_history) < 15:
            return {'converged': False, 'reason': 'insufficient_data'}
        
        recent_scores = score_history[-20:]
        
        # Augmented Dickey-Fuller テスト
        try:
            from statsmodels.tsa.stattools import adfuller
            adf_result = adfuller(recent_scores)
            is_stationary = adf_result[1] < 0.05  # p-value < 0.05
        except ImportError:
            # statsmodelsが利用できない場合の簡易テスト
            trend_slope = self._calculate_trend_slope(recent_scores)
            is_stationary = abs(trend_slope) < 0.1
            adf_result = (None, None, None, None, None, None)
        
        # 分散の安定性
        first_half = recent_scores[:len(recent_scores)//2]
        second_half = recent_scores[len(recent_scores)//2:]
        
        var_ratio = np.var(second_half) / max(np.var(first_half), 1e-8)
        variance_stable = 0.5 < var_ratio < 2.0
        
        return {
            'converged': is_stationary and variance_stable,
            'is_stationary': is_stationary,
            'adf_statistic': adf_result[0],
            'adf_pvalue': adf_result[1],
            'variance_ratio': var_ratio,
            'variance_stable': variance_stable
        }
    
    def _test_lyapunov_stability(
        self, 
        score_history: List[float], 
        weight_history: List[Dict], 
        gradient_history: Optional[List] = None
    ) -> Dict:
        """リャプノフ安定性テスト"""
        
        if len(score_history) < 30:
            return {'converged': False, 'reason': 'insufficient_data'}
        
        # 簡易リャプノフ指数の計算
        recent_scores = score_history[-30:]
        
        # 隣接点間の距離変化を追跡
        distances = []
        for i in range(1, len(recent_scores) - 1):
            d1 = abs(recent_scores[i] - recent_scores[i-1])
            d2 = abs(recent_scores[i+1] - recent_scores[i])
            
            if d1 > 1e-8:
                distance_ratio = d2 / d1
                distances.append(np.log(distance_ratio))
        
        if not distances:
            return {'converged': True, 'reason': 'no_measurable_distances'}
        
        # 平均リャプノフ指数
        lyapunov_exponent = np.mean(distances)
        
        # 安定性判定（負のリャプノフ指数は安定）
        is_stable = lyapunov_exponent < 0
        
        return {
            'converged': is_stable,
            'lyapunov_exponent': lyapunov_exponent,
            'is_stable': is_stable,
            'chaos_indicator': lyapunov_exponent > 0.1
        }
    
    def _assess_overall_convergence(self, analysis_results: Dict) -> Dict:
        """総合収束判定"""
        
        # 各テストの重み
        test_weights = {
            'score_stability': 0.25,
            'weight_stability': 0.25,
            'gradient_convergence': 0.15,
            'oscillation_detection': 0.15,
            'trend_analysis': 0.1,
            'lyapunov_stability': 0.1
        }
        
        # 重み付きスコア計算
        weighted_score = 0
        total_weight = 0
        
        convergence_votes = 0
        total_votes = 0
        
        for test_name, result in analysis_results.items():
            if 'error' in result:
                continue
                
            weight = test_weights.get(test_name, 0.1)
            total_weight += weight
            total_votes += 1
            
            if result.get('converged', False):
                weighted_score += weight
                convergence_votes += 1
        
        # 総合判定
        if total_weight > 0:
            convergence_ratio = weighted_score / total_weight
            vote_ratio = convergence_votes / total_votes if total_votes > 0 else 0
        else:
            convergence_ratio = 0
            vote_ratio = 0
        
        # 最終判定
        overall_converged = convergence_ratio >= 0.7 and vote_ratio >= 0.6
        
        return {
            'converged': overall_converged,
            'convergence_ratio': convergence_ratio,
            'vote_ratio': vote_ratio,
            'confidence_level': self._classify_confidence_level(convergence_ratio)
        }
    
    def _calculate_convergence_confidence(self, analysis_results: Dict) -> float:
        """収束信頼度計算"""
        
        successful_tests = [
            result for result in analysis_results.values()
            if 'error' not in result
        ]
        
        if not successful_tests:
            return 0.0
        
        # 各テストの信頼度スコアを統合
        confidence_scores = []
        
        for result in successful_tests:
            if 'stability_score' in result:
                confidence_scores.append(result['stability_score'])
            elif result.get('converged', False):
                confidence_scores.append(0.8)
            else:
                confidence_scores.append(0.2)
        
        return np.mean(confidence_scores)
    
    def _generate_recommendations(self, analysis_results: Dict) -> List[str]:
        """推奨事項生成"""
        
        recommendations = []
        
        # 各テスト結果に基づく推奨事項
        for test_name, result in analysis_results.items():
            if 'error' in result:
                continue
                
            if not result.get('converged', False):
                if test_name == 'score_stability':
                    recommendations.append("スコアの安定化のため、学習率の調整を検討してください")
                elif test_name == 'weight_stability':
                    recommendations.append("重みの安定化のため、収束基準の緩和を検討してください")
                elif test_name == 'oscillation_detection':
                    recommendations.append("振動が検出されました。ダンピング項の追加を検討してください")
                elif test_name == 'lyapunov_stability':
                    recommendations.append("システムが不安定です。パラメータの再調整が必要です")
        
        if not recommendations:
            recommendations.append("収束が確認されました。現在の設定を維持してください")
        
        return recommendations
    
    # ユーティリティメソッド
    def _calculate_trend_slope(self, data: List[float]) -> float:
        """トレンド傾きの計算"""
        if len(data) < 2:
            return 0
        
        x = np.arange(len(data))
        slope, _, _, _, _ = stats.linregress(x, data)
        return slope
    
    def _calculate_autocorrelation(self, data: List[float]) -> np.ndarray:
        """自己相関の計算"""
        data_array = np.array(data)
        data_centered = data_array - np.mean(data_array)
        
        autocorr = np.correlate(data_centered, data_centered, mode='full')
        autocorr = autocorr[autocorr.size // 2:]
        
        # 正規化
        autocorr = autocorr / autocorr[0]
        
        return autocorr
    
    def _classify_confidence_level(self, convergence_ratio: float) -> str:
        """信頼度レベルの分類"""
        
        if convergence_ratio >= 0.9:
            return "very_high"
        elif convergence_ratio >= 0.7:
            return "high"
        elif convergence_ratio >= 0.5:
            return "moderate"
        elif convergence_ratio >= 0.3:
            return "low"
        else:
            return "very_low"
```

この高度なアルゴリズム最適化により、トリプルパースペクティブ型戦略AIレーダーは、複雑な多視点評価を効率的に統合し、信頼性の高い静止点を検出することができます。適応的重み調整、多段階検出手法、包括的収束分析により、様々な状況下での安定した意思決定支援を実現します。

