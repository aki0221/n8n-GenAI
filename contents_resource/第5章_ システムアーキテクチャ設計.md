# 第5章: システムアーキテクチャ設計

**作成支援**: Manus AI

## マイクロサービス設計の詳細化

トリプルパースペクティブ型戦略AIレーダーの実装において、マイクロサービスアーキテクチャは単なる技術的選択ではなく、戦略的価値創出を最大化するための設計思想です。本章では、Phase 1で確立したコア概念と評価メカニズムを基盤として、実装可能で拡張性の高いシステムアーキテクチャを詳細に設計します。

### 戦略的マイクロサービス設計の原則

マイクロサービス設計において、我々は以下の戦略的原則を採用します。これらの原則は、技術的効率性だけでなく、ビジネス価値創出と組織の成長を支援することを目的としています。

**ビジネス価値中心の分割原則**では、技術的な都合ではなく、ビジネス価値創出の観点からサービス境界を定義します。各マイクロサービスは、明確なビジネス価値を提供し、独立してデプロイ・運用可能な単位として設計されます。これにより、ビジネス要件の変化に迅速に対応し、継続的な価値創出を実現します。

**ドメイン駆動設計（DDD）の適用**により、複雑なビジネスドメインを適切に分割し、各サービスが特定のドメインに特化した責任を持つよう設計します。境界付きコンテキスト（Bounded Context）の概念を活用し、サービス間の結合度を最小化しながら、ドメインの整合性を維持します。

**自律性と独立性の確保**では、各マイクロサービスが独自のデータストア、デプロイメントサイクル、技術スタックを持つことを可能にします。これにより、チーム間の依存関係を最小化し、開発・運用の効率性を向上させます。

### コアマイクロサービスの設計

トリプルパースペクティブ型戦略AIレーダーは、以下の8つのコアマイクロサービスで構成されます。各サービスは、特定のビジネス機能に特化し、明確なAPI境界を持ちます。

#### 1. パースペクティブ管理サービス（Perspective Management Service）

**責任範囲**: 3つの視点（技術・市場・ビジネス）の定義、管理、メタデータ維持

```python
# services/perspective_management/models.py
from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum
import uuid
from datetime import datetime

class PerspectiveType(Enum):
    TECHNOLOGY = "technology"
    MARKET = "market"
    BUSINESS = "business"

@dataclass
class PerspectiveDefinition:
    """視点定義モデル"""
    id: uuid.UUID
    type: PerspectiveType
    name: str
    description: str
    keywords: List[str]
    weight_factors: Dict[str, float]
    evaluation_criteria: List[str]
    data_sources: List[str]
    created_at: datetime
    updated_at: datetime
    version: str

@dataclass
class PerspectiveConfiguration:
    """視点設定モデル"""
    perspective_id: uuid.UUID
    organization_id: uuid.UUID
    custom_weights: Dict[str, float]
    custom_criteria: List[str]
    active_data_sources: List[str]
    update_frequency: str  # cron expression
    notification_settings: Dict[str, bool]

class PerspectiveManagementService:
    """パースペクティブ管理サービス"""
    
    def __init__(self, repository, event_publisher):
        self.repository = repository
        self.event_publisher = event_publisher
    
    async def create_perspective_definition(
        self, 
        definition: PerspectiveDefinition
    ) -> PerspectiveDefinition:
        """新しい視点定義の作成"""
        
        # バリデーション
        await self._validate_perspective_definition(definition)
        
        # 保存
        saved_definition = await self.repository.save_perspective_definition(definition)
        
        # イベント発行
        await self.event_publisher.publish({
            'event_type': 'perspective_definition_created',
            'perspective_id': str(saved_definition.id),
            'perspective_type': saved_definition.type.value,
            'timestamp': datetime.utcnow().isoformat()
        })
        
        return saved_definition
    
    async def configure_perspective_for_organization(
        self,
        perspective_id: uuid.UUID,
        organization_id: uuid.UUID,
        configuration: PerspectiveConfiguration
    ) -> PerspectiveConfiguration:
        """組織固有の視点設定"""
        
        # 視点定義の存在確認
        perspective_def = await self.repository.get_perspective_definition(perspective_id)
        if not perspective_def:
            raise ValueError(f"Perspective definition not found: {perspective_id}")
        
        # 設定の保存
        saved_config = await self.repository.save_perspective_configuration(configuration)
        
        # 設定変更イベント発行
        await self.event_publisher.publish({
            'event_type': 'perspective_configuration_updated',
            'perspective_id': str(perspective_id),
            'organization_id': str(organization_id),
            'timestamp': datetime.utcnow().isoformat()
        })
        
        return saved_config
    
    async def get_active_perspectives_for_organization(
        self, 
        organization_id: uuid.UUID
    ) -> List[PerspectiveDefinition]:
        """組織のアクティブな視点一覧取得"""
        
        configurations = await self.repository.get_perspective_configurations_by_organization(
            organization_id
        )
        
        active_perspectives = []
        for config in configurations:
            perspective_def = await self.repository.get_perspective_definition(
                config.perspective_id
            )
            if perspective_def:
                active_perspectives.append(perspective_def)
        
        return active_perspectives
    
    async def _validate_perspective_definition(
        self, 
        definition: PerspectiveDefinition
    ) -> None:
        """視点定義のバリデーション"""
        
        # 必須フィールドの確認
        if not definition.name or not definition.description:
            raise ValueError("Name and description are required")
        
        # キーワードの妥当性確認
        if not definition.keywords or len(definition.keywords) == 0:
            raise ValueError("At least one keyword is required")
        
        # 重み係数の妥当性確認
        if definition.weight_factors:
            total_weight = sum(definition.weight_factors.values())
            if abs(total_weight - 1.0) > 0.01:
                raise ValueError("Weight factors must sum to 1.0")
```

#### 2. データ収集サービス（Data Collection Service）

**責任範囲**: 多様なデータソースからの情報収集、前処理、品質管理

```python
# services/data_collection/collectors.py
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
import asyncio
import aiohttp
from dataclasses import dataclass
from datetime import datetime
import hashlib

@dataclass
class CollectedData:
    """収集データモデル"""
    id: str
    source_id: str
    perspective_type: PerspectiveType
    title: str
    content: str
    url: Optional[str]
    published_at: Optional[datetime]
    collected_at: datetime
    metadata: Dict[str, Any]
    quality_score: float
    relevance_score: float
    raw_data: Dict[str, Any]

class DataCollector(ABC):
    """データ収集器の基底クラス"""
    
    @abstractmethod
    async def collect(
        self, 
        perspective_config: PerspectiveConfiguration,
        keywords: List[str]
    ) -> List[CollectedData]:
        """データ収集の実行"""
        pass
    
    @abstractmethod
    def get_collector_type(self) -> str:
        """収集器タイプの取得"""
        pass

class WebScrapingCollector(DataCollector):
    """Webスクレイピング収集器"""
    
    def __init__(self, session: aiohttp.ClientSession):
        self.session = session
        self.rate_limiter = asyncio.Semaphore(5)  # 同時接続数制限
    
    async def collect(
        self, 
        perspective_config: PerspectiveConfiguration,
        keywords: List[str]
    ) -> List[CollectedData]:
        """Webスクレイピングによるデータ収集"""
        
        collected_data = []
        
        for source_url in perspective_config.active_data_sources:
            async with self.rate_limiter:
                try:
                    data = await self._scrape_website(source_url, keywords)
                    if data:
                        collected_data.extend(data)
                    
                    # レート制限のための待機
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    # エラーログ記録
                    print(f"Error scraping {source_url}: {e}")
                    continue
        
        return collected_data
    
    async def _scrape_website(
        self, 
        url: str, 
        keywords: List[str]
    ) -> List[CollectedData]:
        """個別サイトのスクレイピング"""
        
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return []
                
                html_content = await response.text()
                
                # HTMLパース（BeautifulSoupなどを使用）
                parsed_data = await self._parse_html_content(html_content, url)
                
                # キーワード関連性チェック
                relevant_data = []
                for data in parsed_data:
                    relevance_score = self._calculate_relevance_score(data, keywords)
                    if relevance_score > 0.3:  # 閾値以上の関連性
                        data.relevance_score = relevance_score
                        relevant_data.append(data)
                
                return relevant_data
                
        except Exception as e:
            print(f"Error in _scrape_website for {url}: {e}")
            return []
    
    def get_collector_type(self) -> str:
        return "web_scraping"

class APICollector(DataCollector):
    """API経由データ収集器"""
    
    def __init__(self, api_configs: Dict[str, Dict]):
        self.api_configs = api_configs
    
    async def collect(
        self, 
        perspective_config: PerspectiveConfiguration,
        keywords: List[str]
    ) -> List[CollectedData]:
        """API経由でのデータ収集"""
        
        collected_data = []
        
        for api_name in perspective_config.active_data_sources:
            if api_name in self.api_configs:
                try:
                    data = await self._collect_from_api(api_name, keywords)
                    collected_data.extend(data)
                except Exception as e:
                    print(f"Error collecting from API {api_name}: {e}")
                    continue
        
        return collected_data
    
    async def _collect_from_api(
        self, 
        api_name: str, 
        keywords: List[str]
    ) -> List[CollectedData]:
        """個別API からのデータ収集"""
        
        api_config = self.api_configs[api_name]
        
        # API固有の実装
        if api_name == "news_api":
            return await self._collect_from_news_api(api_config, keywords)
        elif api_name == "twitter_api":
            return await self._collect_from_twitter_api(api_config, keywords)
        elif api_name == "reddit_api":
            return await self._collect_from_reddit_api(api_config, keywords)
        else:
            return []
    
    def get_collector_type(self) -> str:
        return "api_collection"

class DataCollectionService:
    """データ収集サービス"""
    
    def __init__(self):
        self.collectors: Dict[str, DataCollector] = {}
        self.quality_analyzer = DataQualityAnalyzer()
        self.deduplicator = DataDeduplicator()
    
    def register_collector(self, collector: DataCollector):
        """データ収集器の登録"""
        collector_type = collector.get_collector_type()
        self.collectors[collector_type] = collector
    
    async def collect_data_for_perspective(
        self,
        perspective_config: PerspectiveConfiguration,
        keywords: List[str]
    ) -> List[CollectedData]:
        """視点に対するデータ収集の実行"""
        
        all_collected_data = []
        
        # 全ての収集器でデータ収集
        collection_tasks = []
        for collector in self.collectors.values():
            task = collector.collect(perspective_config, keywords)
            collection_tasks.append(task)
        
        # 並列実行
        collection_results = await asyncio.gather(*collection_tasks, return_exceptions=True)
        
        # 結果の統合
        for result in collection_results:
            if isinstance(result, list):
                all_collected_data.extend(result)
            elif isinstance(result, Exception):
                print(f"Collection error: {result}")
        
        # 品質分析
        quality_analyzed_data = []
        for data in all_collected_data:
            quality_score = await self.quality_analyzer.analyze_quality(data)
            data.quality_score = quality_score
            
            # 品質閾値チェック
            if quality_score > 0.5:
                quality_analyzed_data.append(data)
        
        # 重複除去
        deduplicated_data = await self.deduplicator.remove_duplicates(quality_analyzed_data)
        
        return deduplicated_data

class DataQualityAnalyzer:
    """データ品質分析器"""
    
    async def analyze_quality(self, data: CollectedData) -> float:
        """データ品質の分析"""
        
        quality_factors = []
        
        # コンテンツの長さ
        content_length_score = self._analyze_content_length(data.content)
        quality_factors.append(('content_length', content_length_score, 0.2))
        
        # 言語品質
        language_quality_score = self._analyze_language_quality(data.content)
        quality_factors.append(('language_quality', language_quality_score, 0.3))
        
        # 情報の新しさ
        freshness_score = self._analyze_freshness(data.published_at)
        quality_factors.append(('freshness', freshness_score, 0.2))
        
        # ソースの信頼性
        source_reliability_score = self._analyze_source_reliability(data.source_id)
        quality_factors.append(('source_reliability', source_reliability_score, 0.3))
        
        # 重み付き平均
        total_score = sum(score * weight for _, score, weight in quality_factors)
        
        return min(max(total_score, 0.0), 1.0)
    
    def _analyze_content_length(self, content: str) -> float:
        """コンテンツ長の分析"""
        length = len(content)
        
        if length < 50:
            return 0.2
        elif length < 200:
            return 0.6
        elif length < 1000:
            return 1.0
        elif length < 5000:
            return 0.9
        else:
            return 0.7  # 長すぎる場合は品質が下がる
    
    def _analyze_language_quality(self, content: str) -> float:
        """言語品質の分析"""
        # 簡易的な実装例
        
        # 文字種の多様性
        char_diversity = len(set(content)) / len(content) if content else 0
        
        # 句読点の適切な使用
        punctuation_ratio = sum(1 for c in content if c in '.,!?;:') / len(content) if content else 0
        
        # 単語の平均長
        words = content.split()
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        
        # 総合スコア
        diversity_score = min(char_diversity * 10, 1.0)
        punctuation_score = min(punctuation_ratio * 50, 1.0)
        word_length_score = min(avg_word_length / 6, 1.0)
        
        return (diversity_score + punctuation_score + word_length_score) / 3
    
    def _analyze_freshness(self, published_at: Optional[datetime]) -> float:
        """情報の新しさ分析"""
        if not published_at:
            return 0.5  # 不明な場合は中間値
        
        now = datetime.utcnow()
        age_days = (now - published_at).days
        
        if age_days <= 1:
            return 1.0
        elif age_days <= 7:
            return 0.9
        elif age_days <= 30:
            return 0.7
        elif age_days <= 90:
            return 0.5
        else:
            return 0.3
    
    def _analyze_source_reliability(self, source_id: str) -> float:
        """ソース信頼性の分析"""
        # 実装では、ソース信頼性データベースを参照
        # ここでは簡易的な実装
        
        reliable_sources = {
            'reuters.com': 0.95,
            'bloomberg.com': 0.95,
            'techcrunch.com': 0.85,
            'github.com': 0.90,
            'arxiv.org': 0.90
        }
        
        for domain, score in reliable_sources.items():
            if domain in source_id:
                return score
        
        return 0.6  # デフォルト値

class DataDeduplicator:
    """データ重複除去器"""
    
    async def remove_duplicates(self, data_list: List[CollectedData]) -> List[CollectedData]:
        """重複データの除去"""
        
        seen_hashes = set()
        unique_data = []
        
        for data in data_list:
            # コンテンツハッシュの計算
            content_hash = self._calculate_content_hash(data)
            
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique_data.append(data)
        
        return unique_data
    
    def _calculate_content_hash(self, data: CollectedData) -> str:
        """コンテンツハッシュの計算"""
        
        # タイトルとコンテンツの正規化
        normalized_title = self._normalize_text(data.title)
        normalized_content = self._normalize_text(data.content)
        
        # ハッシュ計算
        combined_text = f"{normalized_title}|{normalized_content}"
        return hashlib.sha256(combined_text.encode('utf-8')).hexdigest()
    
    def _normalize_text(self, text: str) -> str:
        """テキストの正規化"""
        import re
        
        # 小文字化
        text = text.lower()
        
        # 空白の正規化
        text = re.sub(r'\s+', ' ', text)
        
        # 特殊文字の除去
        text = re.sub(r'[^\w\s]', '', text)
        
        return text.strip()
```

#### 3. 評価エンジンサービス（Evaluation Engine Service）

**責任範囲**: Phase 1で設計した評価メカニズムの実装、重要度・確信度・整合性の計算

```python
# services/evaluation_engine/engine.py
from typing import Dict, List, Tuple, Optional
import asyncio
from dataclasses import dataclass
from datetime import datetime
import numpy as np

@dataclass
class EvaluationRequest:
    """評価リクエストモデル"""
    perspective_type: PerspectiveType
    collected_data: List[CollectedData]
    evaluation_criteria: List[str]
    weight_factors: Dict[str, float]
    context: Dict[str, Any]

@dataclass
class EvaluationResult:
    """評価結果モデル"""
    data_id: str
    perspective_type: PerspectiveType
    importance_score: float
    confidence_score: float
    relevance_score: float
    impact_score: float
    urgency_score: float
    evaluation_details: Dict[str, Any]
    evaluated_at: datetime

class EvaluationEngine:
    """評価エンジン"""
    
    def __init__(self):
        self.importance_calculator = ImportanceCalculator()
        self.confidence_calculator = ConfidenceCalculator()
        self.impact_calculator = ImpactCalculator()
        self.urgency_calculator = UrgencyCalculator()
    
    async def evaluate_data(
        self, 
        request: EvaluationRequest
    ) -> List[EvaluationResult]:
        """データの評価実行"""
        
        evaluation_results = []
        
        # 並列評価の実行
        evaluation_tasks = []
        for data in request.collected_data:
            task = self._evaluate_single_data(data, request)
            evaluation_tasks.append(task)
        
        results = await asyncio.gather(*evaluation_tasks)
        evaluation_results.extend(results)
        
        return evaluation_results
    
    async def _evaluate_single_data(
        self, 
        data: CollectedData, 
        request: EvaluationRequest
    ) -> EvaluationResult:
        """単一データの評価"""
        
        # 各指標の並列計算
        importance_task = self.importance_calculator.calculate(data, request)
        confidence_task = self.confidence_calculator.calculate(data, request)
        impact_task = self.impact_calculator.calculate(data, request)
        urgency_task = self.urgency_calculator.calculate(data, request)
        
        importance_score, confidence_score, impact_score, urgency_score = await asyncio.gather(
            importance_task, confidence_task, impact_task, urgency_task
        )
        
        # 評価詳細の構築
        evaluation_details = {
            'importance_factors': importance_score.get('factors', {}),
            'confidence_factors': confidence_score.get('factors', {}),
            'impact_factors': impact_score.get('factors', {}),
            'urgency_factors': urgency_score.get('factors', {}),
            'perspective_specific_metrics': self._calculate_perspective_specific_metrics(
                data, request.perspective_type
            )
        }
        
        return EvaluationResult(
            data_id=data.id,
            perspective_type=request.perspective_type,
            importance_score=importance_score.get('score', 0.0),
            confidence_score=confidence_score.get('score', 0.0),
            relevance_score=data.relevance_score,
            impact_score=impact_score.get('score', 0.0),
            urgency_score=urgency_score.get('score', 0.0),
            evaluation_details=evaluation_details,
            evaluated_at=datetime.utcnow()
        )
    
    def _calculate_perspective_specific_metrics(
        self, 
        data: CollectedData, 
        perspective_type: PerspectiveType
    ) -> Dict[str, float]:
        """視点固有メトリクスの計算"""
        
        if perspective_type == PerspectiveType.TECHNOLOGY:
            return self._calculate_technology_metrics(data)
        elif perspective_type == PerspectiveType.MARKET:
            return self._calculate_market_metrics(data)
        elif perspective_type == PerspectiveType.BUSINESS:
            return self._calculate_business_metrics(data)
        else:
            return {}
    
    def _calculate_technology_metrics(self, data: CollectedData) -> Dict[str, float]:
        """技術視点固有メトリクス"""
        
        metrics = {}
        
        # 技術的新規性
        tech_keywords = ['AI', 'machine learning', 'blockchain', 'quantum', 'IoT', '5G', 'cloud']
        tech_novelty = sum(1 for keyword in tech_keywords if keyword.lower() in data.content.lower())
        metrics['technical_novelty'] = min(tech_novelty / len(tech_keywords), 1.0)
        
        # 実装可能性
        implementation_keywords = ['prototype', 'beta', 'production', 'deployment', 'implementation']
        implementation_signals = sum(1 for keyword in implementation_keywords if keyword.lower() in data.content.lower())
        metrics['implementation_feasibility'] = min(implementation_signals / len(implementation_keywords), 1.0)
        
        # 技術的影響度
        impact_keywords = ['breakthrough', 'revolutionary', 'disruptive', 'innovation', 'advancement']
        impact_signals = sum(1 for keyword in impact_keywords if keyword.lower() in data.content.lower())
        metrics['technical_impact'] = min(impact_signals / len(impact_keywords), 1.0)
        
        return metrics
    
    def _calculate_market_metrics(self, data: CollectedData) -> Dict[str, float]:
        """市場視点固有メトリクス"""
        
        metrics = {}
        
        # 市場規模指標
        market_size_keywords = ['billion', 'million', 'market size', 'revenue', 'growth']
        market_size_signals = sum(1 for keyword in market_size_keywords if keyword.lower() in data.content.lower())
        metrics['market_size_indicator'] = min(market_size_signals / len(market_size_keywords), 1.0)
        
        # 競争激化度
        competition_keywords = ['competition', 'competitor', 'market share', 'rivalry', 'competitive']
        competition_signals = sum(1 for keyword in competition_keywords if keyword.lower() in data.content.lower())
        metrics['competition_intensity'] = min(competition_signals / len(competition_keywords), 1.0)
        
        # 顧客需要
        demand_keywords = ['demand', 'customer', 'user', 'adoption', 'preference']
        demand_signals = sum(1 for keyword in demand_keywords if keyword.lower() in data.content.lower())
        metrics['customer_demand'] = min(demand_signals / len(demand_keywords), 1.0)
        
        return metrics
    
    def _calculate_business_metrics(self, data: CollectedData) -> Dict[str, float]:
        """ビジネス視点固有メトリクス"""
        
        metrics = {}
        
        # 収益性指標
        profitability_keywords = ['profit', 'revenue', 'ROI', 'margin', 'earnings']
        profitability_signals = sum(1 for keyword in profitability_keywords if keyword.lower() in data.content.lower())
        metrics['profitability_indicator'] = min(profitability_signals / len(profitability_keywords), 1.0)
        
        # 戦略的重要性
        strategic_keywords = ['strategy', 'strategic', 'vision', 'mission', 'objective']
        strategic_signals = sum(1 for keyword in strategic_keywords if keyword.lower() in data.content.lower())
        metrics['strategic_importance'] = min(strategic_signals / len(strategic_keywords), 1.0)
        
        # リスク要因
        risk_keywords = ['risk', 'threat', 'challenge', 'uncertainty', 'volatility']
        risk_signals = sum(1 for keyword in risk_keywords if keyword.lower() in data.content.lower())
        metrics['risk_factor'] = min(risk_signals / len(risk_keywords), 1.0)
        
        return metrics

class ImportanceCalculator:
    """重要度計算器"""
    
    async def calculate(
        self, 
        data: CollectedData, 
        request: EvaluationRequest
    ) -> Dict[str, Any]:
        """重要度の計算"""
        
        factors = {}
        
        # 影響範囲の評価
        impact_scope = self._calculate_impact_scope(data, request)
        factors['impact_scope'] = impact_scope
        
        # 変化の大きさ
        change_magnitude = self._calculate_change_magnitude(data, request)
        factors['change_magnitude'] = change_magnitude
        
        # 戦略的関連性
        strategic_relevance = self._calculate_strategic_relevance(data, request)
        factors['strategic_relevance'] = strategic_relevance
        
        # 時間的緊急性
        temporal_urgency = self._calculate_temporal_urgency(data, request)
        factors['temporal_urgency'] = temporal_urgency
        
        # 重み付き合計
        weights = request.weight_factors.get('importance', {
            'impact_scope': 0.3,
            'change_magnitude': 0.25,
            'strategic_relevance': 0.25,
            'temporal_urgency': 0.2
        })
        
        weighted_score = (
            impact_scope * weights.get('impact_scope', 0.3) +
            change_magnitude * weights.get('change_magnitude', 0.25) +
            strategic_relevance * weights.get('strategic_relevance', 0.25) +
            temporal_urgency * weights.get('temporal_urgency', 0.2)
        )
        
        return {
            'score': min(max(weighted_score, 0.0), 1.0),
            'factors': factors
        }
    
    def _calculate_impact_scope(
        self, 
        data: CollectedData, 
        request: EvaluationRequest
    ) -> float:
        """影響範囲の計算"""
        
        # 業界への影響
        industry_keywords = ['industry', 'sector', 'market', 'ecosystem']
        industry_impact = sum(1 for keyword in industry_keywords if keyword.lower() in data.content.lower())
        
        # 地理的影響
        geographic_keywords = ['global', 'international', 'worldwide', 'regional']
        geographic_impact = sum(1 for keyword in geographic_keywords if keyword.lower() in data.content.lower())
        
        # 組織的影響
        organizational_keywords = ['organization', 'company', 'enterprise', 'business']
        organizational_impact = sum(1 for keyword in organizational_keywords if keyword.lower() in data.content.lower())
        
        # 正規化
        total_keywords = len(industry_keywords) + len(geographic_keywords) + len(organizational_keywords)
        total_impact = industry_impact + geographic_impact + organizational_impact
        
        return min(total_impact / total_keywords, 1.0)
    
    def _calculate_change_magnitude(
        self, 
        data: CollectedData, 
        request: EvaluationRequest
    ) -> float:
        """変化の大きさ計算"""
        
        # 変化を示すキーワード
        change_keywords = ['change', 'shift', 'transformation', 'evolution', 'revolution']
        magnitude_keywords = ['significant', 'major', 'substantial', 'dramatic', 'radical']
        
        change_signals = sum(1 for keyword in change_keywords if keyword.lower() in data.content.lower())
        magnitude_signals = sum(1 for keyword in magnitude_keywords if keyword.lower() in data.content.lower())
        
        # 数値的変化の検出（パーセンテージ、倍数など）
        import re
        percentage_matches = re.findall(r'(\d+)%', data.content)
        multiplier_matches = re.findall(r'(\d+)x|(\d+) times', data.content)
        
        numerical_change_score = 0
        if percentage_matches:
            max_percentage = max(int(p) for p in percentage_matches)
            numerical_change_score = min(max_percentage / 100, 1.0)
        
        if multiplier_matches:
            max_multiplier = max(int(m[0] or m[1]) for m in multiplier_matches)
            numerical_change_score = max(numerical_change_score, min(max_multiplier / 10, 1.0))
        
        # 総合スコア
        keyword_score = (change_signals + magnitude_signals) / (len(change_keywords) + len(magnitude_keywords))
        
        return min(max(keyword_score, numerical_change_score), 1.0)
    
    def _calculate_strategic_relevance(
        self, 
        data: CollectedData, 
        request: EvaluationRequest
    ) -> float:
        """戦略的関連性の計算"""
        
        # 評価基準との関連性
        criteria_relevance = 0
        for criterion in request.evaluation_criteria:
            if criterion.lower() in data.content.lower():
                criteria_relevance += 1
        
        criteria_score = min(criteria_relevance / len(request.evaluation_criteria), 1.0) if request.evaluation_criteria else 0
        
        # 戦略的キーワードとの関連性
        strategic_keywords = ['strategy', 'competitive advantage', 'differentiation', 'innovation', 'growth']
        strategic_signals = sum(1 for keyword in strategic_keywords if keyword.lower() in data.content.lower())
        strategic_score = min(strategic_signals / len(strategic_keywords), 1.0)
        
        # 組み合わせ
        return (criteria_score * 0.6 + strategic_score * 0.4)
    
    def _calculate_temporal_urgency(
        self, 
        data: CollectedData, 
        request: EvaluationRequest
    ) -> float:
        """時間的緊急性の計算"""
        
        # 緊急性を示すキーワード
        urgency_keywords = ['urgent', 'immediate', 'critical', 'emergency', 'now']
        timeline_keywords = ['soon', 'quickly', 'rapidly', 'fast', 'accelerated']
        
        urgency_signals = sum(1 for keyword in urgency_keywords if keyword.lower() in data.content.lower())
        timeline_signals = sum(1 for keyword in timeline_keywords if keyword.lower() in data.content.lower())
        
        # 時間表現の検出
        import re
        time_expressions = re.findall(r'(\d+)\s*(day|week|month|year)s?', data.content.lower())
        
        time_urgency_score = 0
        if time_expressions:
            min_time_value = float('inf')
            for value, unit in time_expressions:
                days = int(value)
                if unit == 'week':
                    days *= 7
                elif unit == 'month':
                    days *= 30
                elif unit == 'year':
                    days *= 365
                
                min_time_value = min(min_time_value, days)
            
            # 短期間ほど緊急性が高い
            if min_time_value <= 30:  # 1ヶ月以内
                time_urgency_score = 1.0
            elif min_time_value <= 90:  # 3ヶ月以内
                time_urgency_score = 0.8
            elif min_time_value <= 365:  # 1年以内
                time_urgency_score = 0.5
            else:
                time_urgency_score = 0.2
        
        # 総合スコア
        keyword_score = (urgency_signals + timeline_signals) / (len(urgency_keywords) + len(timeline_keywords))
        
        return min(max(keyword_score, time_urgency_score), 1.0)

class ConfidenceCalculator:
    """確信度計算器"""
    
    async def calculate(
        self, 
        data: CollectedData, 
        request: EvaluationRequest
    ) -> Dict[str, Any]:
        """確信度の計算"""
        
        factors = {}
        
        # 情報源信頼性
        source_reliability = self._calculate_source_reliability(data)
        factors['source_reliability'] = source_reliability
        
        # データ品質
        data_quality = data.quality_score
        factors['data_quality'] = data_quality
        
        # 一貫性
        consistency = self._calculate_consistency(data, request)
        factors['consistency'] = consistency
        
        # 検証可能性
        verifiability = self._calculate_verifiability(data)
        factors['verifiability'] = verifiability
        
        # 幾何平均（Phase 1の数式に基づく）
        confidence_score = (
            source_reliability ** 0.3 *
            data_quality ** 0.25 *
            consistency ** 0.25 *
            verifiability ** 0.2
        )
        
        return {
            'score': confidence_score,
            'factors': factors
        }
    
    def _calculate_source_reliability(self, data: CollectedData) -> float:
        """情報源信頼性の計算"""
        
        # データ収集サービスで計算済みの値を使用
        # 追加的な信頼性要因を考慮
        
        base_reliability = data.metadata.get('source_reliability', 0.6)
        
        # 引用・参照の存在
        citation_keywords = ['source', 'reference', 'study', 'research', 'report']
        citation_signals = sum(1 for keyword in citation_keywords if keyword.lower() in data.content.lower())
        citation_bonus = min(citation_signals * 0.1, 0.2)
        
        # 著者・組織の明示
        author_keywords = ['author', 'researcher', 'analyst', 'expert', 'professor']
        author_signals = sum(1 for keyword in author_keywords if keyword.lower() in data.content.lower())
        author_bonus = min(author_signals * 0.05, 0.1)
        
        return min(base_reliability + citation_bonus + author_bonus, 1.0)
    
    def _calculate_consistency(
        self, 
        data: CollectedData, 
        request: EvaluationRequest
    ) -> float:
        """一貫性の計算"""
        
        # 内部一貫性（論理的整合性）
        contradiction_keywords = ['however', 'but', 'although', 'despite', 'contrary']
        contradiction_signals = sum(1 for keyword in contradiction_keywords if keyword.lower() in data.content.lower())
        
        # 矛盾が多いほど一貫性が低い
        internal_consistency = max(1.0 - (contradiction_signals * 0.1), 0.0)
        
        # 外部一貫性（他の情報との整合性）
        # 簡易実装：キーワードの一致度
        context_keywords = request.context.get('related_keywords', [])
        if context_keywords:
            matching_keywords = sum(1 for keyword in context_keywords if keyword.lower() in data.content.lower())
            external_consistency = min(matching_keywords / len(context_keywords), 1.0)
        else:
            external_consistency = 0.7  # デフォルト値
        
        return (internal_consistency * 0.6 + external_consistency * 0.4)
    
    def _calculate_verifiability(self, data: CollectedData) -> float:
        """検証可能性の計算"""
        
        verifiability_factors = []
        
        # URL・リンクの存在
        import re
        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        urls = re.findall(url_pattern, data.content)
        url_score = min(len(urls) * 0.2, 1.0)
        verifiability_factors.append(url_score)
        
        # 具体的な数値・データの存在
        number_pattern = r'\d+(?:\.\d+)?'
        numbers = re.findall(number_pattern, data.content)
        number_score = min(len(numbers) * 0.05, 1.0)
        verifiability_factors.append(number_score)
        
        # 日付・時間の存在
        date_keywords = ['2023', '2024', 'january', 'february', 'march', 'april', 'may', 'june']
        date_signals = sum(1 for keyword in date_keywords if keyword.lower() in data.content.lower())
        date_score = min(date_signals * 0.1, 1.0)
        verifiability_factors.append(date_score)
        
        # 組織・企業名の存在
        organization_pattern = r'\b[A-Z][a-zA-Z]*\s+(?:Inc|Corp|Ltd|LLC|Company|Corporation)\b'
        organizations = re.findall(organization_pattern, data.content)
        org_score = min(len(organizations) * 0.15, 1.0)
        verifiability_factors.append(org_score)
        
        return min(sum(verifiability_factors) / len(verifiability_factors), 1.0)
```

この詳細なマイクロサービス設計により、トリプルパースペクティブ型戦略AIレーダーは、Phase 1で確立したコア概念を実装可能な形で具現化し、スケーラブルで保守性の高いシステムアーキテクチャを実現します。各サービスは独立して開発・デプロイ可能でありながら、統合的な戦略分析機能を提供します。


## スケーラビリティ設計

エンタープライズレベルでの運用を想定したトリプルパースペクティブ型戦略AIレーダーは、急激な負荷増加や大規模データ処理に対応できるスケーラビリティ設計が不可欠です。本セクションでは、水平・垂直スケーリング戦略、負荷分散、パフォーマンス最適化、障害対応を包括的に設計します。

### 水平・垂直スケーリング戦略

スケーラビリティ設計において、我々は水平スケーリング（スケールアウト）と垂直スケーリング（スケールアップ）を組み合わせた**ハイブリッドスケーリング戦略**を採用します。

#### 水平スケーリング（Scale-Out）戦略

水平スケーリングは、同一機能を持つサーバーインスタンスを追加することで処理能力を向上させる手法です。トリプルパースペクティブ型戦略AIレーダーでは、以下の設計原則に基づいて実装します。

```python
# infrastructure/scaling/horizontal_scaler.py
from dataclasses import dataclass
from typing import Dict, List, Optional
import asyncio
import kubernetes
from datetime import datetime, timedelta

@dataclass
class ScalingMetrics:
    """スケーリングメトリクス"""
    cpu_utilization: float
    memory_utilization: float
    request_rate: float
    response_time: float
    queue_length: int
    error_rate: float
    timestamp: datetime

@dataclass
class ScalingPolicy:
    """スケーリングポリシー"""
    service_name: str
    min_replicas: int
    max_replicas: int
    target_cpu_utilization: float
    target_memory_utilization: float
    target_response_time: float
    scale_up_threshold: float
    scale_down_threshold: float
    cooldown_period: timedelta
    scale_up_step: int
    scale_down_step: int

class HorizontalScaler:
    """水平スケーラー"""
    
    def __init__(self, k8s_client, metrics_collector):
        self.k8s_client = k8s_client
        self.metrics_collector = metrics_collector
        self.scaling_policies: Dict[str, ScalingPolicy] = {}
        self.last_scaling_actions: Dict[str, datetime] = {}
        self.scaling_history: List[Dict] = []
    
    def register_scaling_policy(self, policy: ScalingPolicy):
        """スケーリングポリシーの登録"""
        self.scaling_policies[policy.service_name] = policy
    
    async def monitor_and_scale(self):
        """監視とスケーリングの実行"""
        
        while True:
            try:
                for service_name, policy in self.scaling_policies.items():
                    await self._evaluate_and_scale_service(service_name, policy)
                
                # 監視間隔
                await asyncio.sleep(30)  # 30秒間隔
                
            except Exception as e:
                print(f"Scaling monitoring error: {e}")
                await asyncio.sleep(60)  # エラー時は長めの間隔
    
    async def _evaluate_and_scale_service(
        self, 
        service_name: str, 
        policy: ScalingPolicy
    ):
        """個別サービスの評価とスケーリング"""
        
        # 現在のメトリクス取得
        metrics = await self.metrics_collector.get_service_metrics(service_name)
        if not metrics:
            return
        
        # 現在のレプリカ数取得
        current_replicas = await self._get_current_replicas(service_name)
        
        # スケーリング判定
        scaling_decision = self._make_scaling_decision(metrics, policy, current_replicas)
        
        if scaling_decision['action'] != 'none':
            await self._execute_scaling(service_name, scaling_decision, policy)
    
    def _make_scaling_decision(
        self, 
        metrics: ScalingMetrics, 
        policy: ScalingPolicy,
        current_replicas: int
    ) -> Dict:
        """スケーリング判定"""
        
        # 複合スコアの計算
        cpu_score = metrics.cpu_utilization / policy.target_cpu_utilization
        memory_score = metrics.memory_utilization / policy.target_memory_utilization
        response_time_score = metrics.response_time / policy.target_response_time
        
        # 重み付き総合スコア
        composite_score = (
            cpu_score * 0.4 +
            memory_score * 0.3 +
            response_time_score * 0.3
        )
        
        # スケールアップ判定
        if composite_score > policy.scale_up_threshold and current_replicas < policy.max_replicas:
            target_replicas = min(
                current_replicas + policy.scale_up_step,
                policy.max_replicas
            )
            return {
                'action': 'scale_up',
                'target_replicas': target_replicas,
                'reason': f'Composite score {composite_score:.2f} > threshold {policy.scale_up_threshold}',
                'metrics': metrics
            }
        
        # スケールダウン判定
        elif composite_score < policy.scale_down_threshold and current_replicas > policy.min_replicas:
            target_replicas = max(
                current_replicas - policy.scale_down_step,
                policy.min_replicas
            )
            return {
                'action': 'scale_down',
                'target_replicas': target_replicas,
                'reason': f'Composite score {composite_score:.2f} < threshold {policy.scale_down_threshold}',
                'metrics': metrics
            }
        
        return {'action': 'none'}
    
    async def _execute_scaling(
        self, 
        service_name: str, 
        decision: Dict, 
        policy: ScalingPolicy
    ):
        """スケーリングの実行"""
        
        # クールダウン期間チェック
        last_action = self.last_scaling_actions.get(service_name)
        if last_action and datetime.utcnow() - last_action < policy.cooldown_period:
            return
        
        try:
            # Kubernetesでのスケーリング実行
            await self._scale_kubernetes_deployment(
                service_name, 
                decision['target_replicas']
            )
            
            # 履歴記録
            self.scaling_history.append({
                'service_name': service_name,
                'action': decision['action'],
                'target_replicas': decision['target_replicas'],
                'reason': decision['reason'],
                'timestamp': datetime.utcnow(),
                'metrics': decision['metrics']
            })
            
            # 最終実行時刻更新
            self.last_scaling_actions[service_name] = datetime.utcnow()
            
            print(f"Scaled {service_name}: {decision['action']} to {decision['target_replicas']} replicas")
            
        except Exception as e:
            print(f"Scaling execution error for {service_name}: {e}")
    
    async def _scale_kubernetes_deployment(self, service_name: str, target_replicas: int):
        """Kubernetesデプロイメントのスケーリング"""
        
        apps_v1 = kubernetes.client.AppsV1Api()
        
        # デプロイメント取得
        deployment = apps_v1.read_namespaced_deployment(
            name=service_name,
            namespace="default"
        )
        
        # レプリカ数更新
        deployment.spec.replicas = target_replicas
        
        # デプロイメント更新
        apps_v1.patch_namespaced_deployment(
            name=service_name,
            namespace="default",
            body=deployment
        )
    
    async def _get_current_replicas(self, service_name: str) -> int:
        """現在のレプリカ数取得"""
        
        apps_v1 = kubernetes.client.AppsV1Api()
        
        try:
            deployment = apps_v1.read_namespaced_deployment(
                name=service_name,
                namespace="default"
            )
            return deployment.status.ready_replicas or 0
        except Exception:
            return 0

class PredictiveScaler:
    """予測的スケーラー"""
    
    def __init__(self, metrics_collector, ml_predictor):
        self.metrics_collector = metrics_collector
        self.ml_predictor = ml_predictor
        self.prediction_horizon = timedelta(minutes=15)  # 15分先を予測
    
    async def predict_and_scale(self):
        """予測に基づくスケーリング"""
        
        while True:
            try:
                # 過去のメトリクス取得
                historical_metrics = await self.metrics_collector.get_historical_metrics(
                    lookback_period=timedelta(hours=24)
                )
                
                # 負荷予測
                predicted_load = await self.ml_predictor.predict_load(
                    historical_metrics,
                    prediction_horizon=self.prediction_horizon
                )
                
                # 予測に基づくスケーリング実行
                await self._execute_predictive_scaling(predicted_load)
                
                # 予測間隔
                await asyncio.sleep(300)  # 5分間隔
                
            except Exception as e:
                print(f"Predictive scaling error: {e}")
                await asyncio.sleep(600)  # エラー時は長めの間隔
    
    async def _execute_predictive_scaling(self, predicted_load: Dict):
        """予測的スケーリングの実行"""
        
        for service_name, load_prediction in predicted_load.items():
            if load_prediction['confidence'] > 0.8:  # 高信頼度の予測のみ
                
                # 必要レプリカ数の計算
                required_replicas = self._calculate_required_replicas(
                    load_prediction['predicted_rps'],
                    service_name
                )
                
                # 事前スケーリング実行
                await self._preemptive_scale(service_name, required_replicas)
    
    def _calculate_required_replicas(self, predicted_rps: float, service_name: str) -> int:
        """必要レプリカ数の計算"""
        
        # サービス別の処理能力（RPS per replica）
        service_capacity = {
            'perspective-management': 100,
            'data-collection': 50,
            'evaluation-engine': 30,
            'consensus-formation': 20,
            'insight-generation': 25,
            'notification': 200,
            'api-gateway': 500,
            'dashboard': 150
        }
        
        capacity_per_replica = service_capacity.get(service_name, 50)
        
        # 安全マージン（20%）を考慮
        required_replicas = int((predicted_rps / capacity_per_replica) * 1.2) + 1
        
        return max(required_replicas, 1)
```

#### 垂直スケーリング（Scale-Up）戦略

垂直スケーリングは、既存のサーバーインスタンスのリソース（CPU、メモリ）を増強する手法です。

```python
# infrastructure/scaling/vertical_scaler.py
from dataclasses import dataclass
from typing import Dict, List
import asyncio

@dataclass
class ResourceProfile:
    """リソースプロファイル"""
    cpu_cores: float
    memory_gb: float
    storage_gb: float
    network_bandwidth_mbps: float

@dataclass
class VerticalScalingPolicy:
    """垂直スケーリングポリシー"""
    service_name: str
    resource_profiles: List[ResourceProfile]
    cpu_threshold_up: float
    cpu_threshold_down: float
    memory_threshold_up: float
    memory_threshold_down: float
    evaluation_period: timedelta
    cooldown_period: timedelta

class VerticalScaler:
    """垂直スケーラー"""
    
    def __init__(self, cloud_provider_client):
        self.cloud_client = cloud_provider_client
        self.scaling_policies: Dict[str, VerticalScalingPolicy] = {}
        self.current_profiles: Dict[str, ResourceProfile] = {}
    
    async def monitor_and_scale_vertically(self):
        """垂直スケーリングの監視と実行"""
        
        while True:
            try:
                for service_name, policy in self.scaling_policies.items():
                    await self._evaluate_vertical_scaling(service_name, policy)
                
                await asyncio.sleep(300)  # 5分間隔
                
            except Exception as e:
                print(f"Vertical scaling error: {e}")
                await asyncio.sleep(600)
    
    async def _evaluate_vertical_scaling(
        self, 
        service_name: str, 
        policy: VerticalScalingPolicy
    ):
        """垂直スケーリングの評価"""
        
        # 期間内の平均メトリクス取得
        avg_metrics = await self._get_average_metrics(
            service_name, 
            policy.evaluation_period
        )
        
        current_profile = self.current_profiles.get(service_name)
        if not current_profile:
            return
        
        # スケールアップ判定
        if (avg_metrics['cpu_utilization'] > policy.cpu_threshold_up or 
            avg_metrics['memory_utilization'] > policy.memory_threshold_up):
            
            next_profile = self._get_next_higher_profile(current_profile, policy)
            if next_profile:
                await self._execute_vertical_scaling(service_name, next_profile)
        
        # スケールダウン判定
        elif (avg_metrics['cpu_utilization'] < policy.cpu_threshold_down and 
              avg_metrics['memory_utilization'] < policy.memory_threshold_down):
            
            next_profile = self._get_next_lower_profile(current_profile, policy)
            if next_profile:
                await self._execute_vertical_scaling(service_name, next_profile)
    
    def _get_next_higher_profile(
        self, 
        current_profile: ResourceProfile, 
        policy: VerticalScalingPolicy
    ) -> Optional[ResourceProfile]:
        """上位プロファイルの取得"""
        
        for profile in policy.resource_profiles:
            if (profile.cpu_cores > current_profile.cpu_cores or 
                profile.memory_gb > current_profile.memory_gb):
                return profile
        
        return None
    
    def _get_next_lower_profile(
        self, 
        current_profile: ResourceProfile, 
        policy: VerticalScalingPolicy
    ) -> Optional[ResourceProfile]:
        """下位プロファイルの取得"""
        
        suitable_profiles = [
            profile for profile in policy.resource_profiles
            if (profile.cpu_cores < current_profile.cpu_cores and 
                profile.memory_gb < current_profile.memory_gb)
        ]
        
        if suitable_profiles:
            # 最も近い下位プロファイルを選択
            return max(suitable_profiles, key=lambda p: p.cpu_cores + p.memory_gb)
        
        return None
```

### 負荷分散とパフォーマンス最適化

効率的な負荷分散は、システム全体のパフォーマンスと可用性を向上させる重要な要素です。

```python
# infrastructure/load_balancing/intelligent_load_balancer.py
from dataclasses import dataclass
from typing import Dict, List, Optional
import asyncio
import random
import time
from collections import defaultdict

@dataclass
class ServiceInstance:
    """サービスインスタンス"""
    id: str
    host: str
    port: int
    health_status: str
    current_connections: int
    response_time_avg: float
    cpu_utilization: float
    memory_utilization: float
    last_health_check: datetime

@dataclass
class LoadBalancingPolicy:
    """負荷分散ポリシー"""
    algorithm: str  # round_robin, weighted_round_robin, least_connections, response_time
    health_check_interval: int
    health_check_timeout: int
    max_retries: int
    circuit_breaker_threshold: int
    circuit_breaker_timeout: int

class IntelligentLoadBalancer:
    """インテリジェント負荷分散器"""
    
    def __init__(self):
        self.service_instances: Dict[str, List[ServiceInstance]] = defaultdict(list)
        self.policies: Dict[str, LoadBalancingPolicy] = {}
        self.circuit_breakers: Dict[str, Dict] = defaultdict(dict)
        self.request_history: Dict[str, List] = defaultdict(list)
    
    def register_service_instances(self, service_name: str, instances: List[ServiceInstance]):
        """サービスインスタンスの登録"""
        self.service_instances[service_name] = instances
    
    def set_load_balancing_policy(self, service_name: str, policy: LoadBalancingPolicy):
        """負荷分散ポリシーの設定"""
        self.policies[service_name] = policy
    
    async def route_request(self, service_name: str, request_data: Dict) -> Optional[ServiceInstance]:
        """リクエストのルーティング"""
        
        available_instances = await self._get_healthy_instances(service_name)
        if not available_instances:
            return None
        
        policy = self.policies.get(service_name)
        if not policy:
            # デフォルトはラウンドロビン
            return self._round_robin_selection(available_instances)
        
        # アルゴリズム別の選択
        if policy.algorithm == "round_robin":
            return self._round_robin_selection(available_instances)
        elif policy.algorithm == "weighted_round_robin":
            return self._weighted_round_robin_selection(available_instances)
        elif policy.algorithm == "least_connections":
            return self._least_connections_selection(available_instances)
        elif policy.algorithm == "response_time":
            return self._response_time_based_selection(available_instances)
        else:
            return self._adaptive_selection(available_instances, service_name)
    
    async def _get_healthy_instances(self, service_name: str) -> List[ServiceInstance]:
        """健全なインスタンスの取得"""
        
        healthy_instances = []
        
        for instance in self.service_instances[service_name]:
            # サーキットブレーカーチェック
            if self._is_circuit_breaker_open(service_name, instance.id):
                continue
            
            # ヘルスチェック
            if await self._health_check(instance):
                healthy_instances.append(instance)
            else:
                # 不健全なインスタンスのサーキットブレーカー更新
                self._update_circuit_breaker(service_name, instance.id, False)
        
        return healthy_instances
    
    def _adaptive_selection(
        self, 
        instances: List[ServiceInstance], 
        service_name: str
    ) -> ServiceInstance:
        """適応的選択アルゴリズム"""
        
        # 複数の要因を考慮したスコアリング
        scored_instances = []
        
        for instance in instances:
            # パフォーマンススコア（低いほど良い）
            performance_score = (
                instance.response_time_avg * 0.4 +
                instance.current_connections * 0.3 +
                instance.cpu_utilization * 0.2 +
                instance.memory_utilization * 0.1
            )
            
            # 成功率スコア
            success_rate = self._calculate_success_rate(service_name, instance.id)
            
            # 総合スコア（低いほど良い）
            total_score = performance_score * (2.0 - success_rate)
            
            scored_instances.append((instance, total_score))
        
        # 最低スコアのインスタンスを選択
        best_instance = min(scored_instances, key=lambda x: x[1])
        return best_instance[0]
    
    def _calculate_success_rate(self, service_name: str, instance_id: str) -> float:
        """成功率の計算"""
        
        history_key = f"{service_name}:{instance_id}"
        recent_requests = self.request_history[history_key][-100:]  # 直近100件
        
        if not recent_requests:
            return 1.0  # デフォルトは100%
        
        successful_requests = sum(1 for req in recent_requests if req['success'])
        return successful_requests / len(recent_requests)
    
    async def _health_check(self, instance: ServiceInstance) -> bool:
        """ヘルスチェック"""
        
        try:
            # 簡易的なHTTPヘルスチェック
            import aiohttp
            
            async with aiohttp.ClientSession() as session:
                health_url = f"http://{instance.host}:{instance.port}/health"
                
                async with session.get(health_url, timeout=5) as response:
                    if response.status == 200:
                        instance.last_health_check = datetime.utcnow()
                        return True
                    else:
                        return False
        
        except Exception:
            return False
    
    def _is_circuit_breaker_open(self, service_name: str, instance_id: str) -> bool:
        """サーキットブレーカーの状態確認"""
        
        breaker_key = f"{service_name}:{instance_id}"
        breaker = self.circuit_breakers[breaker_key]
        
        if not breaker:
            return False
        
        if breaker['state'] == 'open':
            # タイムアウト後に半開状態に移行
            if time.time() - breaker['opened_at'] > breaker['timeout']:
                breaker['state'] = 'half_open'
                return False
            return True
        
        return False
    
    def _update_circuit_breaker(
        self, 
        service_name: str, 
        instance_id: str, 
        success: bool
    ):
        """サーキットブレーカーの更新"""
        
        breaker_key = f"{service_name}:{instance_id}"
        breaker = self.circuit_breakers[breaker_key]
        
        if not breaker:
            breaker = {
                'failure_count': 0,
                'state': 'closed',
                'opened_at': None,
                'timeout': 60  # 60秒
            }
            self.circuit_breakers[breaker_key] = breaker
        
        if success:
            breaker['failure_count'] = 0
            if breaker['state'] == 'half_open':
                breaker['state'] = 'closed'
        else:
            breaker['failure_count'] += 1
            
            # 閾値を超えた場合、サーキットブレーカーを開く
            if breaker['failure_count'] >= 5:  # 閾値
                breaker['state'] = 'open'
                breaker['opened_at'] = time.time()

class PerformanceOptimizer:
    """パフォーマンス最適化器"""
    
    def __init__(self):
        self.cache_manager = CacheManager()
        self.connection_pool_manager = ConnectionPoolManager()
        self.query_optimizer = QueryOptimizer()
    
    async def optimize_request_processing(self, request_data: Dict) -> Dict:
        """リクエスト処理の最適化"""
        
        # キャッシュチェック
        cache_key = self._generate_cache_key(request_data)
        cached_result = await self.cache_manager.get(cache_key)
        
        if cached_result:
            return {
                'result': cached_result,
                'cache_hit': True,
                'processing_time': 0
            }
        
        # 処理実行
        start_time = time.time()
        result = await self._process_request(request_data)
        processing_time = time.time() - start_time
        
        # 結果をキャッシュ
        await self.cache_manager.set(cache_key, result, ttl=300)  # 5分間キャッシュ
        
        return {
            'result': result,
            'cache_hit': False,
            'processing_time': processing_time
        }
    
    def _generate_cache_key(self, request_data: Dict) -> str:
        """キャッシュキーの生成"""
        
        import hashlib
        import json
        
        # リクエストデータをソートしてハッシュ化
        sorted_data = json.dumps(request_data, sort_keys=True)
        return hashlib.sha256(sorted_data.encode()).hexdigest()

class CacheManager:
    """キャッシュマネージャー"""
    
    def __init__(self):
        self.redis_client = None  # Redis接続
        self.local_cache = {}
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0
        }
    
    async def get(self, key: str) -> Optional[Any]:
        """キャッシュからの取得"""
        
        # ローカルキャッシュチェック
        if key in self.local_cache:
            self.cache_stats['hits'] += 1
            return self.local_cache[key]['value']
        
        # Redisキャッシュチェック
        if self.redis_client:
            try:
                value = await self.redis_client.get(key)
                if value:
                    self.cache_stats['hits'] += 1
                    return json.loads(value)
            except Exception:
                pass
        
        self.cache_stats['misses'] += 1
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 300):
        """キャッシュへの設定"""
        
        # ローカルキャッシュ設定
        self.local_cache[key] = {
            'value': value,
            'expires_at': time.time() + ttl
        }
        
        # Redisキャッシュ設定
        if self.redis_client:
            try:
                await self.redis_client.setex(key, ttl, json.dumps(value))
            except Exception:
                pass
        
        # ローカルキャッシュのサイズ制限
        if len(self.local_cache) > 1000:
            await self._evict_expired_entries()
    
    async def _evict_expired_entries(self):
        """期限切れエントリの削除"""
        
        current_time = time.time()
        expired_keys = [
            key for key, entry in self.local_cache.items()
            if entry['expires_at'] < current_time
        ]
        
        for key in expired_keys:
            del self.local_cache[key]
            self.cache_stats['evictions'] += 1
```

### 障害対応とリカバリー設計

システムの可用性を確保するため、包括的な障害対応とリカバリー機能を設計します。

```python
# infrastructure/resilience/fault_tolerance.py
from dataclasses import dataclass
from typing import Dict, List, Callable, Optional
import asyncio
import time
from enum import Enum

class FailureType(Enum):
    NETWORK_TIMEOUT = "network_timeout"
    SERVICE_UNAVAILABLE = "service_unavailable"
    DATABASE_ERROR = "database_error"
    MEMORY_EXHAUSTION = "memory_exhaustion"
    CPU_OVERLOAD = "cpu_overload"

@dataclass
class FailurePattern:
    """障害パターン"""
    failure_type: FailureType
    frequency: float  # 発生頻度（回/時間）
    duration: float   # 継続時間（秒）
    impact_severity: str  # low, medium, high, critical

class CircuitBreaker:
    """サーキットブレーカー"""
    
    def __init__(
        self, 
        failure_threshold: int = 5,
        recovery_timeout: int = 60,
        expected_exception: type = Exception
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
    
    async def call(self, func: Callable, *args, **kwargs):
        """サーキットブレーカー経由での関数呼び出し"""
        
        if self.state == 'OPEN':
            if self._should_attempt_reset():
                self.state = 'HALF_OPEN'
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs)
            self._on_success()
            return result
            
        except self.expected_exception as e:
            self._on_failure()
            raise e
    
    def _should_attempt_reset(self) -> bool:
        """リセット試行の判定"""
        return (
            self.last_failure_time and
            time.time() - self.last_failure_time >= self.recovery_timeout
        )
    
    def _on_success(self):
        """成功時の処理"""
        self.failure_count = 0
        self.state = 'CLOSED'
    
    def _on_failure(self):
        """失敗時の処理"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = 'OPEN'

class RetryMechanism:
    """リトライメカニズム"""
    
    def __init__(
        self,
        max_retries: int = 3,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        exponential_base: float = 2.0,
        jitter: bool = True
    ):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.exponential_base = exponential_base
        self.jitter = jitter
    
    async def execute_with_retry(
        self, 
        func: Callable, 
        *args, 
        **kwargs
    ):
        """リトライ付き実行"""
        
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return await func(*args, **kwargs)
                
            except Exception as e:
                last_exception = e
                
                if attempt == self.max_retries:
                    break
                
                # 遅延時間の計算
                delay = self._calculate_delay(attempt)
                await asyncio.sleep(delay)
        
        raise last_exception
    
    def _calculate_delay(self, attempt: int) -> float:
        """遅延時間の計算"""
        
        # 指数バックオフ
        delay = self.base_delay * (self.exponential_base ** attempt)
        delay = min(delay, self.max_delay)
        
        # ジッター追加
        if self.jitter:
            import random
            delay *= (0.5 + random.random() * 0.5)
        
        return delay

class HealthMonitor:
    """ヘルスモニター"""
    
    def __init__(self):
        self.health_checks: Dict[str, Callable] = {}
        self.health_status: Dict[str, Dict] = {}
        self.alert_handlers: List[Callable] = []
    
    def register_health_check(self, service_name: str, check_func: Callable):
        """ヘルスチェック関数の登録"""
        self.health_checks[service_name] = check_func
    
    def register_alert_handler(self, handler: Callable):
        """アラートハンドラーの登録"""
        self.alert_handlers.append(handler)
    
    async def start_monitoring(self):
        """監視開始"""
        
        while True:
            try:
                await self._perform_health_checks()
                await asyncio.sleep(30)  # 30秒間隔
                
            except Exception as e:
                print(f"Health monitoring error: {e}")
                await asyncio.sleep(60)
    
    async def _perform_health_checks(self):
        """ヘルスチェック実行"""
        
        for service_name, check_func in self.health_checks.items():
            try:
                start_time = time.time()
                is_healthy = await check_func()
                response_time = time.time() - start_time
                
                # ステータス更新
                previous_status = self.health_status.get(service_name, {}).get('healthy', True)
                
                self.health_status[service_name] = {
                    'healthy': is_healthy,
                    'response_time': response_time,
                    'last_check': datetime.utcnow(),
                    'consecutive_failures': 0 if is_healthy else 
                        self.health_status.get(service_name, {}).get('consecutive_failures', 0) + 1
                }
                
                # 状態変化時のアラート
                if previous_status != is_healthy:
                    await self._send_alert(service_name, is_healthy)
                
            except Exception as e:
                print(f"Health check failed for {service_name}: {e}")
                await self._handle_health_check_failure(service_name)
    
    async def _send_alert(self, service_name: str, is_healthy: bool):
        """アラート送信"""
        
        alert_data = {
            'service_name': service_name,
            'healthy': is_healthy,
            'timestamp': datetime.utcnow(),
            'status': self.health_status[service_name]
        }
        
        for handler in self.alert_handlers:
            try:
                await handler(alert_data)
            except Exception as e:
                print(f"Alert handler error: {e}")

class DisasterRecovery:
    """災害復旧"""
    
    def __init__(self):
        self.backup_manager = BackupManager()
        self.failover_manager = FailoverManager()
        self.recovery_procedures: Dict[str, Callable] = {}
    
    def register_recovery_procedure(self, disaster_type: str, procedure: Callable):
        """復旧手順の登録"""
        self.recovery_procedures[disaster_type] = procedure
    
    async def execute_disaster_recovery(self, disaster_type: str, context: Dict):
        """災害復旧の実行"""
        
        if disaster_type not in self.recovery_procedures:
            raise ValueError(f"No recovery procedure for {disaster_type}")
        
        procedure = self.recovery_procedures[disaster_type]
        
        try:
            # 復旧手順実行
            await procedure(context)
            
            # 復旧確認
            recovery_success = await self._verify_recovery(disaster_type, context)
            
            if recovery_success:
                print(f"Disaster recovery successful for {disaster_type}")
            else:
                print(f"Disaster recovery failed for {disaster_type}")
                await self._escalate_recovery_failure(disaster_type, context)
                
        except Exception as e:
            print(f"Disaster recovery error for {disaster_type}: {e}")
            await self._escalate_recovery_failure(disaster_type, context)
    
    async def _verify_recovery(self, disaster_type: str, context: Dict) -> bool:
        """復旧確認"""
        
        # 基本的なヘルスチェック
        health_monitor = HealthMonitor()
        
        # 全サービスのヘルスチェック
        for service_name in context.get('affected_services', []):
            if service_name in health_monitor.health_checks:
                is_healthy = await health_monitor.health_checks[service_name]()
                if not is_healthy:
                    return False
        
        return True
    
    async def _escalate_recovery_failure(self, disaster_type: str, context: Dict):
        """復旧失敗時のエスカレーション"""
        
        # 緊急アラート送信
        alert_data = {
            'type': 'disaster_recovery_failure',
            'disaster_type': disaster_type,
            'context': context,
            'timestamp': datetime.utcnow(),
            'severity': 'critical'
        }
        
        # 管理者への緊急通知
        await self._send_emergency_notification(alert_data)
```

このスケーラビリティ設計により、トリプルパースペクティブ型戦略AIレーダーは、エンタープライズレベルの大規模運用に対応し、高い可用性とパフォーマンスを維持できます。水平・垂直スケーリングの組み合わせ、インテリジェントな負荷分散、包括的な障害対応により、ビジネスクリティカルな環境での安定運用を実現します。


## 技術スタック選定

トリプルパースペクティブ型戦略AIレーダーの実装において、技術スタックの選定は単なる技術的判断ではなく、ビジネス価値創出、開発効率、運用コスト、将来拡張性を総合的に考慮した戦略的意思決定です。本セクションでは、各層の技術選定根拠と統合方針を詳細に解説します。

### 技術選定の戦略的原則

技術スタック選定において、我々は以下の戦略的原則を採用します。

**ビジネス価値最大化原則**では、技術選択がビジネス価値創出に直接貢献することを重視します。開発速度、運用効率、拡張性がビジネス成果に与える影響を定量的に評価し、最適な技術を選定します。

**エコシステム統合原則**では、選定する技術が既存のエンタープライズ環境や開発チームのスキルセットと調和することを確保します。学習コスト、移行コスト、統合コストを最小化しながら、最大の効果を実現します。

**将来適応性原則**では、技術の進歩や要件変化に柔軟に対応できる技術を選定します。ベンダーロックインを回避し、オープンスタンダードを重視することで、長期的な技術的自由度を確保します。

### 各層の技術選定根拠

#### 1. オーケストレーション層

**選定技術**: n8n + Apache Airflow

```yaml
# infrastructure/orchestration/n8n-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: n8n-orchestrator
  labels:
    app: n8n
    component: orchestration
spec:
  replicas: 3
  selector:
    matchLabels:
      app: n8n
  template:
    metadata:
      labels:
        app: n8n
    spec:
      containers:
      - name: n8n
        image: n8nio/n8n:latest
        ports:
        - containerPort: 5678
        env:
        - name: DB_TYPE
          value: "postgresdb"
        - name: DB_POSTGRESDB_HOST
          value: "postgresql-service"
        - name: DB_POSTGRESDB_DATABASE
          value: "n8n"
        - name: DB_POSTGRESDB_USER
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: username
        - name: DB_POSTGRESDB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        - name: N8N_BASIC_AUTH_ACTIVE
          value: "true"
        - name: N8N_BASIC_AUTH_USER
          valueFrom:
            secretKeyRef:
              name: n8n-secret
              key: username
        - name: N8N_BASIC_AUTH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: n8n-secret
              key: password
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        volumeMounts:
        - name: n8n-data
          mountPath: /home/node/.n8n
      volumes:
      - name: n8n-data
        persistentVolumeClaim:
          claimName: n8n-pvc
```

**選定根拠**:
- **n8n**: ビジュアルワークフロー設計による開発効率化、豊富なコネクタエコシステム、TypeScript/Node.js基盤による拡張性
- **Apache Airflow**: 複雑なデータパイプライン管理、Python生態系との統合、エンタープライズ級の運用機能

**統合戦略**: n8nを主要オーケストレーターとし、Airflowを重いデータ処理タスクの補完として活用

#### 2. アプリケーション層

**選定技術**: FastAPI + Node.js + Go

```python
# services/api_gateway/main.py
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import asyncio
import uvicorn
from typing import Dict, List, Optional
import logging

# FastAPI アプリケーション設定
app = FastAPI(
    title="Triple Perspective Strategic AI Radar API",
    description="Enterprise-grade strategic intelligence platform",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# ミドルウェア設定
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # 本番環境では制限
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)

# セキュリティ設定
security = HTTPBearer()

class TechnologyStackManager:
    """技術スタック管理"""
    
    def __init__(self):
        self.stack_components = {
            "orchestration": {
                "primary": "n8n",
                "secondary": "apache_airflow",
                "rationale": "Visual workflow design + enterprise data pipeline management"
            },
            "api_layer": {
                "primary": "fastapi",
                "secondary": "node_express",
                "rationale": "High performance async API + JavaScript ecosystem integration"
            },
            "data_processing": {
                "primary": "python",
                "secondary": "go",
                "rationale": "ML/AI ecosystem + high-performance concurrent processing"
            },
            "frontend": {
                "primary": "react_typescript",
                "secondary": "vue3",
                "rationale": "Enterprise-grade component ecosystem + type safety"
            },
            "database": {
                "primary": "postgresql",
                "secondary": "mongodb",
                "rationale": "ACID compliance + flexible document storage"
            },
            "cache": {
                "primary": "redis",
                "secondary": "memcached",
                "rationale": "Advanced data structures + simple key-value caching"
            },
            "search": {
                "primary": "elasticsearch",
                "secondary": "opensearch",
                "rationale": "Full-text search + analytics capabilities"
            },
            "message_queue": {
                "primary": "apache_kafka",
                "secondary": "rabbitmq",
                "rationale": "High-throughput streaming + reliable message delivery"
            },
            "container": {
                "primary": "docker",
                "secondary": "podman",
                "rationale": "Industry standard + security-focused alternative"
            },
            "orchestration_platform": {
                "primary": "kubernetes",
                "secondary": "docker_swarm",
                "rationale": "Enterprise orchestration + simple deployment option"
            },
            "monitoring": {
                "primary": "prometheus_grafana",
                "secondary": "datadog",
                "rationale": "Open source observability + enterprise monitoring"
            },
            "ai_ml": {
                "primary": "pytorch_transformers",
                "secondary": "tensorflow",
                "rationale": "Research flexibility + production stability"
            }
        }
    
    def get_stack_recommendation(self, deployment_scale: str) -> Dict:
        """デプロイメント規模に応じたスタック推奨"""
        
        if deployment_scale == "small":
            return self._get_small_scale_stack()
        elif deployment_scale == "medium":
            return self._get_medium_scale_stack()
        elif deployment_scale == "large":
            return self._get_large_scale_stack()
        else:
            return self._get_enterprise_scale_stack()
    
    def _get_small_scale_stack(self) -> Dict:
        """小規模デプロイメント用スタック"""
        return {
            "orchestration": "n8n",
            "api": "fastapi",
            "frontend": "react",
            "database": "postgresql",
            "cache": "redis",
            "deployment": "docker_compose",
            "monitoring": "prometheus_grafana",
            "estimated_cost": "$500-2000/month",
            "team_size": "2-5 developers",
            "data_volume": "< 1TB",
            "concurrent_users": "< 100"
        }
    
    def _get_medium_scale_stack(self) -> Dict:
        """中規模デプロイメント用スタック"""
        return {
            "orchestration": "n8n + airflow",
            "api": "fastapi + node.js",
            "frontend": "react + typescript",
            "database": "postgresql + mongodb",
            "cache": "redis_cluster",
            "search": "elasticsearch",
            "message_queue": "rabbitmq",
            "deployment": "kubernetes",
            "monitoring": "prometheus_grafana + jaeger",
            "estimated_cost": "$2000-10000/month",
            "team_size": "5-15 developers",
            "data_volume": "1-10TB",
            "concurrent_users": "100-1000"
        }
    
    def _get_large_scale_stack(self) -> Dict:
        """大規模デプロイメント用スタック"""
        return {
            "orchestration": "n8n + airflow + temporal",
            "api": "fastapi + node.js + go",
            "frontend": "react + typescript + micro_frontends",
            "database": "postgresql_cluster + mongodb_cluster",
            "cache": "redis_cluster + memcached",
            "search": "elasticsearch_cluster",
            "message_queue": "apache_kafka",
            "deployment": "kubernetes + helm",
            "monitoring": "prometheus_grafana + jaeger + elk_stack",
            "security": "vault + oauth2 + rbac",
            "estimated_cost": "$10000-50000/month",
            "team_size": "15-50 developers",
            "data_volume": "10-100TB",
            "concurrent_users": "1000-10000"
        }
    
    def _get_enterprise_scale_stack(self) -> Dict:
        """エンタープライズ規模用スタック"""
        return {
            "orchestration": "n8n + airflow + temporal + prefect",
            "api": "fastapi + node.js + go + grpc",
            "frontend": "react + typescript + micro_frontends + pwa",
            "database": "postgresql_cluster + mongodb_cluster + cassandra",
            "cache": "redis_cluster + memcached + hazelcast",
            "search": "elasticsearch_cluster + solr",
            "message_queue": "apache_kafka + pulsar",
            "deployment": "kubernetes + helm + gitops",
            "monitoring": "prometheus_grafana + jaeger + elk_stack + datadog",
            "security": "vault + oauth2 + rbac + zero_trust",
            "ai_ml": "kubeflow + mlflow + feast",
            "estimated_cost": "$50000+/month",
            "team_size": "50+ developers",
            "data_volume": "100TB+",
            "concurrent_users": "10000+"
        }

# API エンドポイント
@app.get("/api/v1/stack/recommendation")
async def get_stack_recommendation(
    scale: str = "medium",
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """技術スタック推奨の取得"""
    
    stack_manager = TechnologyStackManager()
    recommendation = stack_manager.get_stack_recommendation(scale)
    
    return {
        "scale": scale,
        "recommendation": recommendation,
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/api/v1/stack/components")
async def get_stack_components(
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """技術スタックコンポーネント一覧の取得"""
    
    stack_manager = TechnologyStackManager()
    
    return {
        "components": stack_manager.stack_components,
        "timestamp": datetime.utcnow().isoformat()
    }
```

**選定根拠**:
- **FastAPI**: 高性能非同期処理、自動API文書生成、型安全性、Python AI/MLエコシステムとの統合
- **Node.js**: JavaScript生態系活用、リアルタイム処理、フロントエンドとの技術統一
- **Go**: 高性能並行処理、低メモリフットプリント、マイクロサービス最適化

#### 3. データ層

**選定技術**: PostgreSQL + MongoDB + Redis + Elasticsearch

```sql
-- database/schema/postgresql_schema.sql
-- PostgreSQL スキーマ設計

-- 組織管理
CREATE TABLE organizations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    domain VARCHAR(255) UNIQUE,
    subscription_tier VARCHAR(50) NOT NULL DEFAULT 'basic',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    settings JSONB DEFAULT '{}'::jsonb
);

-- ユーザー管理
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    role VARCHAR(50) NOT NULL DEFAULT 'user',
    profile JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    last_login_at TIMESTAMP WITH TIME ZONE
);

-- パースペクティブ定義
CREATE TABLE perspective_definitions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    type VARCHAR(50) NOT NULL CHECK (type IN ('technology', 'market', 'business')),
    name VARCHAR(255) NOT NULL,
    description TEXT,
    keywords TEXT[] DEFAULT '{}',
    weight_factors JSONB DEFAULT '{}'::jsonb,
    evaluation_criteria TEXT[] DEFAULT '{}',
    data_sources TEXT[] DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    version VARCHAR(20) DEFAULT '1.0.0'
);

-- データソース管理
CREATE TABLE data_sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    type VARCHAR(50) NOT NULL CHECK (type IN ('web', 'api', 'rss', 'social', 'document')),
    url TEXT,
    configuration JSONB DEFAULT '{}'::jsonb,
    reliability_score DECIMAL(3,2) DEFAULT 0.60,
    relevance_score DECIMAL(3,2) DEFAULT 0.50,
    crawl_frequency VARCHAR(50) DEFAULT '0 */6 * * *', -- cron expression
    last_crawled_at TIMESTAMP WITH TIME ZONE,
    status VARCHAR(20) DEFAULT 'active',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- 収集データ（メタデータのみ、本文はMongoDBに保存）
CREATE TABLE collected_data (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source_id UUID REFERENCES data_sources(id) ON DELETE CASCADE,
    perspective_type VARCHAR(50) NOT NULL,
    title VARCHAR(500),
    url TEXT,
    published_at TIMESTAMP WITH TIME ZONE,
    collected_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    quality_score DECIMAL(3,2),
    relevance_score DECIMAL(3,2),
    content_hash VARCHAR(64) UNIQUE, -- SHA-256 hash for deduplication
    mongodb_document_id VARCHAR(24), -- Reference to MongoDB document
    status VARCHAR(20) DEFAULT 'processed'
);

-- 評価結果
CREATE TABLE evaluation_results (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    data_id UUID REFERENCES collected_data(id) ON DELETE CASCADE,
    perspective_type VARCHAR(50) NOT NULL,
    importance_score DECIMAL(3,2),
    confidence_score DECIMAL(3,2),
    impact_score DECIMAL(3,2),
    urgency_score DECIMAL(3,2),
    evaluation_details JSONB DEFAULT '{}'::jsonb,
    evaluated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- 変化点検出
CREATE TABLE change_points (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    topic_id UUID,
    perspective_type VARCHAR(50) NOT NULL,
    detected_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    change_type VARCHAR(50) NOT NULL CHECK (change_type IN ('trend', 'disruption', 'evolution', 'regression')),
    significance_score DECIMAL(3,2),
    description TEXT,
    evidence_data_ids UUID[] DEFAULT '{}',
    metadata JSONB DEFAULT '{}'::jsonb
);

-- 予測結果
CREATE TABLE predictions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    topic_id UUID,
    change_point_id UUID REFERENCES change_points(id) ON DELETE CASCADE,
    perspective_type VARCHAR(50) NOT NULL,
    prediction_horizon VARCHAR(20) NOT NULL, -- 'short_term', 'mid_term', 'long_term'
    prediction_text TEXT,
    confidence_score DECIMAL(3,2),
    scenarios JSONB DEFAULT '[]'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- 統合洞察
CREATE TABLE integrated_insights (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title VARCHAR(500) NOT NULL,
    description TEXT,
    priority INTEGER DEFAULT 5 CHECK (priority BETWEEN 1 AND 10),
    action_recommendations TEXT[] DEFAULT '{}',
    related_predictions UUID[] DEFAULT '{}',
    related_change_points UUID[] DEFAULT '{}',
    perspectives VARCHAR(50)[] DEFAULT '{}',
    status VARCHAR(20) DEFAULT 'new' CHECK (status IN ('new', 'reviewed', 'actioned', 'archived')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- インデックス作成
CREATE INDEX idx_collected_data_perspective_type ON collected_data(perspective_type);
CREATE INDEX idx_collected_data_collected_at ON collected_data(collected_at);
CREATE INDEX idx_collected_data_quality_score ON collected_data(quality_score);
CREATE INDEX idx_evaluation_results_perspective_type ON evaluation_results(perspective_type);
CREATE INDEX idx_evaluation_results_importance_score ON evaluation_results(importance_score);
CREATE INDEX idx_change_points_detected_at ON change_points(detected_at);
CREATE INDEX idx_change_points_significance_score ON change_points(significance_score);
CREATE INDEX idx_predictions_created_at ON predictions(created_at);
CREATE INDEX idx_integrated_insights_priority ON integrated_insights(priority);
CREATE INDEX idx_integrated_insights_status ON integrated_insights(status);

-- パーティショニング（大規模データ対応）
CREATE TABLE collected_data_y2024m01 PARTITION OF collected_data
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE collected_data_y2024m02 PARTITION OF collected_data
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- 以下、月次パーティション継続...
```

```javascript
// database/schema/mongodb_schema.js
// MongoDB スキーマ設計

// コレクション: collected_content
// 収集されたコンテンツの本文とメタデータ
db.createCollection("collected_content", {
    validator: {
        $jsonSchema: {
            bsonType: "object",
            required: ["postgresql_id", "content", "metadata"],
            properties: {
                postgresql_id: {
                    bsonType: "string",
                    description: "PostgreSQL の collected_data.id への参照"
                },
                content: {
                    bsonType: "string",
                    description: "収集されたコンテンツの本文"
                },
                metadata: {
                    bsonType: "object",
                    description: "追加メタデータ"
                },
                entities: {
                    bsonType: "array",
                    description: "抽出されたエンティティ"
                },
                embedding: {
                    bsonType: "array",
                    description: "ベクトル埋め込み"
                },
                processed_content: {
                    bsonType: "object",
                    description: "処理済みコンテンツ（要約、キーワードなど）"
                }
            }
        }
    }
});

// インデックス作成
db.collected_content.createIndex({ "postgresql_id": 1 }, { unique: true });
db.collected_content.createIndex({ "metadata.source_type": 1 });
db.collected_content.createIndex({ "metadata.collected_at": 1 });
db.collected_content.createIndex({ "entities.type": 1, "entities.value": 1 });

// コレクション: analysis_cache
// 分析結果のキャッシュ
db.createCollection("analysis_cache", {
    validator: {
        $jsonSchema: {
            bsonType: "object",
            required: ["cache_key", "result", "created_at", "expires_at"],
            properties: {
                cache_key: {
                    bsonType: "string",
                    description: "キャッシュキー"
                },
                result: {
                    bsonType: "object",
                    description: "分析結果"
                },
                created_at: {
                    bsonType: "date",
                    description: "作成日時"
                },
                expires_at: {
                    bsonType: "date",
                    description: "有効期限"
                }
            }
        }
    }
});

db.analysis_cache.createIndex({ "cache_key": 1 }, { unique: true });
db.analysis_cache.createIndex({ "expires_at": 1 }, { expireAfterSeconds: 0 });

// コレクション: workflow_logs
// n8n ワークフローの実行ログ
db.createCollection("workflow_logs");
db.workflow_logs.createIndex({ "workflow_id": 1, "execution_id": 1 });
db.workflow_logs.createIndex({ "timestamp": 1 });
db.workflow_logs.createIndex({ "status": 1 });
```

**選定根拠**:
- **PostgreSQL**: ACID準拠、複雑なクエリ対応、JSON型サポート、エンタープライズ級の信頼性
- **MongoDB**: 柔軟なスキーマ、大容量テキストデータ、水平スケーリング、集約パイプライン
- **Redis**: 高速キャッシュ、セッション管理、リアルタイムデータ、Pub/Sub機能
- **Elasticsearch**: 全文検索、ログ分析、リアルタイム分析、スケーラブルな検索

#### 4. AI/ML層

**選定技術**: PyTorch + Transformers + MLflow + Kubeflow

```python
# ai_ml/model_management/model_registry.py
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
import torch
import transformers
import mlflow
import mlflow.pytorch
from pathlib import Path
import json

@dataclass
class ModelMetadata:
    """モデルメタデータ"""
    name: str
    version: str
    model_type: str  # 'transformer', 'classification', 'regression', 'clustering'
    framework: str   # 'pytorch', 'tensorflow', 'scikit-learn'
    performance_metrics: Dict[str, float]
    training_data_info: Dict[str, Any]
    hyperparameters: Dict[str, Any]
    deployment_config: Dict[str, Any]
    created_at: str
    created_by: str

class ModelRegistry:
    """モデルレジストリ"""
    
    def __init__(self, mlflow_tracking_uri: str):
        mlflow.set_tracking_uri(mlflow_tracking_uri)
        self.client = mlflow.tracking.MlflowClient()
    
    def register_model(
        self, 
        model: torch.nn.Module,
        metadata: ModelMetadata,
        artifacts: Optional[Dict[str, str]] = None
    ) -> str:
        """モデルの登録"""
        
        with mlflow.start_run(run_name=f"{metadata.name}_v{metadata.version}"):
            # メタデータのログ
            mlflow.log_params(metadata.hyperparameters)
            mlflow.log_metrics(metadata.performance_metrics)
            
            # モデルの保存
            model_path = f"models/{metadata.name}/v{metadata.version}"
            mlflow.pytorch.log_model(
                pytorch_model=model,
                artifact_path=model_path,
                registered_model_name=metadata.name
            )
            
            # 追加アーティファクトの保存
            if artifacts:
                for artifact_name, artifact_path in artifacts.items():
                    mlflow.log_artifact(artifact_path, artifact_name)
            
            # メタデータファイルの保存
            metadata_dict = {
                "name": metadata.name,
                "version": metadata.version,
                "model_type": metadata.model_type,
                "framework": metadata.framework,
                "performance_metrics": metadata.performance_metrics,
                "training_data_info": metadata.training_data_info,
                "hyperparameters": metadata.hyperparameters,
                "deployment_config": metadata.deployment_config,
                "created_at": metadata.created_at,
                "created_by": metadata.created_by
            }
            
            metadata_path = Path("temp_metadata.json")
            with open(metadata_path, "w") as f:
                json.dump(metadata_dict, f, indent=2)
            
            mlflow.log_artifact(str(metadata_path), "metadata")
            metadata_path.unlink()  # 一時ファイル削除
            
            run_id = mlflow.active_run().info.run_id
            
        return run_id
    
    def load_model(self, model_name: str, version: str = "latest") -> torch.nn.Module:
        """モデルの読み込み"""
        
        if version == "latest":
            model_version = self.client.get_latest_versions(
                model_name, 
                stages=["Production", "Staging"]
            )[0]
        else:
            model_version = self.client.get_model_version(model_name, version)
        
        model_uri = f"models:/{model_name}/{model_version.version}"
        model = mlflow.pytorch.load_model(model_uri)
        
        return model
    
    def get_model_metadata(self, model_name: str, version: str) -> ModelMetadata:
        """モデルメタデータの取得"""
        
        model_version = self.client.get_model_version(model_name, version)
        run_id = model_version.run_id
        
        run = self.client.get_run(run_id)
        
        # メタデータファイルのダウンロード
        artifacts = self.client.list_artifacts(run_id, "metadata")
        if artifacts:
            metadata_path = self.client.download_artifacts(
                run_id, 
                "metadata/temp_metadata.json"
            )
            
            with open(metadata_path, "r") as f:
                metadata_dict = json.load(f)
            
            return ModelMetadata(**metadata_dict)
        
        # フォールバック: 基本情報から構築
        return ModelMetadata(
            name=model_name,
            version=version,
            model_type="unknown",
            framework="pytorch",
            performance_metrics=run.data.metrics,
            training_data_info={},
            hyperparameters=run.data.params,
            deployment_config={},
            created_at=str(run.info.start_time),
            created_by=run.info.user_id or "unknown"
        )

class TransformerModelManager:
    """Transformerモデル管理"""
    
    def __init__(self, model_registry: ModelRegistry):
        self.model_registry = model_registry
        self.loaded_models: Dict[str, Any] = {}
    
    def load_pretrained_model(
        self, 
        model_name: str,
        model_type: str = "bert-base-uncased"
    ) -> Dict[str, Any]:
        """事前訓練済みモデルの読み込み"""
        
        if model_name in self.loaded_models:
            return self.loaded_models[model_name]
        
        if "bert" in model_type.lower():
            tokenizer = transformers.BertTokenizer.from_pretrained(model_type)
            model = transformers.BertModel.from_pretrained(model_type)
        elif "gpt" in model_type.lower():
            tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_type)
            model = transformers.GPT2Model.from_pretrained(model_type)
        elif "roberta" in model_type.lower():
            tokenizer = transformers.RobertaTokenizer.from_pretrained(model_type)
            model = transformers.RobertaModel.from_pretrained(model_type)
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
        
        model_dict = {
            "tokenizer": tokenizer,
            "model": model,
            "model_type": model_type
        }
        
        self.loaded_models[model_name] = model_dict
        return model_dict
    
    def fine_tune_model(
        self,
        base_model_name: str,
        training_data: List[Dict],
        training_config: Dict[str, Any]
    ) -> str:
        """モデルのファインチューニング"""
        
        # ベースモデルの読み込み
        base_model_dict = self.loaded_models[base_model_name]
        model = base_model_dict["model"]
        tokenizer = base_model_dict["tokenizer"]
        
        # ファインチューニング実装
        # （実際の実装では、データローダー、トレーナー、評価ロジックを含む）
        
        # 簡略化された例
        fine_tuned_model = self._perform_fine_tuning(
            model, tokenizer, training_data, training_config
        )
        
        # ファインチューニング済みモデルの登録
        metadata = ModelMetadata(
            name=f"{base_model_name}_fine_tuned",
            version="1.0.0",
            model_type="transformer_fine_tuned",
            framework="pytorch",
            performance_metrics={"accuracy": 0.85, "f1_score": 0.82},
            training_data_info={
                "dataset_size": len(training_data),
                "training_config": training_config
            },
            hyperparameters=training_config,
            deployment_config={"batch_size": 32, "max_length": 512},
            created_at=datetime.utcnow().isoformat(),
            created_by="system"
        )
        
        run_id = self.model_registry.register_model(fine_tuned_model, metadata)
        return run_id
    
    def _perform_fine_tuning(
        self,
        model: torch.nn.Module,
        tokenizer: transformers.PreTrainedTokenizer,
        training_data: List[Dict],
        config: Dict[str, Any]
    ) -> torch.nn.Module:
        """ファインチューニングの実行"""
        
        # データセットの準備
        dataset = self._prepare_dataset(training_data, tokenizer)
        
        # トレーニング引数の設定
        training_args = transformers.TrainingArguments(
            output_dir=config.get("output_dir", "./fine_tuned_model"),
            num_train_epochs=config.get("epochs", 3),
            per_device_train_batch_size=config.get("batch_size", 16),
            warmup_steps=config.get("warmup_steps", 500),
            weight_decay=config.get("weight_decay", 0.01),
            logging_dir=config.get("logging_dir", "./logs"),
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
        )
        
        # トレーナーの初期化
        trainer = transformers.Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset["train"],
            eval_dataset=dataset["eval"],
            tokenizer=tokenizer,
        )
        
        # トレーニング実行
        trainer.train()
        
        return model
    
    def _prepare_dataset(
        self, 
        training_data: List[Dict], 
        tokenizer: transformers.PreTrainedTokenizer
    ) -> Dict[str, Any]:
        """データセットの準備"""
        
        # 簡略化された実装
        # 実際の実装では、データの前処理、トークン化、データローダーの作成を含む
        
        texts = [item["text"] for item in training_data]
        labels = [item["label"] for item in training_data]
        
        encodings = tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
        
        # データセット分割
        split_idx = int(len(texts) * 0.8)
        
        train_dataset = {
            "input_ids": encodings["input_ids"][:split_idx],
            "attention_mask": encodings["attention_mask"][:split_idx],
            "labels": torch.tensor(labels[:split_idx])
        }
        
        eval_dataset = {
            "input_ids": encodings["input_ids"][split_idx:],
            "attention_mask": encodings["attention_mask"][split_idx:],
            "labels": torch.tensor(labels[split_idx:])
        }
        
        return {"train": train_dataset, "eval": eval_dataset}
```

**選定根拠**:
- **PyTorch**: 研究開発の柔軟性、動的計算グラフ、豊富なエコシステム
- **Transformers**: 最新のNLPモデル、事前訓練済みモデル、ファインチューニング支援
- **MLflow**: モデルライフサイクル管理、実験追跡、モデルレジストリ
- **Kubeflow**: Kubernetes上でのML運用、パイプライン管理、スケーラブルな訓練

### 統合・互換性の確保

技術スタック間の統合と互換性を確保するため、以下の戦略を採用します。

#### API統合戦略

```python
# integration/api_integration/unified_api.py
from fastapi import FastAPI, HTTPException
from typing import Dict, List, Any, Optional
import asyncio
import aiohttp
import json

class UnifiedAPIGateway:
    """統合APIゲートウェイ"""
    
    def __init__(self):
        self.service_registry = {
            "perspective_management": {
                "base_url": "http://perspective-service:8000",
                "health_endpoint": "/health",
                "api_version": "v1"
            },
            "data_collection": {
                "base_url": "http://data-collection-service:8001",
                "health_endpoint": "/health",
                "api_version": "v1"
            },
            "evaluation_engine": {
                "base_url": "http://evaluation-service:8002",
                "health_endpoint": "/health",
                "api_version": "v1"
            },
            "ai_ml": {
                "base_url": "http://ai-ml-service:8003",
                "health_endpoint": "/health",
                "api_version": "v1"
            }
        }
        
        self.circuit_breakers = {}
        self.load_balancers = {}
    
    async def route_request(
        self, 
        service_name: str, 
        endpoint: str, 
        method: str = "GET",
        data: Optional[Dict] = None,
        headers: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """リクエストのルーティング"""
        
        if service_name not in self.service_registry:
            raise HTTPException(
                status_code=404, 
                detail=f"Service {service_name} not found"
            )
        
        service_config = self.service_registry[service_name]
        url = f"{service_config['base_url']}/api/{service_config['api_version']}{endpoint}"
        
        # サーキットブレーカーチェック
        if self._is_circuit_breaker_open(service_name):
            raise HTTPException(
                status_code=503, 
                detail=f"Service {service_name} is temporarily unavailable"
            )
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.request(
                    method=method,
                    url=url,
                    json=data,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        self._record_success(service_name)
                        return result
                    else:
                        self._record_failure(service_name)
                        raise HTTPException(
                            status_code=response.status,
                            detail=f"Service {service_name} returned error"
                        )
        
        except asyncio.TimeoutError:
            self._record_failure(service_name)
            raise HTTPException(
                status_code=504,
                detail=f"Service {service_name} timeout"
            )
        except Exception as e:
            self._record_failure(service_name)
            raise HTTPException(
                status_code=500,
                detail=f"Service {service_name} error: {str(e)}"
            )
    
    def _is_circuit_breaker_open(self, service_name: str) -> bool:
        """サーキットブレーカーの状態確認"""
        # 実装省略（前述のCircuitBreakerクラスを使用）
        return False
    
    def _record_success(self, service_name: str):
        """成功記録"""
        # 実装省略
        pass
    
    def _record_failure(self, service_name: str):
        """失敗記録"""
        # 実装省略
        pass

class DataFormatStandardizer:
    """データフォーマット標準化"""
    
    def __init__(self):
        self.standard_schemas = {
            "collected_data": {
                "type": "object",
                "required": ["id", "source_id", "content", "metadata"],
                "properties": {
                    "id": {"type": "string", "format": "uuid"},
                    "source_id": {"type": "string", "format": "uuid"},
                    "content": {"type": "string"},
                    "metadata": {"type": "object"},
                    "quality_score": {"type": "number", "minimum": 0, "maximum": 1},
                    "relevance_score": {"type": "number", "minimum": 0, "maximum": 1}
                }
            },
            "evaluation_result": {
                "type": "object",
                "required": ["data_id", "perspective_type", "scores"],
                "properties": {
                    "data_id": {"type": "string", "format": "uuid"},
                    "perspective_type": {"type": "string", "enum": ["technology", "market", "business"]},
                    "scores": {
                        "type": "object",
                        "required": ["importance", "confidence", "impact", "urgency"],
                        "properties": {
                            "importance": {"type": "number", "minimum": 0, "maximum": 1},
                            "confidence": {"type": "number", "minimum": 0, "maximum": 1},
                            "impact": {"type": "number", "minimum": 0, "maximum": 1},
                            "urgency": {"type": "number", "minimum": 0, "maximum": 1}
                        }
                    }
                }
            }
        }
    
    def validate_data_format(self, data: Dict, schema_name: str) -> bool:
        """データフォーマットの検証"""
        
        if schema_name not in self.standard_schemas:
            return False
        
        schema = self.standard_schemas[schema_name]
        
        # JSONスキーマ検証の実装
        # 実際の実装では jsonschema ライブラリを使用
        return True
    
    def transform_to_standard_format(
        self, 
        data: Dict, 
        source_format: str, 
        target_format: str
    ) -> Dict:
        """標準フォーマットへの変換"""
        
        # フォーマット変換ロジック
        # 実際の実装では、各サービス固有のフォーマットから標準フォーマットへの変換を行う
        
        return data
```

### 将来拡張性の考慮

将来の技術進歩や要件変化に対応するため、以下の拡張性設計を採用します。

#### プラグインアーキテクチャ

```python
# extensibility/plugin_architecture/plugin_manager.py
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
import importlib
import inspect
from dataclasses import dataclass

@dataclass
class PluginMetadata:
    """プラグインメタデータ"""
    name: str
    version: str
    description: str
    author: str
    dependencies: List[str]
    api_version: str
    plugin_type: str

class PluginInterface(ABC):
    """プラグインインターフェース"""
    
    @abstractmethod
    def get_metadata(self) -> PluginMetadata:
        """プラグインメタデータの取得"""
        pass
    
    @abstractmethod
    async def initialize(self, config: Dict[str, Any]) -> bool:
        """プラグインの初期化"""
        pass
    
    @abstractmethod
    async def execute(self, input_data: Any) -> Any:
        """プラグインの実行"""
        pass
    
    @abstractmethod
    async def cleanup(self) -> bool:
        """プラグインのクリーンアップ"""
        pass

class DataCollectorPlugin(PluginInterface):
    """データ収集プラグインの基底クラス"""
    
    @abstractmethod
    async def collect_data(
        self, 
        source_config: Dict[str, Any],
        keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """データ収集の実行"""
        pass

class EvaluatorPlugin(PluginInterface):
    """評価プラグインの基底クラス"""
    
    @abstractmethod
    async def evaluate_data(
        self, 
        data: Dict[str, Any],
        evaluation_config: Dict[str, Any]
    ) -> Dict[str, float]:
        """データ評価の実行"""
        pass

class PluginManager:
    """プラグインマネージャー"""
    
    def __init__(self):
        self.loaded_plugins: Dict[str, PluginInterface] = {}
        self.plugin_registry: Dict[str, PluginMetadata] = {}
        self.plugin_configs: Dict[str, Dict[str, Any]] = {}
    
    def register_plugin(
        self, 
        plugin_class: type, 
        config: Optional[Dict[str, Any]] = None
    ) -> bool:
        """プラグインの登録"""
        
        try:
            # プラグインインスタンスの作成
            plugin_instance = plugin_class()
            
            # メタデータの取得
            metadata = plugin_instance.get_metadata()
            
            # 依存関係チェック
            if not self._check_dependencies(metadata.dependencies):
                print(f"Plugin {metadata.name}: Dependencies not satisfied")
                return False
            
            # プラグインの初期化
            init_success = await plugin_instance.initialize(config or {})
            if not init_success:
                print(f"Plugin {metadata.name}: Initialization failed")
                return False
            
            # 登録
            self.loaded_plugins[metadata.name] = plugin_instance
            self.plugin_registry[metadata.name] = metadata
            self.plugin_configs[metadata.name] = config or {}
            
            print(f"Plugin {metadata.name} v{metadata.version} registered successfully")
            return True
            
        except Exception as e:
            print(f"Plugin registration failed: {e}")
            return False
    
    def load_plugin_from_module(
        self, 
        module_path: str, 
        class_name: str,
        config: Optional[Dict[str, Any]] = None
    ) -> bool:
        """モジュールからプラグインを読み込み"""
        
        try:
            # モジュールの動的インポート
            module = importlib.import_module(module_path)
            plugin_class = getattr(module, class_name)
            
            # プラグインインターフェースの実装確認
            if not issubclass(plugin_class, PluginInterface):
                print(f"Class {class_name} does not implement PluginInterface")
                return False
            
            return self.register_plugin(plugin_class, config)
            
        except Exception as e:
            print(f"Plugin loading failed: {e}")
            return False
    
    async def execute_plugin(
        self, 
        plugin_name: str, 
        input_data: Any
    ) -> Optional[Any]:
        """プラグインの実行"""
        
        if plugin_name not in self.loaded_plugins:
            print(f"Plugin {plugin_name} not found")
            return None
        
        try:
            plugin = self.loaded_plugins[plugin_name]
            result = await plugin.execute(input_data)
            return result
            
        except Exception as e:
            print(f"Plugin {plugin_name} execution failed: {e}")
            return None
    
    def get_plugins_by_type(self, plugin_type: str) -> List[str]:
        """タイプ別プラグイン一覧の取得"""
        
        return [
            name for name, metadata in self.plugin_registry.items()
            if metadata.plugin_type == plugin_type
        ]
    
    def _check_dependencies(self, dependencies: List[str]) -> bool:
        """依存関係チェック"""
        
        for dependency in dependencies:
            try:
                importlib.import_module(dependency)
            except ImportError:
                return False
        
        return True
    
    async def unload_plugin(self, plugin_name: str) -> bool:
        """プラグインのアンロード"""
        
        if plugin_name not in self.loaded_plugins:
            return False
        
        try:
            plugin = self.loaded_plugins[plugin_name]
            cleanup_success = await plugin.cleanup()
            
            if cleanup_success:
                del self.loaded_plugins[plugin_name]
                del self.plugin_registry[plugin_name]
                del self.plugin_configs[plugin_name]
                print(f"Plugin {plugin_name} unloaded successfully")
                return True
            else:
                print(f"Plugin {plugin_name} cleanup failed")
                return False
                
        except Exception as e:
            print(f"Plugin {plugin_name} unloading failed: {e}")
            return False

# プラグイン例
class TwitterDataCollectorPlugin(DataCollectorPlugin):
    """Twitter データ収集プラグイン"""
    
    def get_metadata(self) -> PluginMetadata:
        return PluginMetadata(
            name="twitter_collector",
            version="1.0.0",
            description="Twitter API を使用したデータ収集プラグイン",
            author="Triple Perspective Team",
            dependencies=["tweepy", "aiohttp"],
            api_version="1.0",
            plugin_type="data_collector"
        )
    
    async def initialize(self, config: Dict[str, Any]) -> bool:
        self.api_key = config.get("api_key")
        self.api_secret = config.get("api_secret")
        
        if not self.api_key or not self.api_secret:
            return False
        
        # Twitter API クライアントの初期化
        # 実装省略
        return True
    
    async def execute(self, input_data: Any) -> Any:
        return await self.collect_data(input_data.get("source_config", {}), input_data.get("keywords", []))
    
    async def collect_data(
        self, 
        source_config: Dict[str, Any],
        keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """Twitter からのデータ収集"""
        
        # Twitter API を使用したデータ収集の実装
        # 実装省略
        
        return [
            {
                "id": "tweet_123",
                "content": "Sample tweet content",
                "author": "user123",
                "created_at": "2024-01-01T00:00:00Z",
                "metrics": {"likes": 10, "retweets": 5}
            }
        ]
    
    async def cleanup(self) -> bool:
        # リソースのクリーンアップ
        return True
```

この包括的な技術スタック選定により、トリプルパースペクティブ型戦略AIレーダーは、現在の要件を満たしながら将来の拡張にも柔軟に対応できる、堅牢で保守性の高いシステムアーキテクチャを実現します。各技術の選定根拠は明確で、統合戦略により一貫性のあるシステムを構築できます。

